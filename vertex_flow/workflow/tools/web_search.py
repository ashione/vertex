import json
import os
import re
import threading
import time
from typing import Any, Dict, List, Optional
from urllib.parse import quote_plus

import requests

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.tools.functions import FunctionTool

logging = LoggerUtil.get_logger()

# æœç´¢æœåŠ¡å¸¸é‡
SEARCH_PROVIDER_BRAVE = "brave"
SEARCH_PROVIDER_BOCHA = "bocha"
SEARCH_PROVIDER_SERPAPI = "serpapi"
SEARCH_PROVIDER_SEARCHAPI = "searchapi"
SEARCH_PROVIDER_DUCKDUCKGO = "duckduckgo"

# æœç´¢æœåŠ¡åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
SEARCH_PROVIDERS = [
    SEARCH_PROVIDER_BRAVE,
    SEARCH_PROVIDER_BOCHA,
    SEARCH_PROVIDER_SERPAPI,
    SEARCH_PROVIDER_SEARCHAPI,
    SEARCH_PROVIDER_DUCKDUCKGO,
]

# å…¨å±€é…ç½®ç¼“å­˜
_web_search_config_cache = None
_config_loaded = False
_config_lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”


def _is_valid_date_format(freshness: str) -> bool:
    """éªŒè¯æ—¥æœŸæ ¼å¼æ˜¯å¦æœ‰æ•ˆ

    Args:
        freshness: æ—¥æœŸå­—ç¬¦ä¸²

    Returns:
        bool: æ˜¯å¦ä¸ºæœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼
    """
    # åŒ¹é… YYYY-MM-DD æ ¼å¼
    single_date_pattern = r"^\d{4}-\d{2}-\d{2}$"
    # åŒ¹é… YYYY-MM-DD..YYYY-MM-DD æ ¼å¼
    date_range_pattern = r"^\d{4}-\d{2}-\d{2}\.\.\d{4}-\d{2}-\d{2}$"

    return bool(re.match(single_date_pattern, freshness) or re.match(date_range_pattern, freshness))


def _get_web_search_config():
    """è·å–Webæœç´¢é…ç½®ï¼ˆå¸¦ç¼“å­˜ï¼‰

    Returns:
        åŒ…å«è¢«å¯ç”¨çš„æœç´¢æœåŠ¡é…ç½®å’Œå¯¹åº”æœç´¢å‡½æ•°çš„å­—å…¸
    """
    global _web_search_config_cache, _config_loaded

    # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿é…ç½®åŠ è½½çš„çº¿ç¨‹å®‰å…¨
    with _config_lock:
        if not _config_loaded or _web_search_config_cache is None:
            try:
                from vertex_flow.workflow.service import VertexFlowService

                service = VertexFlowService.get_instance()

                # è·å–å„ç§æœç´¢æœåŠ¡çš„é…ç½®
                _web_search_config_cache = {}
                enabled_services = []

                # æœç´¢å‡½æ•°æ˜ å°„
                search_functions = {
                    SEARCH_PROVIDER_BRAVE: _search_with_brave,
                    SEARCH_PROVIDER_BOCHA: _search_with_bocha,
                    SEARCH_PROVIDER_SERPAPI: _search_with_serpapi,
                    SEARCH_PROVIDER_SEARCHAPI: _search_with_searchapi,
                    SEARCH_PROVIDER_DUCKDUCKGO: _search_with_duckduckgo,
                }

                # è·å–å„ä¸ªæœç´¢æœåŠ¡çš„é…ç½®ï¼Œåªä¿ç•™å¯ç”¨çš„æœåŠ¡
                for provider in SEARCH_PROVIDERS:
                    try:
                        config = service.get_web_search_config(provider)
                        if config.get("enabled", False):
                            _web_search_config_cache[provider] = {
                                "config": config,
                                "function": search_functions[provider],
                            }
                            enabled_services.append(provider)
                    except Exception as e:
                        logging.warning(f"è·å–{provider}é…ç½®å¤±è´¥: {e}")

                _config_loaded = True
                # åªåœ¨æœ‰å¯ç”¨æœåŠ¡æ—¶æ‰“å°æ—¥å¿—
                if enabled_services:
                    logging.info(f"Webæœç´¢é…ç½®å·²åŠ è½½å¹¶ç¼“å­˜ï¼Œå¯ç”¨çš„æœåŠ¡: {enabled_services}")
                else:
                    logging.warning("Webæœç´¢é…ç½®å·²åŠ è½½ï¼Œä½†æ²¡æœ‰å¯ç”¨ä»»ä½•æœç´¢æœåŠ¡")
            except Exception as e:
                logging.error(f"è·å–Webæœç´¢é…ç½®å¤±è´¥: {str(e)}")
                _web_search_config_cache = {}

    return _web_search_config_cache


def reset_web_search_config_cache():
    """é‡ç½®Webæœç´¢é…ç½®ç¼“å­˜

    å½“é…ç½®æ–‡ä»¶æ›´æ–°æ—¶ï¼Œå¯ä»¥è°ƒç”¨æ­¤å‡½æ•°é‡æ–°åŠ è½½é…ç½®ã€‚
    """
    global _web_search_config_cache, _config_loaded
    with _config_lock:
        _web_search_config_cache = None
        _config_loaded = False
        logging.info("Webæœç´¢é…ç½®ç¼“å­˜å·²é‡ç½®")


class BochaWebSearch:
    """åšæŸ¥Webæœç´¢APIå®¢æˆ·ç«¯"""

    def __init__(self, api_key: str):
        """åˆå§‹åŒ–åšæŸ¥æœç´¢å®¢æˆ·ç«¯

        Args:
            api_key: åšæŸ¥APIå¯†é’¥
        """
        self.api_key = api_key
        self.base_url = "https://api.bochaai.com/v1"
        self.headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    def search(
        self, query: str, count: int = 8, freshness: str = "noLimit", summary: bool = True, search_type: str = "web"
    ) -> Dict[str, Any]:
        """æ‰§è¡ŒWebæœç´¢

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
            count: è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤8ä¸ª
            freshness: æœç´¢ç»“æœæ—¶æ•ˆæ€§ï¼Œå¯é€‰å€¼:
                - noLimit: ä¸é™ï¼ˆé»˜è®¤ï¼Œæ¨èä½¿ç”¨ï¼‰
                - oneDay: ä¸€å¤©å†…
                - oneWeek: ä¸€å‘¨å†…
                - oneMonth: ä¸€ä¸ªæœˆå†…
                - oneYear: ä¸€å¹´å†…
                - YYYY-MM-DD..YYYY-MM-DD: æœç´¢æ—¥æœŸèŒƒå›´
                - YYYY-MM-DD: æœç´¢æŒ‡å®šæ—¥æœŸ
            summary: æ˜¯å¦è¿”å›AIæ€»ç»“ï¼Œé»˜è®¤True
            search_type: æœç´¢ç±»å‹ï¼Œé»˜è®¤"web"

        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        url = f"{self.base_url}/web-search"

        payload = {"query": query, "count": count, "freshness": freshness, "summary": summary}

        try:
            response = requests.post(url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()

            result = response.json()
            # åšæŸ¥APIå“åº”æ ¼å¼: result.data.webPages
            data = result.get("data", {})
            logging.info(f"åšæŸ¥æœç´¢æˆåŠŸï¼ŒæŸ¥è¯¢: {query}, è¿”å›ç»“æœæ•°: {len(data.get('webPages', {}).get('value', []))}")
            return data

        except requests.exceptions.RequestException as e:
            logging.error(f"åšæŸ¥æœç´¢è¯·æ±‚å¤±è´¥: {e}, {response}")
            return {"error": f"æœç´¢è¯·æ±‚å¤±è´¥: {str(e)}", "webPages": {"value": []}}
        except json.JSONDecodeError as e:
            logging.error(f"åšæŸ¥æœç´¢å“åº”è§£æå¤±è´¥: {e}")
            return {"error": f"å“åº”è§£æå¤±è´¥: {str(e)}", "webPages": {"value": []}}


def web_search_function(inputs: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """ç»Ÿä¸€çš„Webæœç´¢å·¥å…·å‡½æ•°

    æ ¹æ®é…ç½®è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æœç´¢æœåŠ¡ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•ã€‚
    """
    # å‚æ•°éªŒè¯
    if not inputs.get("query"):
        return {"success": False, "error": "æŸ¥è¯¢å‚æ•°ä¸èƒ½ä¸ºç©º", "search_engine": "none"}

    query = inputs.get("query", "")
    count = inputs.get("count", 5)
    freshness = inputs.get("freshness", "noLimit")
    summary = inputs.get("summary", True)

    # å‚æ•°ç±»å‹éªŒè¯
    if not query or not isinstance(query, str):
        return {"success": False, "error": "æŸ¥è¯¢å‚æ•°å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²", "search_engine": "none"}
    if not isinstance(count, int) or count <= 0:
        count = 5

    try:
        from vertex_flow.workflow.service import VertexFlowService

        service = VertexFlowService.get_instance()

        # è·å–ç¼“å­˜çš„é…ç½®ï¼Œé¿å…é‡å¤åŠ è½½
        configs = _get_web_search_config() or {}

        # æŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•æœç´¢æœåŠ¡
        for provider in SEARCH_PROVIDERS:
            provider_info = configs.get(provider)
            if provider_info:  # å¦‚æœé…ç½®å­˜åœ¨ï¼Œè¯´æ˜è¯¥æœåŠ¡å·²å¯ç”¨
                search_func = provider_info["function"]
                logging.debug(f"å°è¯•ä½¿ç”¨ {provider} æœç´¢æœåŠ¡")
                result = search_func(service, query, count, freshness, summary, configs)
                if result.get("success", False):
                    result["search_engine"] = provider
                    return result
                else:
                    logging.warning(f"{provider} æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

        # æ²¡æœ‰å¯ç”¨çš„æœç´¢æœåŠ¡
        return {
            "success": False,
            "error": "æ²¡æœ‰å¯ç”¨çš„æœç´¢æœåŠ¡ï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­å¯ç”¨è‡³å°‘ä¸€ä¸ªæœç´¢æœåŠ¡",
            "search_engine": "none",
            "query": query,
            "summary": "",
            "results": [],
            "total_count": 0,
        }

    except Exception as e:
        logging.error(f"Webæœç´¢é…ç½®è·å–å¤±è´¥: {e}")
        return {
            "success": False,
            "error": f"æœç´¢é…ç½®è·å–å¤±è´¥: {str(e)}",
            "query": query,
            "summary": "",
            "results": [],
            "total_count": 0,
            "search_engine": "none",
        }


def _search_with_bocha(
    service, query: str, count: int, freshness: str, summary: bool, configs: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """ä½¿ç”¨Bocha AIæœç´¢"""
    try:
        # ä½¿ç”¨ç¼“å­˜çš„é…ç½®
        provider_info = configs.get(SEARCH_PROVIDER_BOCHA, {})
        config = provider_info.get("config", {})

        if not config.get("sk"):
            return {"success": False, "error": "Bocha APIå¯†é’¥æœªé…ç½®"}

        # åˆ›å»ºBochaæœç´¢å®¢æˆ·ç«¯
        bocha_client = BochaWebSearch(config["sk"])

        # æ‰§è¡Œæœç´¢
        result = bocha_client.search(query, count, freshness, summary)

        if "error" in result:
            return {"success": False, "error": result["error"]}

        # è§£ææœç´¢ç»“æœ
        web_pages = result.get("webPages", {})
        pages = web_pages.get("value", [])
        summary_text = result.get("summary", "")

        # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
        formatted_results = []
        for page in pages:
            formatted_results.append(
                {
                    "title": page.get("title", ""),
                    "url": page.get("url", ""),
                    "snippet": page.get("snippet", ""),
                    "source": "Bocha AI",
                }
            )

        return {
            "success": True,
            "query": query,
            "summary": summary_text,
            "results": formatted_results,
            "total_count": len(formatted_results),
        }

    except Exception as e:
        logging.error(f"Bochaæœç´¢å¼‚å¸¸: {e}")
        return {"success": False, "error": f"Bochaæœç´¢å¼‚å¸¸: {str(e)}"}


def _direct_serpapi_search(api_key: str, query: str, count: int = 5) -> Dict[str, Any]:
    """ç›´æ¥è°ƒç”¨SerpAPIè¿›è¡Œæœç´¢"""
    try:
        url = "https://serpapi.com/search"
        params = {
            "engine": "google",  # ä½¿ç”¨Googleå¼•æ“è·å¾—æ›´å¥½çš„æœç´¢ç»“æœ
            "q": query,
            "api_key": api_key,
            "num": min(count, 10),  # SerpAPIé™åˆ¶æ¯æ¬¡æœ€å¤š10ä¸ªç»“æœ
        }

        response = requests.get(url, params=params, timeout=15)
        response.raise_for_status()

        data = response.json()

        results = []
        # æ£€æŸ¥æœ‰æœºæœç´¢ç»“æœ
        if data.get("organic_results"):
            for result in data["organic_results"][:count]:
                results.append(
                    {
                        "title": result.get("title", ""),
                        "url": result.get("link", ""),
                        "snippet": result.get("snippet", ""),
                        "source": "SerpAPI-Google",
                    }
                )

        # æ£€æŸ¥çŸ¥è¯†é¢æ¿
        elif data.get("knowledge_graph"):
            kg = data["knowledge_graph"]
            results.append(
                {
                    "title": kg.get("title", ""),
                    "url": kg.get("website", ""),
                    "snippet": kg.get("description", ""),
                    "source": "SerpAPI-Knowledge Graph",
                }
            )

        return {"results": results, "total_count": len(results)}

    except Exception as e:
        logging.error(f"SerpAPIç›´æ¥æœç´¢å¤±è´¥: {e}")
        return {"error": str(e)}


def _search_with_duckduckgo(
    service, query: str, count: int, freshness: str, summary: bool, configs: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """ä½¿ç”¨DuckDuckGoæœç´¢"""
    logging.info(f"DuckDuckGoæœç´¢: {query}, {count}, {freshness}, {summary}")

    try:
        # ä½¿ç”¨ç¼“å­˜çš„é…ç½®
        provider_info = configs.get(SEARCH_PROVIDER_DUCKDUCKGO, {})
        config = provider_info.get("config", {}) if provider_info else {}

        # ä½¿ç”¨DuckDuckGo Instant Answer API
        url = "https://api.duckduckgo.com/"
        params = {
            "q": query,
            "format": "json",
            "no_html": "1",
            "skip_disambig": "1",
        }

        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()

        data = response.json()

        # è§£æDuckDuckGoå“åº”
        results = []
        summary_text = ""

        # æå–å³æ—¶ç­”æ¡ˆ
        if data.get("Abstract"):
            results.append(
                {
                    "title": data.get("Heading", "DuckDuckGoå³æ—¶ç­”æ¡ˆ"),
                    "url": data.get("AbstractURL", ""),
                    "snippet": data.get("Abstract", ""),
                    "source": "DuckDuckGo",
                }
            )
            summary_text = data.get("Abstract", "")

        # æå–ç›¸å…³ä¸»é¢˜
        for topic in data.get("RelatedTopics", [])[: count - 1]:
            if isinstance(topic, dict) and topic.get("Text"):
                results.append(
                    {
                        "title": (
                            topic.get("Text", "").split(" - ")[0]
                            if " - " in topic.get("Text", "")
                            else topic.get("Text", "")
                        ),
                        "url": topic.get("FirstURL", ""),
                        "snippet": topic.get("Text", ""),
                        "source": "DuckDuckGo",
                    }
                )

        return {
            "success": True,
            "query": query,
            "summary": summary_text,
            "results": results,
            "total_count": len(results),
        }

    except Exception as e:
        logging.error(f"DuckDuckGoæœç´¢å¼‚å¸¸: {e}")
        return {"success": False, "error": f"DuckDuckGoæœç´¢å¼‚å¸¸: {str(e)}"}


def _search_with_serpapi(
    service, query: str, count: int, freshness: str, summary: bool, configs: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """ä½¿ç”¨SerpAPIæœç´¢"""
    try:
        # ä½¿ç”¨ç¼“å­˜çš„é…ç½®
        provider_info = configs.get(SEARCH_PROVIDER_SERPAPI, {})
        config = provider_info.get("config", {}) if provider_info else {}

        if not config.get("api_key"):
            return {"success": False, "error": "SerpAPIå¯†é’¥æœªé…ç½®"}

        # ä½¿ç”¨SerpAPIæœç´¢
        result = _direct_serpapi_search(config["api_key"], query, count)

        if not result.get("success", False):
            return {"success": False, "error": result.get("error", "SerpAPIæœç´¢å¤±è´¥")}

        return {
            "success": True,
            "query": query,
            "summary": "",  # SerpAPIä¸æä¾›AIæ€»ç»“
            "results": result.get("results", []),
            "total_count": len(result.get("results", [])),
        }

    except Exception as e:
        logging.error(f"SerpAPIæœç´¢å¼‚å¸¸: {e}")
        return {"success": False, "error": f"SerpAPIæœç´¢å¼‚å¸¸: {str(e)}"}


def _search_with_searchapi(
    service, query: str, count: int, freshness: str, summary: bool, configs: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """ä½¿ç”¨SearchAPIæœç´¢"""
    try:
        # ä½¿ç”¨ç¼“å­˜çš„é…ç½®
        provider_info = configs.get(SEARCH_PROVIDER_SEARCHAPI, {})
        config = provider_info.get("config", {}) if provider_info else {}

        if not config.get("api_key"):
            return {"success": False, "error": "SearchAPIå¯†é’¥æœªé…ç½®"}

        api_key = config["api_key"]
        url = "https://www.searchapi.io/api/v1/search"

        params = {
            "api_key": api_key,
            "q": query,
            "num": count,
            "engine": "google",
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()

        data = response.json()

        if "error" in data:
            return {"success": False, "error": f"SearchAPIé”™è¯¯: {data['error']}"}

        # è§£ææœç´¢ç»“æœ
        results = []
        organic_results = data.get("organic_results", [])

        for result in organic_results:
            results.append(
                {
                    "title": result.get("title", ""),
                    "url": result.get("link", ""),
                    "snippet": result.get("snippet", ""),
                    "source": "SearchAPI",
                }
            )

        return {
            "success": True,
            "query": query,
            "summary": "",  # SearchAPIä¸æä¾›AIæ€»ç»“
            "results": results,
            "total_count": len(results),
        }

    except Exception as e:
        logging.error(f"SearchAPIæœç´¢å¼‚å¸¸: {e}")
        return {"success": False, "error": f"SearchAPIæœç´¢å¼‚å¸¸: {str(e)}"}


def _search_with_brave(
    service, query: str, count: int, freshness: str, summary: bool, configs: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """ä½¿ç”¨Brave Search APIæœç´¢"""
    logging.info(f"Brave Searchæœç´¢: {query}, {count}, {freshness}, {summary}")
    try:
        provider_info = configs.get(SEARCH_PROVIDER_BRAVE, {})
        config = provider_info.get("config", {}) if provider_info else {}
        if not config.get("api_key"):
            return {"success": False, "error": "Brave Search APIå¯†é’¥æœªé…ç½®"}
        api_key = config["api_key"]
        url = "https://api.search.brave.com/res/v1/web/search"
        headers = {"Accept": "application/json", "X-Subscription-Token": api_key}
        params = {
            "q": query,
            "count": count,
        }

        response = requests.get(url, headers=headers, params=params, timeout=30)
        response.raise_for_status()

        data = response.json()

        # è§£æBrave Searchç»“æœ
        results = []
        web_results = data.get("web", {}).get("results", [])

        for result in web_results:
            results.append(
                {
                    "title": result.get("title", ""),
                    "url": result.get("url", ""),
                    "snippet": result.get("description", ""),
                    "source": "Brave Search",
                }
            )

        return {
            "success": True,
            "query": query,
            "summary": "",  # Brave Searchä¸æä¾›AIæ€»ç»“
            "results": results,
            "total_count": len(results),
        }

    except Exception as e:
        logging.error(f"Brave Searchæœç´¢å¼‚å¸¸: {e}")
        return {"success": False, "error": f"Brave Searchæœç´¢å¼‚å¸¸: {str(e)}"}


def create_web_search_tool() -> FunctionTool:
    """åˆ›å»ºç»Ÿä¸€çš„Webæœç´¢å·¥å…·å®ä¾‹

    Returns:
        é…ç½®å¥½çš„FunctionToolå®ä¾‹ï¼Œæ”¯æŒå¤šç§æœç´¢å¼•æ“ï¼Œå¯ç›´æ¥ç”¨äºfunction calling
    """
    schema = {
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œæè¿°è¦æœç´¢çš„å†…å®¹"},
            "count": {
                "type": "integer",
                "description": "è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5ä¸ªï¼ŒèŒƒå›´1-20",
                "minimum": 1,
                "maximum": 20,
                "default": 5,
            },
            "freshness": {
                "type": "string",
                "description": "æœç´¢ç»“æœæ—¶æ•ˆæ€§ã€‚å¯é€‰å€¼ï¼šnoLimitï¼ˆä¸é™ï¼Œé»˜è®¤æ¨èï¼‰ã€oneDayï¼ˆä¸€å¤©å†…ï¼‰ã€oneWeekï¼ˆä¸€å‘¨å†…ï¼‰ã€oneMonthï¼ˆä¸€ä¸ªæœˆå†…ï¼‰ã€oneYearï¼ˆä¸€å¹´å†…ï¼‰ã€‚æ³¨æ„ï¼šæ—¥æœŸèŒƒå›´æ ¼å¼ï¼ˆYYYY-MM-DD..YYYY-MM-DDï¼‰å’ŒæŒ‡å®šæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ä»…Bocha AIæ”¯æŒ",
                "default": "noLimit",
            },
            "summary": {
                "type": "boolean",
                "description": "æ˜¯å¦è¿”å›AIç”Ÿæˆçš„æœç´¢ç»“æœæ€»ç»“ã€‚æ³¨æ„ï¼šAIæ€»ç»“åŠŸèƒ½ä¸»è¦ç”±Bocha AIæ”¯æŒï¼Œå…¶ä»–å¼•æ“æä¾›ç®€å•æ€»ç»“",
                "default": True,
            },
        },
        "required": ["query"],
    }

    return FunctionTool(
        name="web_search",
        description="æ™ºèƒ½Webæœç´¢å·¥å…·ï¼Œæ”¯æŒå¤šç§æœç´¢å¼•æ“ã€‚æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„enabledçŠ¶æ€è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„æœç´¢æœåŠ¡ï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºå°è¯•ï¼šBrave Searchã€Bocha AIï¼ˆé«˜è´¨é‡AIæ€»ç»“ï¼‰ã€SerpAPIï¼ˆGoogleæœç´¢ç»“æœï¼‰ã€SearchAPIï¼ˆå¤šæœç´¢å¼•æ“æ”¯æŒï¼‰ã€DuckDuckGoï¼ˆå…è´¹å³æ—¶ç­”æ¡ˆï¼‰ã€‚æ¯æ¬¡åªä½¿ç”¨ä¸€ä¸ªå¯ç”¨çš„æœç´¢æœåŠ¡ã€‚å¯ä»¥æœç´¢æœ€æ–°çš„ç½‘ç»œä¿¡æ¯ï¼Œæ”¯æŒæ–°é—»ã€ç™¾ç§‘ã€å­¦æœ¯ç­‰å¤šç§å†…å®¹æºã€‚é€‚ç”¨äºè·å–å®æ—¶ä¿¡æ¯ã€ç ”ç©¶èµ„æ–™æ”¶é›†ã€äº‹å®æ ¸æŸ¥ç­‰åœºæ™¯ã€‚",
        func=web_search_function,
        schema=schema,
        id="web_search_unified",
    )


# ä¾¿æ·çš„å·¥å…·å®ä¾‹
web_search_tool = create_web_search_tool()


class FreeWebSearchTool:
    """
    Free Web Search Tool - é›†æˆå¤šä¸ªå…è´¹APIçš„æœç´¢å·¥å…·

    æ”¯æŒçš„å…è´¹API:
    1. DuckDuckGo Instant Answer API (å®Œå…¨å…è´¹)
    2. SerpAPIå…è´¹å±‚ (æ¯æœˆ100æ¬¡)
    3. SearchAPI.ioå…è´¹å±‚ (æ¯æœˆ100æ¬¡)
    4. å¤‡ç”¨HTMLè§£ææœç´¢
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.logger = LoggerUtil.get_logger(__name__)

        # APIé…ç½®
        self.serpapi_key = self.config.get("serpapi_key")
        self.searchapi_key = self.config.get("searchapi_key")

        # å…è´¹APIé™åˆ¶è·Ÿè¸ª
        self.daily_usage = {"serpapi": 0, "searchapi": 0, "duckduckgo": 0}

    def search_duckduckgo_instant(self, query: str) -> Dict[str, Any]:
        """
        DuckDuckGo Instant Answer API - å®Œå…¨å…è´¹
        ä¸»è¦ç”¨äºè·å–å³æ—¶ç­”æ¡ˆï¼Œä¸æ˜¯å®Œæ•´çš„æœç´¢ç»“æœ
        """
        try:
            url = "https://api.duckduckgo.com/"
            params = {"q": query, "format": "json", "no_html": "1", "skip_disambig": "1"}

            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()

            data = response.json()

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            results = {"query": query, "source": "duckduckgo_instant", "results": [], "instant_answer": None}

            # å³æ—¶ç­”æ¡ˆ
            if data.get("Answer"):
                results["instant_answer"] = {
                    "answer": data["Answer"],
                    "answer_type": data.get("AnswerType", ""),
                    "source": data.get("AbstractSource", ""),
                }

            # ç›¸å…³ä¸»é¢˜
            if data.get("RelatedTopics"):
                for topic in data["RelatedTopics"][:5]:  # é™åˆ¶5ä¸ªç»“æœ
                    if isinstance(topic, dict) and topic.get("Text"):
                        results["results"].append(
                            {
                                "title": (
                                    topic.get("Text", "")[:100] + "..."
                                    if len(topic.get("Text", "")) > 100
                                    else topic.get("Text", "")
                                ),
                                "url": topic.get("FirstURL", ""),
                                "snippet": topic.get("Text", ""),
                                "source": "DuckDuckGo",
                            }
                        )

            self.daily_usage["duckduckgo"] += 1
            return results

        except Exception as e:
            self.logger.error(f"DuckDuckGoæœç´¢å¤±è´¥: {e}")
            return {"error": str(e), "source": "duckduckgo_instant"}

    def search_serpapi_free(self, query: str) -> Dict[str, Any]:
        """
        SerpAPIå…è´¹å±‚ - æ¯æœˆ100æ¬¡å…è´¹æœç´¢
        """
        if not self.serpapi_key:
            return {"error": "SerpAPI key not configured", "source": "serpapi"}

        if self.daily_usage["serpapi"] >= 3:  # æ¯æ—¥é™åˆ¶3æ¬¡ï¼ŒèŠ‚çœæœˆåº¦é…é¢
            return {"error": "Daily SerpAPI quota exceeded", "source": "serpapi"}

        try:
            url = "https://serpapi.com/search"
            params = {"engine": "duckduckgo", "q": query, "api_key": self.serpapi_key}

            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()

            data = response.json()

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            results = {"query": query, "source": "serpapi_duckduckgo", "results": []}

            # æœ‰æœºæœç´¢ç»“æœ
            if data.get("organic_results"):
                for result in data["organic_results"][:5]:
                    results["results"].append(
                        {
                            "title": result.get("title", ""),
                            "url": result.get("link", ""),
                            "snippet": result.get("snippet", ""),
                            "source": "SerpAPI-DuckDuckGo",
                        }
                    )

            self.daily_usage["serpapi"] += 1
            return results

        except Exception as e:
            self.logger.error(f"SerpAPIæœç´¢å¤±è´¥: {e}")
            return {"error": str(e), "source": "serpapi"}

    def search_searchapi_free(self, query: str) -> Dict[str, Any]:
        """
        SearchAPI.ioå…è´¹å±‚ - æ¯æœˆ100æ¬¡å…è´¹æœç´¢
        """
        if not self.searchapi_key:
            return {"error": "SearchAPI key not configured", "source": "searchapi"}

        if self.daily_usage["searchapi"] >= 3:  # æ¯æ—¥é™åˆ¶3æ¬¡
            return {"error": "Daily SearchAPI quota exceeded", "source": "searchapi"}

        try:
            url = "https://www.searchapi.io/api/v1/search"
            params = {"engine": "duckduckgo", "q": query, "api_key": self.searchapi_key}

            response = requests.get(url, params=params, timeout=15)
            response.raise_for_status()

            data = response.json()

            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            results = {"query": query, "source": "searchapi_duckduckgo", "results": []}

            # æœ‰æœºæœç´¢ç»“æœ
            if data.get("organic_results"):
                for result in data["organic_results"][:5]:
                    results["results"].append(
                        {
                            "title": result.get("title", ""),
                            "url": result.get("link", ""),
                            "snippet": result.get("snippet", ""),
                            "source": "SearchAPI-DuckDuckGo",
                        }
                    )

            self.daily_usage["searchapi"] += 1
            return results

        except Exception as e:
            self.logger.error(f"SearchAPIæœç´¢å¤±è´¥: {e}")
            return {"error": str(e), "source": "searchapi"}

    def search_backup_html(self, query: str) -> Dict[str, Any]:
        """
        å¤‡ç”¨HTMLè§£ææœç´¢ - ç›´æ¥è§£ææœç´¢å¼•æ“ç»“æœé¡µé¢
        ä½œä¸ºæœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
        """
        try:
            # ä½¿ç”¨DuckDuckGoçš„HTMLæœç´¢ï¼ˆæ›´å®½æ¾çš„robots.txtï¼‰
            url = "https://html.duckduckgo.com/html/"
            params = {"q": query}

            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }

            response = requests.get(url, params=params, headers=headers, timeout=10)
            response.raise_for_status()

            # ç®€å•çš„HTMLè§£æï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨éœ€è¦æ›´å¤æ‚çš„è§£æï¼‰
            html = response.text
            results = {"query": query, "source": "backup_html", "results": []}

            # æ·»åŠ ä¸€ä¸ªç®€å•çš„ç»“æœè¯´æ˜
            results["results"].append(
                {
                    "title": f"æœç´¢ç»“æœ: {query}",
                    "url": f"https://duckduckgo.com/?q={quote_plus(query)}",
                    "snippet": f'æ‰¾åˆ°äº†å…³äº"{query}"çš„æœç´¢ç»“æœã€‚ç‚¹å‡»æŸ¥çœ‹å®Œæ•´ç»“æœã€‚',
                    "source": "Backup Search",
                }
            )

            return results

        except Exception as e:
            self.logger.error(f"å¤‡ç”¨æœç´¢å¤±è´¥: {e}")
            return {"error": str(e), "source": "backup_html"}

    def search(self, query: str, max_results: int = 5) -> Dict[str, Any]:
        """
        æ™ºèƒ½æœç´¢ - æŒ‰ä¼˜å…ˆçº§å°è¯•ä¸åŒçš„API
        """
        self.logger.info(f"å¼€å§‹æœç´¢: {query}")

        # æœç´¢ç­–ç•¥ä¼˜å…ˆçº§
        search_methods = [
            ("DuckDuckGo Instant", self.search_duckduckgo_instant),
            ("SerpAPI Free", self.search_serpapi_free),
            ("SearchAPI Free", self.search_searchapi_free),
            ("Backup HTML", self.search_backup_html),
        ]

        all_results = []
        errors = []

        for method_name, method in search_methods:
            try:
                self.logger.info(f"å°è¯•ä½¿ç”¨ {method_name} æœç´¢")
                result = method(query)

                if "error" not in result and result.get("results"):
                    all_results.extend(result["results"])
                    self.logger.info(f"{method_name} æœç´¢æˆåŠŸï¼Œè·å¾— {len(result['results'])} ä¸ªç»“æœ")

                    # å¦‚æœå·²ç»æœ‰è¶³å¤Ÿçš„ç»“æœï¼Œå°±åœæ­¢
                    if len(all_results) >= max_results:
                        break
                else:
                    if "error" in result:
                        errors.append(f"{method_name}: {result['error']}")
                        self.logger.warning(f"{method_name} æœç´¢å¤±è´¥: {result['error']}")

                # é¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(0.5)

            except Exception as e:
                error_msg = f"{method_name}: {str(e)}"
                errors.append(error_msg)
                self.logger.error(f"{method_name} æœç´¢å¼‚å¸¸: {e}")

        # å»é‡å’Œé™åˆ¶ç»“æœæ•°é‡
        unique_results = []
        seen_urls = set()

        for result in all_results:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
                if len(unique_results) >= max_results:
                    break

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            "query": query,
            "total_results": len(unique_results),
            "results": unique_results,
            "search_methods_used": [
                method[0] for method in search_methods if method[0] not in [e.split(":")[0] for e in errors]
            ],
            "errors": errors if errors else None,
            "usage_stats": self.daily_usage.copy(),
        }

        self.logger.info(f"æœç´¢å®Œæˆï¼Œæ€»å…±è·å¾— {len(unique_results)} ä¸ªæœ‰æ•ˆç»“æœ")
        return final_result


class WebSearchTool:
    def __init__(self, config=None):
        self.config = config or {}
        self.logger = LoggerUtil.get_logger(__name__)

        # ä¼˜å…ˆä½¿ç”¨å…è´¹æœç´¢å·¥å…·
        self.free_search = FreeWebSearchTool(config)

        # ä¿æŒåŸæœ‰çš„ä»˜è´¹APIé…ç½®ä½œä¸ºå¤‡é€‰
        self.serp_api_key = self.config.get("serp_api_key")
        self.google_api_key = self.config.get("google_api_key")
        self.google_cse_id = self.config.get("google_cse_id")

    def search_web(self, query: str, num_results: int = 5) -> str:
        """
        Webæœç´¢ä¸»å…¥å£ - ä¼˜å…ˆä½¿ç”¨å…è´¹API
        """
        try:
            # é¦–å…ˆå°è¯•å…è´¹æœç´¢
            result = self.free_search.search(query, num_results)

            logging.info(f"æœç´¢ç»“æœ: {result}")

            if result.get("results"):
                # æ ¼å¼åŒ–ç»“æœ
                formatted_results = []
                for i, item in enumerate(result["results"], 1):
                    formatted_result = f"{i}. **{item.get('title', 'No Title')}**\n"
                    formatted_result += f"   URL: {item.get('url', 'No URL')}\n"
                    formatted_result += f"   æ‘˜è¦: {item.get('snippet', 'No snippet available')}\n"
                    formatted_result += f"   æ¥æº: {item.get('source', 'Unknown')}\n"
                    formatted_results.append(formatted_result)

                search_summary = f"ğŸ” æœç´¢æŸ¥è¯¢: {query}\n"
                search_summary += f"ğŸ“Š æ‰¾åˆ° {result['total_results']} ä¸ªç»“æœ\n"
                search_summary += f"ğŸ› ï¸ ä½¿ç”¨çš„æœç´¢æ–¹æ³•: {', '.join(result.get('search_methods_used', []))}\n\n"

                if result.get("errors"):
                    search_summary += f"âš ï¸ éƒ¨åˆ†æœç´¢æ–¹æ³•å¤±è´¥: {'; '.join(result['errors'])}\n\n"

                search_summary += "ğŸ“‹ æœç´¢ç»“æœ:\n" + "\n".join(formatted_results)

                # æ·»åŠ ä½¿ç”¨ç»Ÿè®¡
                usage_stats = result.get("usage_stats", {})
                if any(usage_stats.values()):
                    search_summary += f"\nğŸ“ˆ ä»Šæ—¥APIä½¿ç”¨æƒ…å†µ: "
                    stats = [f"{k}: {v}" for k, v in usage_stats.items() if v > 0]
                    search_summary += ", ".join(stats)

                return search_summary
            else:
                # å¦‚æœå…è´¹æœç´¢å¤±è´¥ï¼Œå°è¯•åŸæœ‰çš„ä»˜è´¹API
                return self._fallback_search(query, num_results)

        except Exception as e:
            self.logger.error(f"æœç´¢å¤±è´¥: {e}")
            return f"æœç´¢å¤±è´¥: {str(e)}"

    def _fallback_search(self, query: str, num_results: int = 5) -> str:
        """
        å¤‡ç”¨æœç´¢æ–¹æ³• - ä½¿ç”¨åŸæœ‰çš„ä»˜è´¹APIé€»è¾‘
        """
        return f"å…è´¹æœç´¢APIæš‚æ—¶ä¸å¯ç”¨ï¼Œå»ºè®®é…ç½®ä»˜è´¹APIå¯†é’¥æˆ–ç¨åé‡è¯•ã€‚\næœç´¢æŸ¥è¯¢: {query}"
