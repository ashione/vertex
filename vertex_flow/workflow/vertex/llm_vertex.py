import asyncio
import inspect
import json
import traceback
from typing import List, Optional

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.stream_data import StreamData

logger = LoggerUtil.get_logger(__name__)
from vertex_flow.workflow.chat import ChatModel
from vertex_flow.workflow.constants import (
    CONTENT_KEY,
    CONVERSATION_HISTORY,
    DEFAULT_MAX_TOOL_ROUNDS,
    ENABLE_REASONING_KEY,
    ENABLE_SEARCH_KEY,
    ENABLE_STREAM,
    MESSAGE_KEY,
    MESSAGE_TYPE_END,
    MESSAGE_TYPE_ERROR,
    MESSAGE_TYPE_REASONING,
    MESSAGE_TYPE_REGULAR,
    MODEL,
    POSTPROCESS,
    PREPROCESS,
    REASONING_CONTENT_ATTR,
    SHOW_REASONING,
    SHOW_REASONING_KEY,
    SOURCE_SCOPE,
    SOURCE_VAR,
    SYSTEM,
    TYPE_KEY,
    USER,
    VERTEX_ID_KEY,
)
from vertex_flow.workflow.event_channel import EventType
from vertex_flow.workflow.tools.tool_caller import RuntimeToolCall, create_tool_caller
from vertex_flow.workflow.tools.tool_manager import ToolManager
from vertex_flow.workflow.utils import (
    compatiable_env_str,
    env_str,
    var_str,
)

from .vertex import (
    Any,
    Callable,
    Dict,
    T,
    Vertex,
    WorkflowContext,
)

logging = LoggerUtil.get_logger()


class LLMVertex(Vertex[T]):
    """è¯­è¨€æ¨¡å‹é¡¶ç‚¹ï¼Œæœ‰ä¸€ä¸ªè¾“å…¥å’Œä¸€ä¸ªè¾“å‡º"""

    def __init__(
        self,
        id: str,
        name: str = None,
        task: Optional[Callable[[Dict[str, Any], WorkflowContext[T]], T]] = None,
        params: Dict[str, Any] = None,
        tools: list = None,  # æ–°å¢å‚æ•°
        variables: List[Dict[str, Any]] = None,
        model: ChatModel = None,  # æ·»åŠ modelå‚æ•°
        tool_caller=None,  # æ–°å¢tool_callerå‚æ•°
    ):
        # """å¦‚æœä¼ å…¥taskåˆ™ä»¥taskä¸ºæ‰§è¡Œå•å…ƒï¼Œå¦åˆ™æ‰§è¡Œå½“å‰llmçš„chatæ–¹æ³•."""
        self.model: ChatModel = model  # ä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„model
        self.messages = []
        self.system_message = None
        self.user_messages = []
        self.preprocess = None
        self.postprocess = None
        self.tools = tools or []  # ä¿å­˜å¯ç”¨çš„function tools
        self.tool_caller = tool_caller  # ä¿å­˜å·¥å…·è°ƒç”¨å™¨
        self.enable_stream = params.get(ENABLE_STREAM, False) if params else False  # ä½¿ç”¨å¸¸é‡ ENABLE_STREAM
        self.enable_reasoning = params.get(ENABLE_REASONING_KEY, False) if params else False  # æ”¯æŒæ€è€ƒè¿‡ç¨‹
        self.show_reasoning = (
            params.get(SHOW_REASONING_KEY, SHOW_REASONING) if params else SHOW_REASONING
        )  # æ˜¯å¦æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
        self.token_usage = {}  # æ·»åŠ tokenç»Ÿè®¡å±æ€§
        self.usage_history = []  # æ·»åŠ usageå†å²è®°å½•ï¼Œç”¨äºå¤šè½®å¯¹è¯ç»Ÿè®¡

        # åˆå§‹åŒ–ç»Ÿä¸€å·¥å…·ç®¡ç†å™¨
        self.tool_manager = ToolManager(tool_caller, tools or [])

        # å¦‚æœæ²¡æœ‰ä¼ å…¥tool_callerï¼Œåˆ™æ ¹æ®æ¨¡å‹æä¾›å•†åˆ›å»ºé»˜è®¤çš„tool_caller
        if self.tool_caller is None and self.model:
            provider = getattr(self.model, "provider", "openai")
            self.tool_caller = create_tool_caller(provider, self.tools)

        # ä¸ºæ¨¡å‹è®¾ç½®ç»Ÿä¸€å·¥å…·ç®¡ç†å™¨
        if self.model:
            if not self.model.tool_manager:
                self.model.tool_manager = self.tool_manager

        if task is None:
            logging.info("Use llm chat in task executing.")
            # å¦‚æœæ²¡æœ‰ä¼ å…¥modelï¼Œåˆ™ä»paramsä¸­è·å–
            if self.model is None:
                self.model = params[MODEL]  # ä½¿ç”¨å¸¸é‡ MODEL
            self.system_message = params[SYSTEM] if SYSTEM in params else ""  # ä½¿ç”¨å¸¸é‡ SYSTEM
            self.user_messages = params[USER] if USER in params else []  # ä½¿ç”¨å¸¸é‡ USER
            task = self.chat
            self.preprocess = params[PREPROCESS] if PREPROCESS in params else None  # ä½¿ç”¨å¸¸é‡ PREPROCESS
            self.postprocess = params[POSTPROCESS] if POSTPROCESS in params else None  # ä½¿ç”¨å¸¸é‡ POSTPROCESS
        super().__init__(id=id, name=name, task_type="LLM", task=task, params=params, variables=variables or [])

    def __get_state__(self):
        data = super().__get_state__()
        # ä¸åºåˆ—åŒ–modelã€‚
        if "params" in data and "model" in data["params"]:
            del data["params"]["model"]
        data.update(
            {
                "user_messages": self.user_messages,
                "system_message": self.system_message,
                "model": self.model.__get_state__(),
            }
        )
        return data

    def execute(self, inputs: Dict[str, T] = None, context: WorkflowContext[T] = None):
        if callable(self._task):
            dependencies_outputs = {dep_id: context.get_output(dep_id) for dep_id in self._dependencies}
            all_inputs = {**dependencies_outputs, **(inputs or {})}

            # æ›´ç²¾ç¡®çš„æ¸…ç©ºç­–ç•¥ï¼šåªåœ¨æ²¡æœ‰conversation_historyæ—¶æ¸…ç©ºï¼Œé¿å…å½±å“å¤šè½®å¯¹è¯
            if not (inputs and CONVERSATION_HISTORY in inputs):
                self.messages = []

            # è·å– task å‡½æ•°çš„ç­¾å
            sig = inspect.signature(self._task)
            has_context = "context" in sig.parameters
            # replace all variables
            self.messages_redirect(all_inputs, context=context)
            try:
                if has_context or self._task == self.chat:
                    # å¦‚æœ task å‡½æ•°å®šä¹‰äº† context å‚æ•°ï¼Œåˆ™ä¼ é€’ context
                    self.output = self._task(inputs=all_inputs, context=context)
                else:
                    # å¦åˆ™ï¼Œä¸ä¼ é€’ context å‚æ•°
                    self.output = self._task(inputs=all_inputs)
            except BaseException as e:
                print(f"Error executing vertex {self._id}: {e}")
                traceback.print_exc()
                raise e
            logging.info(f"LLM {self.id} finished, output : {self.output}.")
        else:
            raise ValueError("For LLM type, task should be a callable function.")

    def messages_redirect(self, inputs, context: WorkflowContext[T]):

        logging.debug(f"{self.id} chat context inputs {inputs}")

        # Add system message if provided
        if self.system_message:
            self.messages.append(
                {"role": "system", "content": self.system_message},
            )

        if self.preprocess is not None:
            self.user_messages = self.preprocess(self.user_messages, inputs, context)

        # Handle conversation history if provided in inputs
        if inputs and CONVERSATION_HISTORY in inputs:
            conversation_history = inputs[CONVERSATION_HISTORY]
            if isinstance(conversation_history, list):
                # Add each message in the conversation history
                for msg in conversation_history:
                    if isinstance(msg, dict) and "role" in msg and "content" in msg:
                        # æ ‡å‡†æ¶ˆæ¯æ ¼å¼
                        self.messages.append(msg)
                    elif isinstance(msg, (tuple, list)) and len(msg) == 2:
                        # Handle (user_msg, assistant_msg) tuple/list format
                        user_msg, assistant_msg = msg
                        self.messages.append({"role": "user", "content": str(user_msg)})
                        self.messages.append({"role": "assistant", "content": str(assistant_msg)})
            elif isinstance(conversation_history, str):
                # Handle string format conversation history (fallback)
                self.messages.append({"role": "user", "content": conversation_history})
        else:
            # Handle traditional user_messages format
            for user_message in self.user_messages:
                self.messages.append({"role": "user", "content": user_message})

        # Handle current user message if provided separately
        current_message = inputs.get("current_message") if inputs else None
        image_url = inputs.get("image_url") if inputs else None

        if current_message or image_url:
            if image_url:
                # æœ‰å›¾ç‰‡ï¼Œåˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
                multimodal_content = []

                # æ·»åŠ æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if current_message:
                    multimodal_content.append({"type": "text", "text": str(current_message)})
                elif inputs.get("text"):
                    multimodal_content.append({"type": "text", "text": str(inputs["text"])})

                # æ·»åŠ å›¾ç‰‡å†…å®¹
                multimodal_content.append({"type": "image_url", "image_url": {"url": image_url}})

                # æ›¿æ¢æˆ–æ·»åŠ å¤šæ¨¡æ€æ¶ˆæ¯
                if self.messages and self.messages[-1]["role"] == "user":
                    # æ›¿æ¢æœ€åä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯
                    self.messages[-1]["content"] = multimodal_content
                else:
                    # æ·»åŠ æ–°çš„å¤šæ¨¡æ€æ¶ˆæ¯
                    self.messages.append({"role": "user", "content": multimodal_content})
            else:
                # åªæœ‰æ–‡æœ¬æ¶ˆæ¯
                if isinstance(current_message, dict) and "content" in current_message:
                    # å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
                    self.messages.append(current_message)
                else:
                    # çº¯æ–‡æœ¬æ¶ˆæ¯
                    self.messages.append({"role": "user", "content": str(current_message)})

        system_contains = False
        # replace by env parameters, user parameters and inputs.
        for message in self.messages:
            if message["role"] == "system":
                if system_contains:
                    continue
                system_contains = True

            if "content" not in message or message["content"] is None:
                continue

            # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
            if isinstance(message["content"], list):
                # å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œåªå¤„ç†æ–‡æœ¬éƒ¨åˆ†
                for content_item in message["content"]:
                    if content_item.get("type") == "text":
                        text_content = content_item["text"]
                        # æ›¿æ¢ç¯å¢ƒå˜é‡
                        for key, value in context.get_env_parameters().items():
                            value = value if isinstance(value, str) else str(value)
                            text_content = text_content.replace(env_str(key), value)
                            text_content = text_content.replace(compatiable_env_str(key), value)

                        # æ›¿æ¢ç”¨æˆ·å‚æ•°
                        for key, value in context.get_user_parameters().items():
                            value = value if isinstance(value, str) else str(value)
                            text_content = text_content.replace(var_str(key), value)

                        # æ›¿æ¢è¾“å…¥å‚æ•°
                        if inputs:
                            for key, value in inputs.items():
                                if key in [CONVERSATION_HISTORY, "current_message", "image_url", "text"]:
                                    continue  # Skip special keys that we've already handled
                                value = value if isinstance(value, str) else str(value)
                                input_placeholder = "{{" + key + "}}"
                                text_content = text_content.replace(input_placeholder, value)

                        text_content = self._replace_placeholders(text_content)
                        content_item["text"] = text_content
            else:
                # çº¯æ–‡æœ¬æ¶ˆæ¯
                text_content = message["content"]
                for key, value in context.get_env_parameters().items():
                    value = value if isinstance(value, str) else str(value)
                    text_content = text_content.replace(env_str(key), value)
                    # For dify workflow compatiable env.
                    text_content = text_content.replace(compatiable_env_str(key), value)

                for key, value in context.get_user_parameters().items():
                    value = value if isinstance(value, str) else str(value)
                    text_content = text_content.replace(var_str(key), value)

                # replace by inputs parameters
                if inputs:
                    for key, value in inputs.items():
                        if key in [CONVERSATION_HISTORY, "current_message", "image_url", "text"]:
                            continue  # Skip special keys that we've already handled
                        value = value if isinstance(value, str) else str(value)
                        # Support {{inputs.key}} format
                        input_placeholder = "{{" + key + "}}"
                        text_content = text_content.replace(input_placeholder, value)

                text_content = self._replace_placeholders(text_content)
                message["content"] = text_content

        logging.debug(f"{self}, {self.id} chat context messages {self.messages}")

    def _handle_token_usage(self):
        """å¤„ç†tokenä½¿ç”¨ç»Ÿè®¡çš„é€šç”¨æ–¹æ³•ï¼Œä¾›å­ç±»è°ƒç”¨"""
        # è®°å½•tokenä½¿ç”¨æƒ…å†µ
        if hasattr(self.model, "get_usage"):
            usage = self.model.get_usage()
            self.usage_history.append(usage)  # æ·»åŠ åˆ°å†å²è®°å½•
            self.token_usage = usage  # å½“å‰è½®æ¬¡
            logging.info(f"LLM {self.id} token usage: {self.token_usage}")

    def chat(self, inputs: Dict[str, Any], context: WorkflowContext[T] = None):
        """Chat with LLM and handle token usage"""
        if self.enable_stream and hasattr(self.model, "chat_stream"):
            return self._chat_stream(inputs, context)

        # éæµå¼å¤„ç†ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹çš„chatæ–¹æ³•å¹¶å¤„ç†å·¥å…·è°ƒç”¨
        option = self._build_llm_option(inputs, context)
        llm_tools = self._build_llm_tools()

        # ä½¿ç”¨tool_managerå¤„ç†å·¥å…·è°ƒç”¨å¾ªç¯
        if self.tool_manager and llm_tools:
            choice = self.tool_manager.handle_tool_calls_complete(
                None, context, self.messages, lambda: self.model.chat(self.messages, option=option, tools=llm_tools)
            )
            content = (
                choice.message.content
                if hasattr(choice, "message") and hasattr(choice.message, "content")
                else str(choice)
            )
        else:
            choice = self.model.chat(self.messages, option=option, tools=llm_tools)
            content = choice.message.content if hasattr(choice.message, "content") else str(choice)

        # åº”ç”¨postprocesså¤„ç†
        result = content if self.postprocess is None else self.postprocess(content, inputs, context)
        self.output = result

        # Handle token usage
        self._handle_token_usage()

        logging.debug(f"chat bot response : {result}")
        return result

    def chat_stream_generator(self, inputs: Dict[str, Any], context: WorkflowContext[T] = None):
        """è¿”å›æµå¼è¾“å‡ºçš„ç”Ÿæˆå™¨ï¼Œæ”¯æŒreasoningå’Œå·¥å…·è°ƒç”¨"""
        if not (self.enable_stream and hasattr(self.model, "chat_stream")):
            # å¦‚æœä¸æ”¯æŒæµå¼è¾“å‡ºï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
            result = self.chat(inputs, context)
            yield result
            return

        # ä½¿ç”¨ä¸“é—¨çš„æµå¼ç”Ÿæˆå™¨é€»è¾‘ï¼Œä¸å‘é€äº‹ä»¶ä½†æ”¯æŒå·¥å…·è°ƒç”¨
        for chunk in self._stream_generator_core(inputs, context):
            yield chunk

    def _chat_stream(self, inputs: Dict[str, Any], context: WorkflowContext[T] = None):
        """åŸºäºäº‹ä»¶çš„æµå¼èŠå¤©ï¼ˆç”¨äºworkflowï¼‰"""
        full_content = ""
        for chunk in self._stream_chat_core(inputs, context, emit_events=True):
            full_content += chunk

        # åº”ç”¨postprocesså¤„ç†
        result = full_content if self.postprocess is None else self.postprocess(full_content, inputs, context)

        self.output = result
        # ç»“æŸäº‹ä»¶ç°åœ¨ç”±_unified_stream_coreç»Ÿä¸€å¤„ç†
        logging.debug(f"chat bot response : {result}")
        return result

    def _stream_generator_core(self, inputs: Dict[str, Any], context: WorkflowContext):
        """
        ä¸“é—¨ç”¨äºchat_stream_generatorçš„æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒreasoningå’Œå·¥å…·è°ƒç”¨
        ä¸å‘é€äº‹ä»¶ï¼Œåªè¿”å›æµå¼å†…å®¹
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„æµå¼æ ¸å¿ƒæ–¹æ³•ï¼Œä½†ä¸å‘é€äº‹ä»¶
        for chunk in self._unified_stream_core(inputs, context, emit_events=False):
            yield chunk

    def _stream_chat_core(self, inputs: Dict[str, Any], context: WorkflowContext, emit_events: bool = True):
        """
        Core streaming chat method with reasoning support and tool calling
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„æµå¼æ ¸å¿ƒæ–¹æ³•ï¼Œå¯é€‰æ‹©æ˜¯å¦å‘é€äº‹ä»¶
        for chunk in self._unified_stream_core(inputs, context, emit_events=emit_events):
            yield chunk

    def _unified_stream_core(self, inputs: Dict[str, Any], context: WorkflowContext, emit_events: bool = True):
        """
        ç»Ÿä¸€çš„æµå¼æ ¸å¿ƒé€»è¾‘ï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨ç›´åˆ°stop
        æ ¹æ®emit_eventså‚æ•°å†³å®šæ˜¯å¦å‘é€äº‹ä»¶
        """
        try:
            # Build LLM options
            option = self._build_llm_option(inputs, context)
            llm_tools = self._build_llm_tools()

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨reasoningï¼ˆç”¨äºæ¶ˆæ¯ç±»å‹åˆ¤æ–­ï¼‰
            enable_reasoning = self.params.get(ENABLE_REASONING_KEY, False)
            message_type = MESSAGE_TYPE_REASONING if enable_reasoning else MESSAGE_TYPE_REGULAR

            # ç¡®ä¿åªåœ¨æµå¼æ¨¡å¼ä¸‹ä½¿ç”¨æ­¤æ–¹æ³•
            if not (self.enable_stream and hasattr(self.model, "chat_stream")):
                raise ValueError(
                    f"_unified_stream_core requires streaming mode, but enable_stream={self.enable_stream} or model doesn't support chat_stream"
                )

            # å¤šè½®å¯¹è¯å¾ªç¯ï¼Œç›´åˆ°æ²¡æœ‰å·¥å…·è°ƒç”¨ä¸ºæ­¢
            # æ”¯æŒé€šè¿‡å‚æ•°é…ç½®æœ€å¤§å·¥å…·è°ƒç”¨è½®æ•°ï¼Œé»˜è®¤ä½¿ç”¨å¸¸é‡å€¼
            max_tool_rounds = self.params.get("max_tool_rounds", DEFAULT_MAX_TOOL_ROUNDS)
            tool_round_count = 0

            while True:
                # å¼€å§‹æ–°ä¸€è½®æµå¼å¯¹è¯
                has_tool_calls = False

                # å¤„ç†å•è½®æµå¼å¯¹è¯
                for content_chunk in self._process_single_stream_round(
                    option, llm_tools, message_type, emit_events, context
                ):
                    if content_chunk == "__TOOL_CALLS_DETECTED__":
                        has_tool_calls = True
                        break
                    else:
                        yield content_chunk

                # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œç»“æŸå¯¹è¯
                if not has_tool_calls:
                    break

                # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•°ä¿æŠ¤
                tool_round_count += 1
                if tool_round_count >= max_tool_rounds:
                    warning_msg = f"âš ï¸ å·¥å…·è°ƒç”¨å·²è¾¾åˆ°æœ€å¤§è½®æ•°é™åˆ¶ ({max_tool_rounds})ï¼Œåœæ­¢ç»§ç»­è°ƒç”¨"
                    logger.warning(warning_msg)
                    if emit_events and self.workflow:
                        self.workflow.emit_event(
                            EventType.MESSAGES,
                            {
                                VERTEX_ID_KEY: self.id,
                                CONTENT_KEY: warning_msg,
                                TYPE_KEY: MESSAGE_TYPE_ERROR,
                            },
                        )
                    yield warning_msg
                    break

                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œç»§ç»­ä¸‹ä¸€è½®ï¼ˆå·¥å…·ç»“æœå·²ç»è¢«tool_manageræ·»åŠ åˆ°messagesä¸­ï¼‰
                logger.info(
                    f"Starting tool round {tool_round_count}/{max_tool_rounds}, messages count: {len(self.messages)}"
                )

        except Exception as e:
            error_msg = f"LLM processing error: {str(e)}"
            traceback.print_exc()
            logging.error(error_msg)
            if emit_events and self.workflow:
                self.workflow.emit_event(
                    EventType.MESSAGES, {VERTEX_ID_KEY: self.id, "error": error_msg, TYPE_KEY: MESSAGE_TYPE_ERROR}
                )
            yield error_msg
        finally:
            # Handle token usage after processing is complete
            self._handle_token_usage()

            # Send end event when processing is complete (only for event-based streaming)
            if emit_events and self.workflow:
                self.workflow.emit_event(
                    EventType.MESSAGES, {VERTEX_ID_KEY: self.id, MESSAGE_KEY: None, "status": MESSAGE_TYPE_END}
                )

    def _process_single_stream_round(
        self, option: Dict[str, Any], llm_tools, message_type: str, emit_events: bool, context: WorkflowContext
    ):
        """
        å¤„ç†å•è½®æµå¼å¯¹è¯ï¼Œè¿”å›å†…å®¹å—æˆ–å·¥å…·è°ƒç”¨ä¿¡å·
        """
        # ä½¿ç”¨ ChatModel çš„ chat_stream æ¥å£è·å–æµå¼ç”Ÿæˆå™¨
        stream_generator = self.model.chat_stream(self.messages, option, llm_tools)

        for stream_data in stream_generator:
            if stream_data is None:
                continue

            # å¦‚æœæ˜¯StreamDataå¯¹è±¡ï¼ŒæŒ‰ç…§é¢„æœŸå¤„ç†
            if isinstance(stream_data, StreamData):
                data_type = stream_data.type.value
                data_content = stream_data.get_data()

                logger.debug(
                    f"ğŸ”§ [_process_single_stream_round] Received StreamData: type={data_type}, content_type={type(data_content)}"
                )
                if data_type == "tool_calls":
                    logger.info(
                        f"ğŸ”§ [_process_single_stream_round] Tool calls detected: {len(data_content) if data_content and isinstance(data_content, list) else 'invalid'} calls"
                    )
                    if data_content:
                        for i, tc in enumerate(data_content):
                            tc_id = tc.get("id") if isinstance(tc, dict) else getattr(tc, "id", None)
                            tc_name = (
                                tc.get("function", {}).get("name")
                                if isinstance(tc, dict)
                                else getattr(tc, "function", {}).name if hasattr(tc, "function") else "unknown"
                            )
                            logger.info(
                                f"ğŸ”§ [_process_single_stream_round]   Tool call[{i}]: ID={tc_id}, Name={tc_name}"
                            )

                if data_type == "content" or data_type == "reasoning":
                    # å¤„ç†å†…å®¹æ•°æ®
                    self._emit_content_event(data_content, message_type, emit_events)
                    yield data_content
                elif data_type == "tool_calls":
                    # å¤„ç†å·¥å…·è°ƒç”¨
                    logger.info(
                        f"ğŸ”§ [_process_single_stream_round] About to call _handle_tool_calls with {len(data_content) if data_content and isinstance(data_content, list) else 'invalid'} tool calls"
                    )
                    if data_content and self._handle_tool_calls(data_content, context, emit_events):
                        logger.info(
                            f"ğŸ”§ [_process_single_stream_round] Tool calls handled successfully, yielding signal"
                        )
                        yield "__TOOL_CALLS_DETECTED__"
                        return
                    else:
                        logger.warning(
                            f"ğŸ”§ [_process_single_stream_round] Tool calls handling failed or returned False"
                        )
                elif data_type == "error":
                    # å¤„ç†é”™è¯¯æ•°æ®
                    self._emit_error_event(data_content, emit_events)
                    yield f"\nâš ï¸ {data_content}\n"
                elif data_type == "usage":
                    # å¤„ç†ä½¿ç”¨ç»Ÿè®¡æ•°æ®ï¼Œç´¯åŠ åˆ°å†å²ä¸­
                    self._accumulate_usage(data_content)
                    continue
            else:
                # å¤„ç†éStreamDataå¯¹è±¡
                content_str = str(stream_data) if stream_data else ""
                if content_str:
                    self._emit_content_event(content_str, message_type, emit_events)
                    yield content_str

    def _emit_content_event(self, content: str, message_type: str, emit_events: bool):
        """å‘é€å†…å®¹äº‹ä»¶"""
        if emit_events and self.workflow:
            self.workflow.emit_event(
                EventType.MESSAGES,
                {
                    VERTEX_ID_KEY: self.id,
                    CONTENT_KEY: content,
                    TYPE_KEY: message_type,
                },
            )

    def _emit_error_event(self, error_content: str, emit_events: bool):
        """å‘é€é”™è¯¯äº‹ä»¶"""
        if emit_events and self.workflow:
            self.workflow.emit_event(
                EventType.MESSAGES,
                {
                    VERTEX_ID_KEY: self.id,
                    "error": error_content,
                    TYPE_KEY: MESSAGE_TYPE_ERROR,
                },
            )

    def _handle_tool_calls(self, tool_calls_data, context: WorkflowContext, emit_events: bool) -> bool:
        """å¤„ç†å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ˜¯å¦æˆåŠŸæ‰§è¡Œäº†å·¥å…·"""
        logger.info(
            f"ğŸ”§ [_handle_tool_calls] Called with {len(tool_calls_data) if tool_calls_data and isinstance(tool_calls_data, list) else 'invalid'} tool calls"
        )
        logger.info(
            f"ğŸ”§ [_handle_tool_calls] Tool manager available: {hasattr(self, 'tool_manager') and self.tool_manager is not None}"
        )

        # æ·»åŠ å½“å‰æ¶ˆæ¯å†å²çš„è°ƒè¯•ä¿¡æ¯
        logger.info(f"ğŸ”§ [_handle_tool_calls] Current messages count: {len(self.messages)}")
        last_messages = self.messages[-3:] if len(self.messages) >= 3 else self.messages
        for i, msg in enumerate(last_messages):
            role = msg.get("role", "unknown")
            content_preview = str(msg.get("content", ""))[:50] + (
                "..." if len(str(msg.get("content", ""))) > 50 else ""
            )
            tool_calls_count = len(msg.get("tool_calls", [])) if msg.get("tool_calls") else 0
            logger.info(
                f'ğŸ”§ [_handle_tool_calls]   Message[-{len(last_messages)-i}]: role={role}, content="{content_preview}", tool_calls={tool_calls_count}'
            )

        # æ˜¾ç¤ºå½“å‰å·¥å…·è°ƒç”¨çš„è¯¦ç»†ä¿¡æ¯
        for i, tool_call in enumerate(tool_calls_data):
            if isinstance(tool_call, dict):
                tc_id = tool_call.get("id", "no-id")
                tc_name = tool_call.get("function", {}).get("name", "no-name")
                tc_args = tool_call.get("function", {}).get("arguments", "no-args")
            else:
                tc_id = getattr(tool_call, "id", "no-id")
                tc_name = (
                    getattr(tool_call.function, "name", "no-name") if hasattr(tool_call, "function") else "no-name"
                )
                tc_args = (
                    getattr(tool_call.function, "arguments", "no-args") if hasattr(tool_call, "function") else "no-args"
                )
            logger.info(f'ğŸ”§ [_handle_tool_calls]   ToolCall[{i}]: ID={tc_id}, Name={tc_name}, Args="{tc_args}"')

        # å‘é€å·¥å…·è°ƒç”¨äº‹ä»¶ï¼Œè®©å‰ç«¯çŸ¥é“å·¥å…·è°ƒç”¨çš„å‚æ•°
        if emit_events and self.workflow:
            for tool_call in tool_calls_data:
                self.workflow.emit_event(
                    EventType.MESSAGES,
                    {
                        VERTEX_ID_KEY: self.id,
                        "tool_call": tool_call,
                        TYPE_KEY: "tool_call",
                    },
                )

        # å®é™…æ‰§è¡Œå·¥å…·è°ƒç”¨
        if hasattr(self, "tool_manager") and self.tool_manager:
            tool_executed = self.tool_manager.handle_tool_calls_complete(tool_calls_data, context, self.messages)
            if tool_executed:
                logger.info(f"Tool executed successfully, messages updated to {len(self.messages)} entries")
                return True
        else:
            # å¦‚æœæ²¡æœ‰å·¥å…·ç®¡ç†å™¨ï¼Œè‡³å°‘æ·»åŠ assistantæ¶ˆæ¯åˆ°å¯¹è¯å†å²
            assistant_msg = {"role": "assistant", "content": "", "tool_calls": tool_calls_data}
            if assistant_msg not in self.messages:
                self.messages.append(assistant_msg)
                logger.info(f"Tool calls detected, assistant message appended: {assistant_msg}")

        return False

    def _accumulate_usage(self, usage_data):
        """ç´¯åŠ tokenä½¿ç”¨é‡"""
        if usage_data and isinstance(usage_data, dict):
            # å°†ä½¿ç”¨ç»Ÿè®¡æ·»åŠ åˆ°å†å²è®°å½•ä¸­
            self.usage_history.append(usage_data)
            logger.debug(f"Token usage accumulated: {usage_data}")

    def _build_llm_tools(self):
        if not self.tools:
            return None  # Return None instead of empty list to avoid API error

        # å¦‚æœæœ‰tool_callerï¼Œç¡®ä¿å…¶å·¥å…·åˆ—è¡¨æ˜¯æœ€æ–°çš„
        if self.tool_caller:
            self.tool_caller.tools = self.tools

        # åˆå§‹åŒ–æˆ–æ›´æ–°ç»Ÿä¸€å·¥å…·ç®¡ç†å™¨
        if not hasattr(self, "tool_manager"):
            self.tool_manager = ToolManager(self.tool_caller, self.tools)
        else:
            self.tool_manager.update_tools(self.tools)

        return [
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.schema,
                },
            }
            for tool in self.tools
        ]

    def _build_llm_option(self, inputs: Dict[str, Any], context: Optional[WorkflowContext] = None) -> Dict[str, Any]:
        """Build LLM options from inputs and context"""
        option = {}

        # Add standard parameters
        if "temperature" in self.params:
            option["temperature"] = self.params["temperature"]
        if "max_tokens" in self.params:
            option["max_tokens"] = self.params["max_tokens"]
        if "top_p" in self.params:
            option["top_p"] = self.params["top_p"]

        # Add reasoning parameters (for display control)
        if SHOW_REASONING_KEY in self.params:
            option[SHOW_REASONING_KEY] = self.params[SHOW_REASONING_KEY]

        # Add enable_search parameter for web search
        if ENABLE_SEARCH_KEY in self.params:
            option[ENABLE_SEARCH_KEY] = self.params[ENABLE_SEARCH_KEY]

        # Add tools if available
        if self.tools:
            option["tools"] = [tool.to_dict() for tool in self.tools]

        return option

    def get_total_usage(self) -> dict:
        """
        è·å–å¤šè½®å¯¹è¯çš„æ€»tokenæ¶ˆè€—ç»Ÿè®¡
        """
        total = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}
        for usage in self.usage_history:
            for key in total:
                if usage.get(key) is not None:
                    total[key] += usage[key]
        return total

    def reset_usage_history(self):
        """
        é‡ç½®usageå†å²è®°å½•
        """
        self.usage_history = []
        self.token_usage = {}
