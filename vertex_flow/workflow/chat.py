import abc
import base64
from typing import Any, Dict, List, Optional, Union

import requests
from openai import OpenAI
from openai.types.chat.chat_completion import Choice

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.constants import (
    CONTENT_ATTR,
    ENABLE_REASONING_KEY,
    ENABLE_SEARCH_KEY,
    REASONING_CONTENT_ATTR,
    SHOW_REASONING_KEY,
)
from vertex_flow.workflow.stream_data import StreamData, StreamDataType
from vertex_flow.workflow.utils import factory_creator, timer_decorator

logger = LoggerUtil.get_logger(__name__)


class StreamProcessor:
    """æµå¼å¤„ç†å™¨ï¼Œè´Ÿè´£è§£ææµå¼å“åº”å¹¶è¿”å›ç»“æ„åŒ–æ•°æ®"""

    def __init__(self, chat_model, messages):
        self.chat_model = chat_model
        self.messages = messages
        self.tool_call_fragments = []
        self.tool_calls_detected = False
        self.tool_calls_completed = False

        # å¢é‡çŠ¶æ€ç®¡ç†ï¼šé¿å…é‡å¤åˆå¹¶æ“ä½œ
        self.merged_tool_calls = {}  # {call_id: merged_call_dict}
        self.executed_call_ids = set()  # å·²æ‰§è¡Œçš„å·¥å…·è°ƒç”¨ID
        self.last_fragment_count = 0  # ä¸Šæ¬¡å¤„ç†æ—¶çš„åˆ†ç‰‡æ•°é‡

    def process_stream(self, completion):
        """å¤„ç†æµå¼å“åº”ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®"""
        chunk_count = 0

        for chunk in completion:
            chunk_count += 1
            yield from self._process_chunk(chunk)

        # æµå¼å¤„ç†ç»“æŸåï¼Œå¤„ç†å‰©ä½™çš„å·¥å…·è°ƒç”¨
        yield from self._finalize_tool_calls()

    def _process_chunk(self, chunk):
        """å¤„ç†å•ä¸ªå“åº”å—ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®"""
        # å¤„ç†usageä¿¡æ¯
        self._handle_usage(chunk)

        if not (chunk.choices and len(chunk.choices) > 0):
            return

        delta = chunk.choices[0].delta

        # å¤„ç†å·¥å…·è°ƒç”¨
        tool_call_results = self._handle_tool_calls_in_chunk(chunk)
        if tool_call_results:
            yield from tool_call_results
            return

        # å¤„ç†å†…å®¹
        content_results = self._handle_content(delta)
        if content_results:
            yield from content_results

    def _handle_usage(self, chunk):
        """å¤„ç†usageä¿¡æ¯"""
        if hasattr(chunk, "usage") and chunk.usage:
            self.chat_model._set_usage(chunk)
            logger.debug(f"Streaming usage received from {self.chat_model.provider}: {chunk.usage}")

    def _handle_tool_calls_in_chunk(self, chunk):
        """å¤„ç†å—ä¸­çš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨æ•°æ®"""
        tool_calls_in_chunk = self.chat_model._extract_tool_calls_from_chunk(chunk)
        if not tool_calls_in_chunk:
            return []

        results = []

        # å¦‚æœå·¥å…·è°ƒç”¨å·²å®Œæˆä½†åˆæ£€æµ‹åˆ°æ–°çš„å·¥å…·è°ƒç”¨ï¼Œå…ˆå¤„ç†ä¹‹å‰çš„
        if self.tool_calls_completed:
            remaining_results = list(self._process_remaining_tool_calls())
            self._reset_tool_call_state()
            results.extend(remaining_results)

        self.tool_calls_detected = True
        self.tool_call_fragments.extend(tool_calls_in_chunk)

        # å°è¯•åˆå¹¶å½“å‰çš„å·¥å…·è°ƒç”¨åˆ†ç‰‡ï¼Œå¦‚æœå¯ä»¥åˆå¹¶æˆå®Œæ•´è°ƒç”¨åˆ™è¿”å›ç»“æ„åŒ–æ•°æ®
        complete_tool_results = list(self._try_execute_complete_tool_calls())
        results.extend(complete_tool_results)

        return results

    def _process_remaining_tool_calls(self):
        """å¤„ç†å‰©ä½™çš„å·¥å…·è°ƒç”¨ç‰‡æ®µï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®"""
        if not self.tool_call_fragments:
            return

        remaining_calls = self.chat_model._merge_tool_call_fragments(self.tool_call_fragments)
        if remaining_calls:
            logger.info(f"Processing {len(remaining_calls)} remaining tool calls before new batch")
            # è¿”å›ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨æ•°æ®
            yield StreamData.create_tool_calls(remaining_calls)

    def _reset_tool_call_state(self):
        """é‡ç½®å·¥å…·è°ƒç”¨çŠ¶æ€"""
        self.tool_call_fragments = []
        self.tool_calls_detected = False
        self.tool_calls_completed = False
        logger.info("Reset tool call state for new batch")

    def _handle_content(self, delta):
        """å¤„ç†å†…å®¹ï¼Œè¿”å›ç»“æ„åŒ–çš„å†…å®¹æ•°æ®"""
        # å¤„ç†reasoningå†…å®¹
        reasoning_content = self._extract_reasoning_content(delta)
        if reasoning_content:
            yield StreamData.create_reasoning(reasoning_content)
            return

        # å¤„ç†æ™®é€šå†…å®¹
        content = self._extract_regular_content(delta)
        if content:
            yield StreamData.create_content(content)

    def _extract_reasoning_content(self, delta):
        """æå–reasoningå†…å®¹"""
        if hasattr(delta, REASONING_CONTENT_ATTR) and getattr(delta, REASONING_CONTENT_ATTR):
            return getattr(delta, REASONING_CONTENT_ATTR)
        return None

    def _extract_regular_content(self, delta):
        """æå–å¹¶å¤„ç†æ™®é€šå†…å®¹"""
        if not (hasattr(delta, CONTENT_ATTR) and getattr(delta, CONTENT_ATTR)):
            return None

        content = getattr(delta, CONTENT_ATTR)

        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨ç†æ ‡è®°
        reasoning_markers = ["<thinking>", "<think>", "<reasoning>", "æ€è€ƒï¼š", "åˆ†æï¼š"]
        if any(marker in content for marker in reasoning_markers):
            return self._clean_reasoning_markers(content)

        return content

    def _clean_reasoning_markers(self, content):
        """æ¸…ç†æ¨ç†æ ‡è®°"""
        display_content = content
        tags_to_remove = ["<thinking>", "</thinking>", "<think>", "</think>", "<reasoning>", "</reasoning>"]
        for tag in tags_to_remove:
            display_content = display_content.replace(tag, "")
        return display_content

    def _try_execute_complete_tool_calls(self):
        """å°è¯•è¯†åˆ«å·²å®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®"""
        if not self.tool_call_fragments:
            logger.debug("No tool call fragments to process")
            return

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„åˆ†ç‰‡éœ€è¦å¤„ç†
        current_fragment_count = len(self.tool_call_fragments)
        if current_fragment_count == self.last_fragment_count:
            logger.debug(
                f"No new fragments to process (current: {current_fragment_count}, last: {self.last_fragment_count})"
            )
            return  # æ²¡æœ‰æ–°åˆ†ç‰‡ï¼Œæ— éœ€é‡æ–°å¤„ç†

        # åªå¤„ç†æ–°å¢çš„åˆ†ç‰‡
        new_fragments = self.tool_call_fragments[self.last_fragment_count :]
        logger.debug(f"Processing {len(new_fragments)} new fragments (total: {current_fragment_count})")
        self._update_merged_calls_incrementally(new_fragments)
        self.last_fragment_count = current_fragment_count

        # æ£€æŸ¥å“ªäº›å·¥å…·è°ƒç”¨ç°åœ¨æ˜¯å®Œæ•´çš„ä¸”æœªæ ‡è®°è¿‡
        complete_calls = []
        for call_id, merged_call in self.merged_tool_calls.items():
            if call_id not in self.executed_call_ids:
                is_complete = self._is_tool_call_complete(merged_call)
                logger.debug(
                    f"Tool call {call_id} complete: {is_complete}, arguments: {merged_call.get('function', {}).get('arguments', 'N/A')}"
                )
                if is_complete:
                    complete_calls.append(merged_call)
                    self.executed_call_ids.add(call_id)
                    logger.info(f"Marking tool call {call_id} as identified (not executed by StreamProcessor)")

        # å¦‚æœæœ‰å®Œæ•´çš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®
        if complete_calls:
            logger.info(f"Found {len(complete_calls)} complete tool calls, sending to LLM layer for execution")
            # è¿”å›ç»“æ„åŒ–çš„å·¥å…·è°ƒç”¨æ•°æ®ï¼Œç”±ä¸Šå±‚LLMè´Ÿè´£å®é™…æ‰§è¡Œ
            yield StreamData.create_tool_calls(complete_calls)
        else:
            logger.debug(
                f"No complete tool calls found (merged: {len(self.merged_tool_calls)}, identified: {len(self.executed_call_ids)})"
            )

    def _update_merged_calls_incrementally(self, new_fragments):
        """å¢é‡æ›´æ–°åˆå¹¶çš„å·¥å…·è°ƒç”¨çŠ¶æ€"""
        for fragment in new_fragments:
            if not hasattr(fragment, "id") or not fragment.id:
                continue

            call_id = fragment.id

            # å¦‚æœæ˜¯æ–°çš„å·¥å…·è°ƒç”¨ï¼Œåˆå§‹åŒ–
            if call_id not in self.merged_tool_calls:
                self.merged_tool_calls[call_id] = {
                    "id": call_id,
                    "function": {
                        "name": getattr(fragment.function, "name", "") if hasattr(fragment, "function") else "",
                        "arguments": "",
                    },
                }

            # å¢é‡æ›´æ–°functionä¿¡æ¯
            if hasattr(fragment, "function") and fragment.function:
                current_call = self.merged_tool_calls[call_id]

                # æ›´æ–°function nameï¼ˆå¦‚æœæœ‰ï¼‰
                if hasattr(fragment.function, "name") and fragment.function.name:
                    current_call["function"]["name"] = fragment.function.name

                # å¢é‡æ‹¼æ¥arguments
                if hasattr(fragment.function, "arguments") and fragment.function.arguments:
                    current_call["function"]["arguments"] += fragment.function.arguments

    def _is_tool_call_complete(self, tool_call):
        """æ£€æŸ¥å·¥å…·è°ƒç”¨æ˜¯å¦å®Œæ•´"""
        if not tool_call or not isinstance(tool_call, dict):
            return False

        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if not tool_call.get("id") or not tool_call.get("function"):
            return False

        function = tool_call.get("function", {})
        if not function.get("name"):
            return False

        # æ£€æŸ¥argumentsæ˜¯å¦å­˜åœ¨ï¼ˆå…è®¸ç©ºå­—ç¬¦ä¸²ï¼‰
        arguments = function.get("arguments")
        if arguments is None:
            return False

        # æ”¾å®½JSONå®Œæ•´æ€§æ£€æŸ¥ï¼šå…è®¸ç©ºå­—ç¬¦ä¸²ã€ç©ºå¯¹è±¡æˆ–çœ‹èµ·æ¥å®Œæ•´çš„JSON
        arguments_str = str(arguments).strip()

        # å…è®¸ç©ºå‚æ•°
        if arguments_str == "" or arguments_str == "{}":
            return True

        # æ£€æŸ¥æ˜¯å¦çœ‹èµ·æ¥åƒå®Œæ•´çš„JSONï¼ˆä»¥}ã€]ã€"æˆ–æ•°å­—ç»“å°¾ï¼‰
        if (
            arguments_str.endswith("}")
            or arguments_str.endswith("]")
            or arguments_str.endswith('"')
            or arguments_str.endswith("'")
            or arguments_str[-1].isdigit()
            or arguments_str.lower() in ["true", "false", "null"]
        ):
            return True

        # å°è¯•è§£æJSONæ¥éªŒè¯å®Œæ•´æ€§
        try:
            import json

            json.loads(arguments_str)
            return True
        except (json.JSONDecodeError, ValueError):
            # JSONä¸å®Œæ•´ï¼Œä½†è®°å½•æ—¥å¿—ä»¥ä¾¿è°ƒè¯•
            logger.debug(f"Tool call {tool_call.get('id')} arguments incomplete: {arguments_str}")
            return False

    def _finalize_tool_calls(self):
        """åœ¨æµå¼å¤„ç†ç»“æŸæ—¶ï¼Œæ¸…ç†æ®‹ç•™çŠ¶æ€"""
        # å¤„ç†æ‰€æœ‰å‰©ä½™çš„åˆ†ç‰‡ï¼ˆå¦‚æœæœ‰æ–°çš„ï¼‰
        if len(self.tool_call_fragments) > self.last_fragment_count:
            new_fragments = self.tool_call_fragments[self.last_fragment_count :]
            self._update_merged_calls_incrementally(new_fragments)

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªæ‰§è¡Œçš„å·¥å…·è°ƒç”¨ï¼Œè®°å½•è­¦å‘Šä½†ä¸æ‰§è¡Œ
        remaining_calls = []
        incomplete_calls = []

        for call_id, merged_call in self.merged_tool_calls.items():
            if call_id not in self.executed_call_ids:
                # æ£€æŸ¥å·¥å…·è°ƒç”¨æ˜¯å¦å®Œæ•´
                if self._is_tool_call_complete(merged_call):
                    remaining_calls.append(merged_call)
                    logger.warning(
                        f"Found unidentified complete tool call {call_id} at stream end: {merged_call.get('function', {}).get('name', 'unknown')}"
                    )
                else:
                    incomplete_calls.append((call_id, merged_call))
                    logger.warning(f"Found incomplete tool call {call_id} at stream end: {merged_call}")

        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        if remaining_calls:
            logger.warning(f"Stream ended with {len(remaining_calls)} unidentified complete tool calls")

        if incomplete_calls:
            logger.warning(f"Stream ended with {len(incomplete_calls)} incomplete tool calls")
            for call_id, incomplete_call in incomplete_calls:
                function_name = incomplete_call.get("function", {}).get("name", "unknown")
                arguments = incomplete_call.get("function", {}).get("arguments", "")
                logger.debug(f"Incomplete tool call {call_id}: function={function_name}, arguments={arguments}")

        # æ¸…ç†æ‰€æœ‰çŠ¶æ€ï¼Œé¿å…åç»­è°ƒç”¨æ—¶çŠ¶æ€æ®‹ç•™
        self._reset_all_state()

        # æµå¼å¤„ç†ç»“æŸï¼Œä¸è¿”å›ä»»ä½•æ•°æ®
        return
        yield  # ä½¿è¿™ä¸ªæ–¹æ³•æˆä¸ºç”Ÿæˆå™¨ï¼Œä½†ä¸äº§ç”Ÿä»»ä½•æ•°æ®

    def _reset_all_state(self):
        """é‡ç½®æ‰€æœ‰çŠ¶æ€ï¼Œé¿å…åç»­è°ƒç”¨æ—¶çŠ¶æ€æ®‹ç•™"""
        self.tool_call_fragments = []
        self.tool_calls_detected = False
        self.tool_calls_completed = True

        # æ¸…ç†å¢é‡çŠ¶æ€ç®¡ç†ç›¸å…³çš„çŠ¶æ€
        self.merged_tool_calls.clear()
        self.executed_call_ids.clear()
        self.last_fragment_count = 0


@factory_creator
class ChatModel(abc.ABC):
    """
    è¿™æ˜¯ä¸€ä¸ªæŠ½è±¡åŸºç±»ç¤ºä¾‹ã€‚
    """

    def __init__(self, name: str, sk: str, base_url: str, provider: str, tool_manager=None, tool_caller=None):
        self.name = name
        self.sk = sk
        self.provider = provider
        self._usage = {}  # å­˜å‚¨æœ€æ–°çš„usageä¿¡æ¯
        logger.info(f"Chat model : {self.name}, sk {self.sk}, provider = {self.provider}, base url {base_url}.")
        # ä¸ºåºåˆ—åŒ–ä¿å­˜.
        self._base_url = base_url
        self.client = OpenAI(
            base_url=self._base_url,
            api_key=sk,
        )

        # å·¥å…·ç®¡ç†å™¨
        self.tool_manager = tool_manager

        # å·¥å…·è°ƒç”¨å™¨
        self.tool_caller = tool_caller
        if self.tool_caller is None:
            from vertex_flow.workflow.tools.tool_caller import create_tool_caller

            self.tool_caller = create_tool_caller(provider, [])

    def __get_state__(self):
        return {
            "class_name": self.__class__.__name__.lower(),
            "base_url": self._base_url,
            "name": self.name,
            "sk": self.sk,
            "provider": self.provider,
        }

    def _process_multimodal_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œå°†æ–‡æœ¬å’Œå›¾ç‰‡URLè½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æ ¼å¼
        åŒæ—¶éªŒè¯æ¶ˆæ¯åºåˆ—çš„å®Œæ•´æ€§
        """
        processed_messages = []

        # é¦–å…ˆæ”¶é›†æ‰€æœ‰å¯ç”¨çš„tool_call_ids
        available_tool_call_ids = set()
        for message in messages:
            role = message.get("role", "")
            if role == "assistant" and "tool_calls" in message:
                tool_calls = message.get("tool_calls", [])
                for tc in tool_calls:
                    tc_id = tc.get("id")
                    if tc_id:
                        available_tool_call_ids.add(tc_id)

        # ç„¶åå¤„ç†æ‰€æœ‰æ¶ˆæ¯
        for message in messages:
            logger.debug(f"Processing message: {message}")

            # æ ¹æ®æ¶ˆæ¯è§’è‰²å’Œå†…å®¹ç±»å‹æ¥å¤„ç†
            role = message.get("role", "")
            content = message.get("content")

            # åŠ©æ‰‹æ¶ˆæ¯ä¸”åŒ…å«å·¥å…·è°ƒç”¨
            if role == "assistant" and "tool_calls" in message:
                # å·¥å…·è°ƒç”¨æ¶ˆæ¯ï¼Œæ£€æŸ¥contentæ˜¯å¦ä¸ºnull
                if content is None:
                    logger.warning(f"Assistant message with null content detected, setting to empty string: {message}")
                    message_copy = message.copy()
                    message_copy["content"] = ""
                    processed_messages.append(message_copy)
                else:
                    processed_messages.append(message)
            # å·¥å…·å“åº”æ¶ˆæ¯
            elif role == "tool":
                tool_call_id = message.get("tool_call_id")

                # éªŒè¯toolæ¶ˆæ¯æ˜¯å¦æœ‰å¯¹åº”çš„tool_calls
                if tool_call_id and tool_call_id not in available_tool_call_ids:
                    logger.error(
                        f"Tool message with tool_call_id '{tool_call_id}' has no corresponding tool_calls message. Skipping this message."
                    )
                    continue  # è·³è¿‡è¿™ä¸ªæ— æ•ˆçš„toolæ¶ˆæ¯

                # å·¥å…·å“åº”æ¶ˆæ¯ï¼Œæ£€æŸ¥contentæ˜¯å¦ä¸ºnull
                if content is None:
                    logger.warning(f"Tool message with null content detected, setting to empty string: {message}")
                    message_copy = message.copy()
                    message_copy["content"] = ""
                    processed_messages.append(message_copy)
                else:
                    processed_messages.append(message)
            elif isinstance(content, list):
                # å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
                processed_content = []
                for content_item in content:
                    if content_item.get("type") == "text":
                        processed_content.append(content_item)
                    elif content_item.get("type") == "image_url":
                        image_url = content_item["image_url"]["url"]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯base64ç¼–ç çš„å›¾ç‰‡
                        if image_url.startswith("data:image"):
                            processed_content.append(content_item)
                        else:
                            # å¯¹äºç½‘ç»œURLï¼Œä¿æŒåŸæ ¼å¼
                            processed_content.append(content_item)
                processed_messages.append({"role": role, "content": processed_content})
            elif isinstance(content, str):
                # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼Œä¿æŒåŸæ ¼å¼
                processed_messages.append(message)
            elif content is None:
                # ç©ºå†…å®¹ï¼Œè®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²é¿å…åºåˆ—åŒ–é—®é¢˜
                logger.warning(f"Message with null content detected, setting to empty string: {message}")
                message_copy = message.copy()
                message_copy["content"] = ""
                processed_messages.append(message_copy)
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬
                logger.warning(f"Unknown message format: {message}")
                processed_messages.append(message)

        logger.debug(f"Processed messages: {processed_messages}")
        return processed_messages

    def _build_api_params(self, messages, option: Optional[Dict[str, Any]] = None, stream: bool = False, tools=None):
        """æ„å»ºAPIè°ƒç”¨å‚æ•°çš„åŸºç¡€æ–¹æ³•ï¼Œä¾›å­ç±»è°ƒç”¨"""
        default_option = {
            "temperature": 1.0,
            "max_tokens": 4096,
            "top_p": 1.0,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stream": stream,
            "response_format": {"type": "text"},
        }
        if option:
            default_option.update(option)

        # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
        processed_messages = self._process_multimodal_messages(messages)

        # æ„å»ºAPIè°ƒç”¨å‚æ•° - è¿‡æ»¤æ‰è‡ªå®šä¹‰å‚æ•°
        filtered_option = {
            k: v
            for k, v in default_option.items()
            if k not in [SHOW_REASONING_KEY, ENABLE_REASONING_KEY, ENABLE_SEARCH_KEY]
        }
        api_params = {"model": self.name, "messages": processed_messages, **filtered_option}
        if tools is not None and len(tools) > 0:
            api_params["tools"] = tools

        # é€šç”¨çš„æµå¼usageç»Ÿè®¡æ”¯æŒï¼ˆé€‚ç”¨äºæ”¯æŒOpenAIæ ¼å¼çš„æä¾›å•†ï¼‰
        if stream and self._should_include_stream_usage():
            api_params["stream_options"] = {"include_usage": True}

        return api_params

    def _should_include_stream_usage(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨æµå¼è°ƒç”¨ä¸­åŒ…å«usageç»Ÿè®¡ï¼Œå­ç±»å¯é‡å†™æ­¤æ–¹æ³•"""
        # é»˜è®¤å¯¹å¤§å¤šæ•°æ”¯æŒOpenAIæ ¼å¼çš„æä¾›å•†å¯ç”¨
        supported_providers = ["openai", "deepseek", "tongyi", "openrouter"]
        return self.provider in supported_providers

    def _emit_tool_call_request(self, tool_calls):
        """å‘é€å·¥å…·è°ƒç”¨è¯·æ±‚æ¶ˆæ¯"""
        if not tool_calls:
            return

        # ç»Ÿä¸€é€šè¿‡tool_managerè®¿é—®tool_calleræ ¼å¼åŒ–åŠŸèƒ½
        if self.tool_manager and self.tool_manager.tool_caller:
            for message in self.tool_manager.tool_caller.format_tool_call_request(tool_calls):
                yield message
        else:
            # å›é€€å®ç°ï¼šç¡®ä¿å³ä½¿æ²¡æœ‰tool_managerä¹Ÿèƒ½å‘é€åŸºæœ¬çš„å·¥å…·è°ƒç”¨è¯·æ±‚æ¶ˆæ¯
            logger.warning("No tool_manager.tool_caller available, using fallback tool call request format")
            for tool_call in tool_calls:
                tool_name = (
                    tool_call.get("function", {}).get("name", "")
                    if isinstance(tool_call, dict)
                    else tool_call.function.name
                )
                yield f"\nğŸ”§ è°ƒç”¨å·¥å…·: {tool_name}\n"

    def _emit_tool_call_results(self, tool_calls, messages):
        """å‘é€å·¥å…·è°ƒç”¨ç»“æœæ¶ˆæ¯"""
        if not tool_calls:
            return

        # ç»Ÿä¸€é€šè¿‡tool_managerè®¿é—®tool_calleræ ¼å¼åŒ–åŠŸèƒ½
        if self.tool_manager and self.tool_manager.tool_caller:
            for message in self.tool_manager.tool_caller.format_tool_call_results(tool_calls, messages):
                yield message
        else:
            # å›é€€å®ç°ï¼šç¡®ä¿å³ä½¿æ²¡æœ‰tool_managerä¹Ÿèƒ½å‘é€åŸºæœ¬çš„å·¥å…·è°ƒç”¨ç»“æœæ¶ˆæ¯
            logger.warning("No tool_manager.tool_caller available, using fallback tool call results format")
            for tool_call in tool_calls:
                tool_call_id = tool_call.get("id", "") if isinstance(tool_call, dict) else tool_call.id
                tool_name = (
                    tool_call.get("function", {}).get("name", "")
                    if isinstance(tool_call, dict)
                    else tool_call.function.name
                )
                # æŸ¥æ‰¾å¯¹åº”çš„å·¥å…·å“åº”
                for msg in reversed(messages):
                    if msg.get("role") == "tool" and msg.get("tool_call_id") == tool_call_id:
                        result_content = msg.get("content", "")
                        yield f"\nâœ… å·¥å…· {tool_name} æ‰§è¡Œç»“æœ:\n```\n{result_content}\n```\n"
                        break

    def _create_completion(self, messages, option: Optional[Dict[str, Any]] = None, stream: bool = False, tools=None):
        """Create completion with proper error handling"""
        api_params = self._build_api_params(messages, option, stream, tools)
        try:
            completion = self.client.chat.completions.create(**api_params)
            logger.info(f"show completion: {completion}")
            return completion
        except Exception as e:
            logger.error(f"Error creating completion: {e}, api_params: {api_params}")
            raise

    def chat(self, messages, option: Optional[Dict[str, Any]] = None, tools=None) -> Choice:
        completion = self._create_completion(messages, option, stream=False, tools=tools)
        # è®°å½•usageä¿¡æ¯
        self._set_usage(completion)
        return completion.choices[0]

    def _set_usage(self, completion=None):
        """
        é»˜è®¤å®ç°ï¼šé€‚é… OpenAI/é€šä¹‰ç­‰ä¸»æµ usage å­—æ®µã€‚
        å­ç±»å¯é‡å†™ä»¥é€‚é…ç‰¹å®šçš„ usage å­—æ®µç»“æ„ã€‚
        """
        usage = {}
        if hasattr(completion, "usage") and completion.usage:
            usage = {
                "input_tokens": getattr(completion.usage, "prompt_tokens", None),
                "output_tokens": getattr(completion.usage, "completion_tokens", None),
                "total_tokens": getattr(completion.usage, "total_tokens", None),
            }
        logger.info(f"usage: {usage}")
        self._usage = usage

    def get_usage(self) -> dict:
        return self._usage

    def _merge_tool_call_fragments(self, fragments):
        """
        åˆå¹¶åˆ†ç‰‡ä¸ºæ ‡å‡†çš„tool_call dictåˆ—è¡¨ï¼Œç»Ÿä¸€ä½¿ç”¨ToolManagerä¸­çš„ToolCaller
        """
        if not fragments:
            return []

        # ç»Ÿä¸€é€šè¿‡tool_managerè®¿é—®tool_callerçš„åˆå¹¶é€»è¾‘
        if self.tool_manager and self.tool_manager.tool_caller:
            return self.tool_manager.tool_caller.merge_tool_call_fragments(fragments)

        # å›é€€åˆ°ç®€å•çš„é»˜è®¤å®ç°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        logger.warning("No tool_manager.tool_caller available, using basic fragment merging")
        return fragments if isinstance(fragments, list) else [fragments]

    def chat_stream(self, messages, option: Optional[Dict[str, Any]] = None, tools=None):
        """ç»Ÿä¸€çš„æµå¼è¾“å‡ºæ¥å£ï¼Œå¤„ç†æ‰€æœ‰å†…å®¹ç±»å‹åŒ…æ‹¬reasoning"""
        completion = self._create_completion(messages, option, stream=True, tools=tools)

        # ç»Ÿä¸€çš„æµå¼å¤„ç†ï¼Œæ ¹æ®å¯ç”¨çš„å·¥å…·å¤„ç†å™¨åŠ¨æ€é€‰æ‹©ç­–ç•¥
        yield from self._unified_stream_processing(completion, messages)

    def _unified_stream_processing(self, completion, messages):
        """ç»Ÿä¸€çš„æµå¼å¤„ç†æ–¹æ³•ï¼ŒåŠ¨æ€é€‰æ‹©å·¥å…·å¤„ç†ç­–ç•¥"""
        processor = StreamProcessor(self, messages)
        yield from processor.process_stream(completion)

    def _extract_tool_calls_from_chunk(self, chunk):
        """ä»æµå¼å“åº”å—ä¸­æå–å·¥å…·è°ƒç”¨ï¼Œç»Ÿä¸€ä½¿ç”¨ToolManagerä¸­çš„ToolCaller"""
        # ç»Ÿä¸€é€šè¿‡tool_managerè®¿é—®tool_caller
        if self.tool_manager and self.tool_manager.tool_caller and self.tool_manager.tool_caller.can_handle_streaming():
            if self.tool_manager.tool_caller.is_tool_call_chunk(chunk):
                return self.tool_manager.tool_caller.extract_tool_calls_from_stream(chunk)

        # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
        if chunk.choices and len(chunk.choices) > 0:
            delta = chunk.choices[0].delta
            if hasattr(delta, "tool_calls") and delta.tool_calls:
                return delta.tool_calls

        return []

    def _handle_tool_calls_in_stream(self, tool_calls, messages):
        """åœ¨æµå¼å¤„ç†ä¸­å¤„ç†å·¥å…·è°ƒç”¨ï¼Œç»Ÿä¸€ä½¿ç”¨tool_manager"""
        # ç»Ÿä¸€ä½¿ç”¨å·¥å…·ç®¡ç†å™¨å¤„ç†å·¥å…·è°ƒç”¨
        if self.tool_manager:
            return self.tool_manager.handle_tool_calls_complete(tool_calls, None, messages)

        # å¦‚æœæ²¡æœ‰å·¥å…·ç®¡ç†å™¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•ï¼ˆä»…æ·»åŠ assistantæ¶ˆæ¯ï¼‰
        else:
            assistant_msg = {"role": "assistant", "content": "", "tool_calls": tool_calls}
            if self._should_append_assistant_message(messages, tool_calls):
                messages.append(assistant_msg)
                logger.info(f"Tool calls detected in stream, assistant message appended: {assistant_msg}")
                return True
            else:
                logger.info(f"Tool calls detected in stream, but identical assistant message already exists, skipping")
                return True

    def _should_append_assistant_message(self, messages, tool_calls):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ·»åŠ assistantæ¶ˆæ¯ï¼Œé¿å…é‡å¤"""
        if not messages:
            return True

        last_msg = messages[-1]
        if (
            last_msg.get("role") == "assistant"
            and last_msg.get("tool_calls")
            and len(last_msg["tool_calls"]) == len(tool_calls)
        ):
            # æ£€æŸ¥å·¥å…·è°ƒç”¨æ˜¯å¦ç›¸åŒ
            existing_ids = {tc.get("id") for tc in last_msg["tool_calls"]}
            new_ids = {tc.get("id") for tc in tool_calls}
            if existing_ids == new_ids:
                return False

        return True

    def model_name(self) -> str:
        return self.name

    def __str__(self):
        return self.model_name() or f"{self.__class__.__name__}({self.provider})"

    # search å·¥å…·çš„å…·ä½“å®ç°ï¼Œè¿™é‡Œæˆ‘ä»¬åªéœ€è¦è¿”å›å‚æ•°å³å¯
    def search_impl(self, arguments: Dict[str, Any]) -> Any:
        """
        ä½†å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¿ç•™è”ç½‘æœç´¢çš„åŠŸèƒ½ï¼Œé‚£ä½ åªéœ€è¦ä¿®æ”¹è¿™é‡Œçš„å®ç°ï¼ˆä¾‹å¦‚è°ƒç”¨æœç´¢
        å’Œè·å–ç½‘é¡µå†…å®¹ç­‰ï¼‰ï¼Œå‡½æ•°ç­¾åä¸å˜ï¼Œä¾ç„¶æ˜¯ work çš„ã€‚

        è¿™æœ€å¤§ç¨‹åº¦ä¿è¯äº†å…¼å®¹æ€§ï¼Œå…è®¸ä½ åœ¨ä¸åŒçš„æ¨¡å‹é—´åˆ‡æ¢ï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹ä»£ç æœ‰ç ´åæ€§çš„ä¿®æ”¹ã€‚
        """
        return arguments


class DeepSeek(ChatModel):
    def __init__(self, name="deepseek-chat", sk="", base_url="https://api.deepseek.com"):
        super().__init__(name=name, sk=sk, base_url=base_url, provider="deepseek")


class Tongyi(ChatModel):
    def __init__(self, name="qwen-max", sk="", base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"):
        super().__init__(
            name=name,
            sk=sk,
            base_url=base_url,
            provider="tongyi",
        )

    def _create_completion(self, messages, option: Optional[Dict[str, Any]] = None, stream: bool = False, tools=None):
        """Tongyiä¸“å±ï¼šæµå¼æ—¶è‡ªåŠ¨åŠ stream_options.include_usageï¼Œå¹¶å¤„ç†enable_searchå‚æ•°"""
        # å…ˆæ„å»ºåŸºç¡€APIå‚æ•°
        default_option = {
            "temperature": 1.0,
            "max_tokens": 4096,
            "top_p": 1.0,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stream": stream,
            "response_format": {"type": "text"},
        }
        if option:
            default_option.update(option)

        # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
        processed_messages = self._process_multimodal_messages(messages)

        # æ„å»ºAPIè°ƒç”¨å‚æ•° - è¿‡æ»¤æ‰è‡ªå®šä¹‰å‚æ•°ï¼Œä½†ä¿ç•™enable_search
        filtered_option = {
            k: v
            for k, v in default_option.items()
            if k not in [SHOW_REASONING_KEY, ENABLE_REASONING_KEY, ENABLE_SEARCH_KEY]
        }
        api_params = {"model": self.name, "messages": processed_messages, **filtered_option}
        if tools is not None and len(tools) > 0:
            api_params["tools"] = tools

        # ä»…Tongyiæµå¼åŠ usage
        if stream:
            api_params["stream_options"] = {"include_usage": True}

        # å¤„ç†enable_searchå‚æ•°ï¼ˆé€šä¹‰åƒé—®æ”¯æŒï¼‰
        if ENABLE_SEARCH_KEY in default_option:
            logger.info(f"Tongyi enable_search requested: {default_option[ENABLE_SEARCH_KEY]}")
            api_params["extra_body"] = {
                "extra_body": {ENABLE_SEARCH_KEY: default_option[ENABLE_SEARCH_KEY], "search_options": True}
            }

        try:
            completion = self.client.chat.completions.create(**api_params)
            return completion
        except Exception as e:
            logger.error(f"Error creating completion: {e}")
            raise


class OpenRouter(ChatModel):
    def __init__(self, name="google/gemini-2.5-pro", sk="", base_url="https://openrouter.ai/api/v1"):
        super().__init__(
            name=name,
            sk=sk,
            base_url=base_url,
            provider="openrouter",
        )


class Ollama(ChatModel):
    def __init__(self, name="qwen:7b", sk="ollama-local", base_url="http://localhost:11434"):
        # Ollamaä¸éœ€è¦çœŸå®çš„API keyï¼Œä½¿ç”¨å ä½ç¬¦
        super().__init__(
            name=name,
            sk=sk,
            base_url=f"{base_url}/v1",
            provider="ollama",
        )

    def model_name(self) -> str:
        return self.name


class Doubao(ChatModel):
    def __init__(self, name="doubao-seed-1.6", sk="", base_url="https://ark.cn-beijing.volces.com/api/v3"):
        super().__init__(
            name=name,
            sk=sk,
            base_url=base_url,
            provider="doubao",
        )

    def model_name(self) -> str:
        return self.name


class Other(ChatModel):
    """
    å…¶ä»–è‡ªå®šä¹‰LLMæä¾›å•†ç±»
    æ”¯æŒæ¯ä¸ªæ¨¡å‹å•ç‹¬é…ç½®providerã€model nameå’Œbase_url
    é€‚ç”¨äºè‡ªå®šä¹‰APIç«¯ç‚¹ã€ç§æœ‰éƒ¨ç½²æ¨¡å‹ç­‰åœºæ™¯
    """

    def __init__(self, name="custom-model", sk="", base_url="", provider="other"):
        super().__init__(
            name=name,
            sk=sk,
            base_url=base_url,
            provider=provider,
        )

    def model_name(self) -> str:
        return self.name
