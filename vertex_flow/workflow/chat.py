import abc
import base64
import requests
from typing import Any, Dict, List, Optional, Union

from openai import OpenAI
from openai.types.chat.chat_completion import Choice

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.utils import factory_creator, timer_decorator

logging = LoggerUtil.get_logger()


@factory_creator
class ChatModel(abc.ABC):
    """
    è¿™æ˜¯ä¸€ä¸ªæŠ½è±¡åŸºç±»ç¤ºä¾‹ã€‚
    """

    def __init__(self, name: str, sk: str, base_url: str, provider: str):
        self.name = name
        self.sk = sk
        self.provider = provider
        logging.info(f"Chat model : {self.name}, sk {self.sk}, provider = {self.provider}, base url {base_url}.")
        # ä¸ºåºåˆ—åŒ–ä¿å­˜.
        self._base_url = base_url
        self.client = OpenAI(
            base_url=self._base_url,
            api_key=sk,
        )

    def __get_state__(self):
        return {
            "class_name": self.__class__.__name__.lower(),
            "base_url": self._base_url,
            "name": self.name,
            "sk": self.sk,
            "provider": self.provider,
        }

    def _process_multimodal_messages(self, messages: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯ï¼Œå°†æ–‡æœ¬å’Œå›¾ç‰‡URLè½¬æ¢ä¸ºOpenAIå…¼å®¹çš„æ ¼å¼
        """
        processed_messages = []
        
        for message in messages:
            logging.debug(f"Processing message: {message}")
            
            if isinstance(message.get("content"), list):
                # å¤šæ¨¡æ€æ¶ˆæ¯æ ¼å¼
                processed_content = []
                for content_item in message["content"]:
                    if content_item.get("type") == "text":
                        processed_content.append(content_item)
                    elif content_item.get("type") == "image_url":
                        image_url = content_item["image_url"]["url"]
                        # æ£€æŸ¥æ˜¯å¦æ˜¯base64ç¼–ç çš„å›¾ç‰‡
                        if image_url.startswith("data:image"):
                            processed_content.append(content_item)
                        else:
                            # å¯¹äºç½‘ç»œURLï¼Œä¿æŒåŸæ ¼å¼
                            processed_content.append(content_item)
                processed_messages.append({
                    "role": message["role"],
                    "content": processed_content
                })
            elif isinstance(message.get("content"), str):
                # çº¯æ–‡æœ¬æ¶ˆæ¯ï¼Œä¿æŒåŸæ ¼å¼
                processed_messages.append(message)
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸ºæ–‡æœ¬
                logging.warning(f"Unknown message format: {message}")
                processed_messages.append(message)
        
        logging.debug(f"Processed messages: {processed_messages}")
        return processed_messages

    def _create_completion(self, messages, option: Optional[Dict[str, Any]] = None, stream: bool = False, tools=None):
        """Create completion with proper error handling"""
        default_option = {
            "temperature": 1.0,
            "max_tokens": 4096,
            "top_p": 1.0,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "stream": stream,
            "response_format": {"type": "text"},
        }
        if option:
            default_option.update(option)
        
        # å¤„ç†å¤šæ¨¡æ€æ¶ˆæ¯
        processed_messages = self._process_multimodal_messages(messages)
        
        # æ„å»ºAPIè°ƒç”¨å‚æ•° - è¿‡æ»¤æ‰è‡ªå®šä¹‰å‚æ•°
        filtered_option = {k: v for k, v in default_option.items() 
                          if k not in ["show_reasoning", "enable_reasoning"]}
        api_params = {"model": self.name, "messages": processed_messages, **filtered_option}
        if tools is not None and len(tools) > 0:
            api_params["tools"] = tools
        
        try:
            completion = self.client.chat.completions.create(**api_params)
            return completion
        except Exception as e:
            logging.error(f"Error creating completion: {e}")
            raise

    def chat(self, messages, option: Optional[Dict[str, Any]] = None, tools=None) -> Choice:
        completion = self._create_completion(messages, option, stream=False, tools=tools)
        return completion.choices[0]

    def chat_stream(self, messages, option: Optional[Dict[str, Any]] = None, tools=None):
        completion = self._create_completion(messages, option, stream=True, tools=tools)
        for chunk in completion:
            # ç¡®ä¿ chunk å¯¹è±¡å…·æœ‰ choices å±æ€§ï¼Œå¹¶æ­£ç¡®å¤„ç†å¢é‡æ›´æ–°å†…å®¹
            if hasattr(chunk, "choices") and len(chunk.choices) > 0 and chunk.choices[0].delta:
                content = chunk.choices[0].delta.content
                if content is not None:  # åªyieldéNoneçš„å†…å®¹
                    yield content
            else:
                logging.debug("Chunk object does not have valid choices or delta content.")

    def chat_stream_with_reasoning(self, messages, option: Optional[Dict[str, Any]] = None):
        """
        Enhanced chat stream method with reasoning support
        
        For DeepSeek R1 models, reasoning content is automatically returned in the response
        without needing special parameters in the request.
        """
        try:
            # Create completion without reasoning parameter
            tools = option.get("tools") if option else None
            completion = self._create_completion(messages, option, stream=True, tools=tools)
            
            reasoning_buffer = ""
            content_buffer = ""
            is_reasoning_phase = True
            
            for chunk in completion:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    
                    # Check for reasoning content (DeepSeek R1 models)
                    if hasattr(delta, 'reasoning_content') and delta.reasoning_content:
                        reasoning_buffer += delta.reasoning_content
                        if option and option.get("show_reasoning", True):
                            yield f"{delta.reasoning_content}"
                        continue
                    
                    # Regular content
                    if hasattr(delta, 'content') and delta.content:
                        # If we were in reasoning phase and now have content, add separator
                        if is_reasoning_phase and reasoning_buffer and option and option.get("show_reasoning", True):
                            yield "\n\nğŸ’­ **å›ç­”ï¼š**\n"
                            is_reasoning_phase = False
                        
                        content_buffer += delta.content
                        yield delta.content
                        
            # Log the complete reasoning and content for debugging
            if reasoning_buffer:
                logging.info(f"Reasoning length: {len(reasoning_buffer)} chars")
            if content_buffer:
                logging.info(f"Content length: {len(content_buffer)} chars")
                
        except Exception as e:
            logging.error(f"Error in chat_stream_with_reasoning: {e}")
            # Fallback to regular streaming
            logging.info("Falling back to regular chat streaming")
            for chunk in self.chat_stream(messages, option):
                yield chunk

    def model_name(self) -> str:
        return self.name

    def __str__(self):
        return self.model_name() or f"{self.__class__.__name__}({self.provider})"

    # search å·¥å…·çš„å…·ä½“å®ç°ï¼Œè¿™é‡Œæˆ‘ä»¬åªéœ€è¦è¿”å›å‚æ•°å³å¯
    def search_impl(self, arguments: Dict[str, Any]) -> Any:
        """
        ä½†å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œå¹¶ä¿ç•™è”ç½‘æœç´¢çš„åŠŸèƒ½ï¼Œé‚£ä½ åªéœ€è¦ä¿®æ”¹è¿™é‡Œçš„å®ç°ï¼ˆä¾‹å¦‚è°ƒç”¨æœç´¢
        å’Œè·å–ç½‘é¡µå†…å®¹ç­‰ï¼‰ï¼Œå‡½æ•°ç­¾åä¸å˜ï¼Œä¾ç„¶æ˜¯ work çš„ã€‚

        è¿™æœ€å¤§ç¨‹åº¦ä¿è¯äº†å…¼å®¹æ€§ï¼Œå…è®¸ä½ åœ¨ä¸åŒçš„æ¨¡å‹é—´åˆ‡æ¢ï¼Œå¹¶ä¸”ä¸éœ€è¦å¯¹ä»£ç æœ‰ç ´åæ€§çš„ä¿®æ”¹ã€‚
        """
        return arguments


class DeepSeek(ChatModel):
    def __init__(self, name="deepseek-chat", sk=""):
        super().__init__(name=name, sk=sk, base_url="https://api.deepseek.com", provider="deepseek")


class Tongyi(ChatModel):
    def __init__(self, name="qwen-max", sk=""):
        super().__init__(
            name=name,
            sk=sk,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            provider="tongyi",
        )


class OpenRouter(ChatModel):
    def __init__(self, name="openrouter-chat", sk=""):
        super().__init__(
            name=name,
            sk=sk,
            base_url="https://openrouter.ai/api/v1",
            provider="openrouter",
        )


class Ollama(ChatModel):
    def __init__(self, name="qwen:7b", sk="ollama-local", base_url="http://localhost:11434"):
        # Ollamaä¸éœ€è¦çœŸå®çš„API keyï¼Œä½¿ç”¨å ä½ç¬¦
        super().__init__(
            name=name,
            sk=sk,
            base_url=f"{base_url}/v1",
            provider="ollama",
        )
    
    def model_name(self) -> str:
        return self.name
