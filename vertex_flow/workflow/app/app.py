import argparse
import json
import threading
import traceback
from typing import Any, Dict, List, Optional

import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.app.finance_message_workflow import create_finance_message_workflow
from vertex_flow.workflow.constants import (
    CONTENT_KEY,
    ENABLE_REASONING_KEY,
    ENABLE_STREAM,
    ERROR_KEY,
    MESSAGE_KEY,
    MESSAGE_TYPE_END,
    MESSAGE_TYPE_ERROR,
    MESSAGE_TYPE_REGULAR,
    OUTPUT_KEY,
    SHOW_REASONING_KEY,
    SYSTEM,
    TYPE_KEY,
    USER,
    VERTEX_ID_KEY,
)
from vertex_flow.workflow.context import WorkflowContext
from vertex_flow.workflow.dify_workflow import get_dify_workflow_instances
from vertex_flow.workflow.event_channel import EventType
from vertex_flow.workflow.service import VertexFlowService
from vertex_flow.workflow.tools.functions import FunctionTool
from vertex_flow.workflow.utils import default_config_path
from vertex_flow.workflow.workflow import Any, LLMVertex, SinkVertex, SourceVertex, Workflow
from vertex_flow.workflow.workflow_instance import WorkflowInstance

# MCPç›¸å…³å¯¼å…¥
try:
    from vertex_flow.workflow.mcp_manager import get_mcp_manager
    from vertex_flow.workflow.vertex.mcp_llm_vertex import MCPLLMVertex

    MCP_AVAILABLE = True
except ImportError as e:
    LoggerUtil.get_logger(__name__).warning(f"MCP functionality not available: {e}")
    MCPLLMVertex = None
    get_mcp_manager = None
    MCP_AVAILABLE = False

logger = LoggerUtil.get_logger(__name__)

vertex_flow = FastAPI(title="Vertex Flow API", version="1.0.0")

# å…¨å±€å˜é‡
dify_workflow_instances = {}
workflow_instances = {}  # å­˜å‚¨workflowå®ä¾‹


class WorkflowInput(BaseModel):
    workflow_name: str
    env_vars: Dict[str, Any] = {}  # ç¯å¢ƒå˜é‡ï¼Œç”¨äºæ¨¡æ¿æ›¿æ¢
    user_vars: Dict[str, Any] = {}  # ç”¨æˆ·å˜é‡ï¼Œç”¨äºæ¨¡æ¿æ›¿æ¢
    content: str = ""  # ç”¨æˆ·è¾“å…¥å†…å®¹
    image_url: Optional[str] = None  # å›¾ç‰‡URLï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥
    stream: bool = False  # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
    enable_mcp: bool = True  # æ˜¯å¦å¯ç”¨MCPåŠŸèƒ½

    # LLMé…ç½®å‚æ•°ï¼ˆä¸user_varsåˆ†ç¦»ï¼‰
    system_prompt: Optional[str] = None  # ç³»ç»Ÿæç¤ºè¯
    enable_reasoning: bool = False  # æ˜¯å¦å¯ç”¨æ¨ç†è¿‡ç¨‹
    show_reasoning: bool = True  # æ˜¯å¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
    temperature: Optional[float] = None  # æ¸©åº¦å‚æ•°
    max_tokens: Optional[int] = None  # æœ€å¤§tokenæ•°
    enable_search: bool = True  # æ–°å¢ï¼šé€šä¹‰åƒé—®è”ç½‘æœç´¢å¢å¼ºï¼Œé»˜è®¤ä¸ºTrue


class WorkflowOutput(BaseModel):
    output: Any
    status: bool = False
    message: str = ""
    vertices_status: Dict[str, Any] = {}
    token_usage: Dict[str, Any] = {}  # æ·»åŠ tokenä½¿ç”¨ç»Ÿè®¡
    total_token_usage: Dict[str, Any] = {}  # æ·»åŠ æ€»tokenä½¿ç”¨ç»Ÿè®¡ï¼ˆå¤šè½®å¯¹è¯ï¼‰


class WorkflowInstanceManager:
    """ç®¡ç†workflowå®ä¾‹ï¼Œé¿å…é‡å¤è¿è¡Œ"""

    def __init__(self):
        self.instances = {}

    def create_instance(self, workflow: Workflow, input_data: Dict[str, Any] = None) -> WorkflowInstance:
        """åˆ›å»ºæ–°çš„workflowå®ä¾‹"""
        # åˆ›å»ºworkflowæ¨¡æ¿æ•°æ®
        workflow_template = {
            "id": f"workflow_{len(self.instances)}",
            "name": "Dynamic Workflow",
            "nodes": [],
            "edges": [],
        }

        # åˆ›å»ºWorkflowInstance
        instance = WorkflowInstance(workflow_template, input_data, None)
        instance.workflow_obj = workflow  # ç›´æ¥è®¾ç½®workflowå¯¹è±¡

        # å­˜å‚¨å®ä¾‹
        self.instances[instance.id] = instance

        return instance

    def execute_instance(self, workflow: Workflow, input_data: Dict[str, Any] = None, stream: bool = False):
        """æ‰§è¡Œworkflowå®ä¾‹"""
        # åˆ›å»ºæ–°å®ä¾‹
        instance = self.create_instance(workflow, input_data)

        try:
            # ç›´æ¥æ‰§è¡Œworkflowå¯¹è±¡ï¼Œè€Œä¸æ˜¯é€šè¿‡WorkflowInstanceçš„executeæ–¹æ³•
            if stream:
                # æµå¼æ‰§è¡Œ
                return instance.workflow_obj.execute_workflow(input_data, stream=True)
            else:
                # æ™®é€šæ‰§è¡Œ
                result = instance.workflow_obj.execute_workflow(input_data, stream=False)
                # æ”¶é›†è¾“å‡º
                instance.output_data = instance.workflow_obj.context.get_outputs()
                instance.status = "completed"
                return instance.output_data
        except Exception as e:
            logger.error(f"Failed to execute workflow instance: {e}")
            instance.status = "failed"
            instance.error_message = str(e)
            raise


# åˆ›å»ºå…¨å±€çš„WorkflowInstanceManager
workflow_instance_manager = WorkflowInstanceManager()


# æ·»åŠ å…¨å±€å¼‚å¸¸å¤„ç†å™¨
@vertex_flow.exception_handler(Exception)
async def custom_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={
            "message": "Internal server error.",
        },
    )


@vertex_flow.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={"message": exc.detail},
    )


@vertex_flow.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    return JSONResponse(status_code=422, content={"message": f"Validation error: {exc}"})


vertex_service = None


def check_mcp_availability():
    """æ£€æŸ¥MCPåŠŸèƒ½æ˜¯å¦å¯ç”¨"""
    if not MCP_AVAILABLE:
        return False, "MCP modules not available"

    try:
        global vertex_service
        if vertex_service and vertex_service.is_mcp_enabled():
            return True, "MCP is available and enabled"
        else:
            return False, "MCP is not enabled in configuration"
    except Exception as e:
        return False, f"Error checking MCP status: {e}"


def create_llm_vertex(input_data: WorkflowInput, chatmodel, function_tools: List[FunctionTool]):
    """åˆ›å»ºLLM Vertexï¼Œæ ¹æ®MCPå¼€å…³é€‰æ‹©ç±»å‹ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥"""

    # æ„å»ºç”¨æˆ·æ¶ˆæ¯ï¼Œæ”¯æŒå¤šæ¨¡æ€
    user_messages = []

    # å¦‚æœæœ‰å›¾ç‰‡URLï¼Œåˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
    if input_data.image_url:
        if input_data.content and input_data.content.strip():
            # æœ‰æ–‡æœ¬å†…å®¹ï¼Œåˆ›å»ºå¤šæ¨¡æ€æ¶ˆæ¯
            multimodal_content = [
                {"type": "text", "text": input_data.content},
                {"type": "image_url", "image_url": {"url": input_data.image_url}},
            ]
            user_messages = [multimodal_content]
        else:
            # åªæœ‰å›¾ç‰‡ï¼Œåˆ›å»ºçº¯å›¾ç‰‡æ¶ˆæ¯
            multimodal_content = [{"type": "image_url", "image_url": {"url": input_data.image_url}}]
            user_messages = [multimodal_content]
    else:
        # åªæœ‰æ–‡æœ¬å†…å®¹
        if input_data.content and input_data.content.strip():
            user_messages = [input_data.content]
        else:
            # å¦‚æœæ—¢æ²¡æœ‰æ–‡æœ¬ä¹Ÿæ²¡æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤æ¶ˆæ¯
            user_messages = ["è¯·å¸®åŠ©æˆ‘ã€‚"]

    # æ„å»ºLLMå‚æ•°
    llm_params = {
        "model": chatmodel,
        SYSTEM: input_data.system_prompt or "ä½ æ˜¯ä¸€ä¸ªçƒ­æƒ…çš„èŠå¤©æœºå™¨äººã€‚",
        USER: user_messages,
        ENABLE_STREAM: input_data.stream,
        ENABLE_REASONING_KEY: input_data.enable_reasoning,
        SHOW_REASONING_KEY: input_data.show_reasoning,
    }

    # æ·»åŠ å¯é€‰çš„LLMå‚æ•°
    if input_data.temperature is not None:
        llm_params["temperature"] = input_data.temperature
    if input_data.max_tokens is not None:
        llm_params["max_tokens"] = input_data.max_tokens
    llm_params["enable_search"] = input_data.enable_search

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨MCP
    if input_data.enable_mcp:
        mcp_available, mcp_message = check_mcp_availability()
        logger.info(f"MCP status: {mcp_message}")

        if mcp_available and MCPLLMVertex:
            try:
                # åˆ›å»ºMCPå¢å¼ºçš„LLM Vertex
                logger.info("Creating MCP-enhanced LLM Vertex")
                mcp_params = llm_params.copy()
                mcp_params.update(
                    {
                        # MCPç›¸å…³å‚æ•°
                        "mcp_enabled": True,
                        "mcp_context_enabled": True,  # è‡ªåŠ¨åŒ…å«MCPä¸Šä¸‹æ–‡
                        "mcp_tools_enabled": True,  # å¯ç”¨MCPå·¥å…·è°ƒç”¨
                        "mcp_prompts_enabled": True,  # å¯ç”¨MCPæç¤º
                    }
                )

                llm_vertex = MCPLLMVertex(
                    id="llm",
                    params=mcp_params,
                    tools=function_tools,
                )

                # æ·»åŠ MCPçŠ¶æ€ä¿¡æ¯åˆ°æ—¥å¿—
                try:
                    mcp_manager = get_mcp_manager()
                    if mcp_manager:
                        connected_clients = mcp_manager.get_connected_clients()
                        logger.info(f"MCP clients connected: {connected_clients}")
                except Exception as e:
                    logger.warning(f"Could not get MCP manager status: {e}")

                return llm_vertex, "MCP-enhanced LLM Vertex created successfully"

            except Exception as e:
                logger.error(f"Failed to create MCP LLM Vertex: {e}")
                logger.info("Falling back to standard LLM Vertex")
        else:
            logger.warning(f"MCP requested but not available: {mcp_message}")

    # åˆ›å»ºæ ‡å‡†LLM Vertexï¼ˆé»˜è®¤æˆ–fallbackï¼‰
    logger.info("Creating standard LLM Vertex")
    llm_vertex = LLMVertex(
        id="llm",
        params=llm_params,
        tools=function_tools,
    )

    mcp_status = "MCP not requested" if not input_data.enable_mcp else "MCP fallback to standard vertex"
    return llm_vertex, mcp_status


def get_default_workflow(input_data):
    global vertex_service
    vertex_service = vertex_service or VertexFlowService(chatmodel_config)
    data = input_data.dict()
    # åˆ›å»ºä¸Šä¸‹æ–‡
    context = WorkflowContext(input_data.env_vars)

    # åˆ›å»ºå·¥ä½œæµ
    workflow = Workflow(context)

    def add_func(inputs, context=None):
        a = inputs.get("a", 0)
        b = inputs.get("b", 0)
        logger.info(f"add function a={a}, b={b}")
        return {"result": a + b}

    def echo_func(inputs, context=None):
        logger.info(f"echo function msg={inputs}")
        msg = inputs.get("msg", "")
        return {"echo": msg}

    def base1234_convert_func(inputs, context=None):
        """
        å°†åè¿›åˆ¶æ•´æ•°è½¬æ¢ä¸º1234è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œæˆ–å°†1234è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ä¸ºåè¿›åˆ¶æ•´æ•°ã€‚
        è¾“å…¥å‚æ•°ï¼š
          - value: è¦è½¬æ¢çš„å€¼ï¼ˆint æˆ– strï¼‰
          - direction: 'to1234'ï¼ˆåè¿›åˆ¶è½¬1234è¿›åˆ¶ï¼‰æˆ– 'to10'ï¼ˆ1234è¿›åˆ¶è½¬åè¿›åˆ¶ï¼‰
        """
        value = inputs.get("value")
        direction = inputs.get("direction", "to1234")
        digits = "1234"
        if direction == "to1234":
            try:
                n = int(value)
                if n == 0:
                    return {"result": "1"}
                res = ""
                while n > 0:
                    res = digits[n % 4] + res
                    n //= 4
                return {"result": res + "XXX"}
            except Exception as e:
                return {"error": str(e)}
        elif direction == "to10":
            try:
                s = str(value)
                n = 0
                for c in s:
                    n = n * 4 + (digits.index(c))
                return {"result": n}
            except Exception as e:
                return {"error": str(e)}
        else:
            return {"error": 'direction must be "to1234" or "to10"'}

    function_tools = [
        FunctionTool(
            name="add",
            description="ä¸¤ä¸ªæ•°å­—ç›¸åŠ ï¼Œè¿”å›å®ƒä»¬çš„å’Œã€‚",
            func=add_func,
            schema={
                "type": "object",
                "properties": {
                    "a": {"type": "number", "description": "ç¬¬ä¸€ä¸ªæ•°å­—"},
                    "b": {"type": "number", "description": "ç¬¬äºŒä¸ªæ•°å­—"},
                },
                "required": ["a", "b"],
            },
        ),
        FunctionTool(
            name="echo",
            description="å›æ˜¾è¾“å…¥çš„å­—ç¬¦ä¸²ã€‚",
            func=echo_func,
            schema={
                "type": "object",
                "properties": {"msg": {"type": "string", "description": "è¦å›æ˜¾çš„å†…å®¹"}},
                "required": ["msg"],
            },
        ),
        FunctionTool(
            name="base1234_convert",
            description="åè¿›åˆ¶ä¸1234è¿›åˆ¶äº’è½¬ã€‚directionä¸º'to1234'æ—¶å°†åè¿›åˆ¶æ•´æ•°è½¬ä¸º1234è¿›åˆ¶å­—ç¬¦ä¸²ï¼Œä¸º'to10'æ—¶å°†1234è¿›åˆ¶å­—ç¬¦ä¸²è½¬ä¸ºåè¿›åˆ¶æ•´æ•°ã€‚",
            func=base1234_convert_func,
            schema={
                "type": "object",
                "properties": {
                    "value": {
                        "type": "string",
                        "description": "è¦è½¬æ¢çš„å€¼ï¼Œæ•´æ•°æˆ–1234è¿›åˆ¶å­—ç¬¦ä¸²",
                    },
                    "direction": {
                        "type": "string",
                        "enum": ["to1234", "to10"],
                        "description": "è½¬æ¢æ–¹å‘",
                    },
                },
                "required": ["value", "direction"],
            },
        ),
    ]

    # åˆ›å»ºé¡¶ç‚¹ - æ”¯æŒå¤šæ¨¡æ€è¾“å…¥
    def source_task(inputs, context=None):
        # æ„å»ºå¤šæ¨¡æ€è¾“å…¥æ•°æ®
        input_data_dict = {
            "content": input_data.content,
            "image_url": input_data.image_url,
            **data.get("user_vars", {}),
        }
        # è¿‡æ»¤æ‰Noneå€¼
        return {k: v for k, v in input_data_dict.items() if v is not None}

    source = SourceVertex(id="source", task=source_task)

    # ğŸ†• ä½¿ç”¨æ–°çš„LLM Vertexåˆ›å»ºå‡½æ•°ï¼Œæ”¯æŒMCPå¼€å…³
    llm_vertex, mcp_status = create_llm_vertex(input_data, vertex_service.get_chatmodel(), function_tools)
    logger.info(f"LLM Vertex status: {mcp_status}")

    sink = SinkVertex(id="sink", task=lambda inputs, context: f"Received: {inputs['llm']}")

    # æ·»åŠ é¡¶ç‚¹åˆ°å·¥ä½œæµ
    workflow.add_vertex(source)
    workflow.add_vertex(llm_vertex)
    workflow.add_vertex(sink)

    # è¿æ¥é¡¶ç‚¹
    source | llm_vertex
    llm_vertex | sink
    return workflow


# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥æ‰§è¡Œæ‚¨å¸Œæœ›åœ¨æ–°çº¿ç¨‹ä¸­è¿è¡Œçš„ä»»åŠ¡
def execute_in_thread(workflow, user_vars):
    try:
        workflow.execute_workflow(user_vars, stream=True)
        logger.info("Workflow executed successfully in a separate thread.")
    except Exception as e:
        logger.error(f"Error executing workflow in thread: {e}")


# åœ¨æ‚¨çš„ä»£ç ä¸­åˆ›å»ºå¹¶å¯åŠ¨ä¸€ä¸ªæ–°çº¿ç¨‹
def execute_workflow_in_thread(workflow, user_vars):
    thread = threading.Thread(target=execute_in_thread, args=(workflow, user_vars))
    thread.start()


@vertex_flow.post("/workflow", response_model=WorkflowOutput)
async def execute_workflow_endpoint(request: Request, input_data: WorkflowInput):
    logger.info(f"request data {input_data}")

    # ğŸ†• è®°å½•MCPå¼€å…³çŠ¶æ€
    logger.info(f"MCP enabled: {input_data.enable_mcp}")

    # è®°å½•å¤šæ¨¡æ€è¾“å…¥çŠ¶æ€
    if input_data.image_url:
        logger.info(f"Multimodal input detected: text='{input_data.content}', image_url='{input_data.image_url}'")
    else:
        logger.info(f"Text-only input: '{input_data.content}'")

    workflow_name = input_data.workflow_name
    workflow: Workflow = None
    if workflow_name in dify_workflow_instances:
        logger.info("Build new workflow from graph")
        instance = dify_workflow_instances[input_data.workflow_name]
        # æ ¹æ®å·¥ä½œæµåç§°æ„å»ºä¸åŒçš„workflowå®ä¾‹
        if input_data.workflow_name == "deep-research":
            workflow = instance["builder"]({**input_data.user_vars, **{"stream": input_data.stream}})
        elif input_data.workflow_name == "finance-message":
            workflow = instance["builder"](
                {
                    "content": input_data.content,
                    "image_url": input_data.image_url,  # æ·»åŠ å›¾ç‰‡URLæ”¯æŒ
                    "env_vars": input_data.env_vars,
                    "user_vars": input_data.user_vars,
                    "stream": input_data.stream,
                }
            )
        else:
            workflow = instance["builder"](instance["graph"])
    else:
        logger.info("Build new workflow from code")
        workflow = get_default_workflow(input_data=input_data)

    if input_data.stream:
        # ä½¿ç”¨WorkflowInstanceManageråˆ›å»ºæ–°å®ä¾‹è¿›è¡Œæµå¼æ‰§è¡Œ
        workflow_instance = workflow_instance_manager.create_instance(workflow, input_data.user_vars)

        execute_workflow_in_thread(workflow_instance.workflow_obj, input_data.user_vars)

        async def result_generator():
            try:
                async for result in workflow_instance.workflow_obj.astream([EventType.MESSAGES, EventType.UPDATES]):
                    logger.info(f"workflow result {result}")
                    if result.get(VERTEX_ID_KEY):
                        # ç»Ÿä¸€å¤„ç†ä¸åŒçš„æ¶ˆæ¯é”®å
                        output_content = result.get(CONTENT_KEY) or result.get(MESSAGE_KEY) or ""
                        # è·å–æ¶ˆæ¯ç±»å‹ï¼Œç”¨äºå‰ç«¯åŒºåˆ†æ˜¾ç¤º
                        message_type = result.get(TYPE_KEY, MESSAGE_TYPE_REGULAR)

                        # æ£€æŸ¥æ˜¯å¦ä¸ºæµå¼ç»“æŸæ¶ˆæ¯ï¼Œé™„åŠ usage
                        if message_type == MESSAGE_TYPE_END:
                            token_usage = {}
                            total_token_usage = {}
                            try:
                                if hasattr(workflow, "vertices"):
                                    for vertex in workflow.vertices.values():
                                        if hasattr(vertex, "task_type") and vertex.task_type == "LLM":
                                            if hasattr(vertex, "token_usage") and vertex.token_usage:
                                                token_usage = vertex.token_usage
                                            if hasattr(vertex, "get_total_usage"):
                                                total_token_usage = vertex.get_total_usage()
                                            break
                            except Exception as e:
                                logger.warning(f"Could not collect token usage: {e}")
                            yield json.dumps(
                                {
                                    VERTEX_ID_KEY: result[VERTEX_ID_KEY],
                                    OUTPUT_KEY: output_content,
                                    TYPE_KEY: message_type,
                                    "status": True,
                                    "token_usage": token_usage,
                                    "total_token_usage": total_token_usage,
                                },
                                ensure_ascii=False,
                            ) + "\n"
                        else:
                            yield json.dumps(
                                {
                                    VERTEX_ID_KEY: result[VERTEX_ID_KEY],
                                    OUTPUT_KEY: output_content,
                                    TYPE_KEY: message_type,
                                    "status": True,
                                },
                                ensure_ascii=False,
                            ) + "\n"
            except BaseException as e:
                logger.info(f"workflow run exception {e}")
                traceback.print_exc()
                yield json.dumps(
                    {
                        OUTPUT_KEY: ERROR_KEY,
                        "status": False,
                        MESSAGE_KEY: str(e),
                        "vertices_status": workflow_instance.workflow_obj.status(),
                    }
                ) + "\n"

        return StreamingResponse(
            result_generator(),
            media_type="application/json",
            headers={"Cache-Control": "no-cache", "Connection": "keep-alive", "X-Accel-Buffering": "no"},
        )
    else:
        # æ™®é€šå“åº”
        try:
            # ä½¿ç”¨WorkflowInstanceManageræ‰§è¡Œï¼Œé¿å…é‡å¤è¿è¡Œ
            result = workflow_instance_manager.execute_instance(workflow, input_data.user_vars, stream=False)

            # æ”¶é›†tokenç»Ÿè®¡ä¿¡æ¯
            token_usage = {}
            total_token_usage = {}
            try:
                # ç›´æ¥ä»workflowå¯¹è±¡ä¸­è·å–LLM vertexçš„tokenç»Ÿè®¡
                if hasattr(workflow, "vertices"):
                    for vertex in workflow.vertices.values():
                        if hasattr(vertex, "task_type") and vertex.task_type == "LLM":
                            if hasattr(vertex, "token_usage") and vertex.token_usage:
                                token_usage = vertex.token_usage
                            if hasattr(vertex, "get_total_usage"):
                                total_token_usage = vertex.get_total_usage()
                            # æ–°å¢æ—¥å¿—
                            logger.info(f"LLM Vertex token usage: {token_usage}, total usage: {total_token_usage}")
                            break
            except Exception as e:
                logger.warning(f"Could not collect token usage: {e}")

            return WorkflowOutput(
                output=result,
                status=True,
                message="Workflow executed successfully",
                vertices_status={},
                token_usage=token_usage,
                total_token_usage=total_token_usage,
            )
        except BaseException as e:
            logger.info(f"workflow run exception {e}")
            traceback.print_exc()
            return {
                OUTPUT_KEY: ERROR_KEY,
                "status": False,
                MESSAGE_KEY: str(e),
                "vertices_status": workflow.status(),
            }


# ğŸ†• æ–°å¢MCPçŠ¶æ€æ£€æŸ¥ç«¯ç‚¹
@vertex_flow.get("/mcp/status")
async def get_mcp_status():
    """è·å–MCPçŠ¶æ€ä¿¡æ¯"""
    mcp_available, mcp_message = check_mcp_availability()

    status_info = {"mcp_available": mcp_available, "message": mcp_message, "modules_loaded": MCP_AVAILABLE}

    if mcp_available:
        try:
            mcp_manager = get_mcp_manager()
            if mcp_manager:
                connected_clients = mcp_manager.get_connected_clients()
                status_info["connected_clients"] = connected_clients
                status_info["client_count"] = len(connected_clients)
        except Exception as e:
            status_info["manager_error"] = str(e)

    return status_info


@vertex_flow.get("/workflow", response_model=WorkflowOutput)
async def execute_workflow_endpoint(request: Request):
    workflow_name = request.query_params.get("name")
    if workflow_name and workflow_name in dify_workflow_instances:
        return {
            "output": f"{dify_workflow_instances[workflow_name].get('graph')}",
            "status": True,
        }
    else:
        return {"output": list(dify_workflow_instances.keys()), "status": True}


class VectorSinkData(BaseModel):
    user_id: str
    vertexflow_id: str
    content: str
    options: Dict[str, Any]


class VectorSinkResponse(BaseModel):
    output: Any
    status: bool = False
    message: str = ""


class VectorSearchData(BaseModel):
    user_id: str
    vertexflow_id: str
    content: str
    options: Dict[str, Any]


class VectorSearchResponse(BaseModel):
    content: List[str]
    status: bool = False
    message: str = ""


@vertex_flow.post("/vector/ingest", response_model=VectorSinkData)
async def execute_workflow_endpoint(request: Request):
    return VectorSinkResponse(output="hello world")


@vertex_flow.post("/vector/search", response_model=VectorSearchData)
async def execute_workflow_endpoint(request: Request):
    return VectorSearchData(output="hello world")


chatmodel_config = None


@vertex_flow.on_event("startup")
async def on_startup():
    logger.info(f"Application startup, config file : ${chatmodel_config}")
    vertex_service = VertexFlowService(chatmodel_config) if chatmodel_config is not None else VertexFlowService()
    global dify_workflow_instances
    dify_workflow_instances = get_dify_workflow_instances(vertex_service=vertex_service)
    # æ³¨å†Œfinance-message workflow
    dify_workflow_instances["finance-message"] = {
        "builder": create_finance_message_workflow(vertex_service),
        "graph": None,
    }
    logger.info(f"Application startup, finished, loaded {len(dify_workflow_instances)}...")


def main():
    parser = argparse.ArgumentParser(description="vertex-flow app")

    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        "--config",
        default=None,  # æ”¹ä¸ºNoneï¼Œè®©VertexFlowServiceè‡ªåŠ¨é€‰æ‹©é…ç½®æ–‡ä»¶
        help="æŒ‡å®šæ¨¡å‹ä¸è¯·æ±‚é…ç½®",
    )

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parser.parse_args()
    global chatmodel_config
    chatmodel_config = args.config
    global vertex_service

    vertex_service = VertexFlowService(chatmodel_config)

    import uvicorn

    uvicorn.run(
        "vertex_flow.workflow.app.app:vertex_flow",
        host=vertex_service.get_service_host(),
        port=vertex_service.get_service_port(),
        reload=True,
    )


if __name__ == "__main__":
    main()
