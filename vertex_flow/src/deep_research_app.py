#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ·±åº¦ç ”ç©¶å·¥ä½œæµ Gradio åº”ç”¨
åŸºäº DeepResearchWorkflow çš„äº¤äº’å¼æ·±åº¦ç ”ç©¶åˆ†æå·¥å…·
"""

import argparse
import asyncio
import json
import os
import re
import sys
import threading
from datetime import datetime
from typing import Any, Dict, List, Tuple

import gradio as gr
import nest_asyncio
from concurrent.futures import ThreadPoolExecutor
import time

from vertex_flow.utils.logger import setup_logger
from vertex_flow.workflow.app.deep_research_workflow import DeepResearchWorkflow
from vertex_flow.workflow.service import VertexFlowService
from vertex_flow.workflow.event_channel import EventType
from vertex_flow.workflow.constants import (
    WORKFLOW_COMPLETE, 
    WORKFLOW_FAILED,
    CONTENT_KEY,
    MESSAGE_KEY,
    VERTEX_ID_KEY,
    TYPE_KEY,
    MESSAGE_TYPE_REGULAR,
    MESSAGE_TYPE_REASONING,
    MESSAGE_TYPE_ERROR,
    MESSAGE_TYPE_END
)

# åº”ç”¨nest_asyncioä»¥æ”¯æŒåµŒå¥—äº‹ä»¶å¾ªç¯
nest_asyncio.apply()

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


class DeepResearchApp:
    """æ·±åº¦ç ”ç©¶å·¥ä½œæµ Gradio åº”ç”¨"""
    
    # ç±»å¸¸é‡ï¼šé˜¶æ®µæ˜ å°„é…ç½®
    STAGE_MAPPING = {
        "topic_analysis": ("ä¸»é¢˜åˆ†æ", "ğŸ”"),
        "research_planning": ("ç ”ç©¶è§„åˆ’", "ğŸ“‹"),
        "information_collection": ("ä¿¡æ¯æ”¶é›†", "ğŸ“š"),
        "deep_analysis": ("æ·±åº¦åˆ†æ", "ğŸ”¬"),
        "cross_validation": ("äº¤å‰éªŒè¯", "âœ…"),
        "summary_report": ("æ€»ç»“æŠ¥å‘Š", "ğŸ“„")
    }
    
    # é˜¶æ®µé¡ºåºåˆ—è¡¨
    STAGE_ORDER = ["topic_analysis", "research_planning", "information_collection", 
                   "deep_analysis", "cross_validation", "summary_report"]
    
    # Markdownæ£€æµ‹æ¨¡å¼
    MARKDOWN_PATTERNS = [
        r'^#+\s',           # æ ‡é¢˜ # ## ###
        r'\*\*.*\*\*',      # ç²—ä½“ **text**
        r'\*.*\*',          # æ–œä½“ *text*
        r'^-\s+',           # åˆ—è¡¨é¡¹ - item
        r'^\d+\.\s+',       # ç¼–å·åˆ—è¡¨ 1. item
        r'\[.*\]\(.*\)',    # é“¾æ¥ [text](url)
        r'```',             # ä»£ç å— ```
        r'`.*`',            # è¡Œå†…ä»£ç  `code`
        r'^\|.*\|',         # è¡¨æ ¼ |col1|col2|
        r'^>',              # å¼•ç”¨ > text
    ]
    
    # ç•Œé¢å¸¸é‡
    CONTENT_PREVIEW_LENGTH = 2000  # å†…å®¹é¢„è§ˆé•¿åº¦
    STREAM_CONTENT_LENGTH = 1500   # æµå¼å†…å®¹æ˜¾ç¤ºé•¿åº¦
    FINAL_REPORT_MIN_LENGTH = 100  # æœ€ç»ˆæŠ¥å‘Šæœ€å°é•¿åº¦
    LONG_TEXT_MIN_LENGTH = 100     # é•¿æ–‡æœ¬æœ€å°é•¿åº¦
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–åº”ç”¨"""
        try:
            # ä½¿ç”¨ä¿®æ”¹åçš„VertexFlowServiceï¼Œå®ƒä¼šè‡ªåŠ¨é€‰æ‹©ç”¨æˆ·é…ç½®æ–‡ä»¶
            if config_path is None:
                logger.info("ä½¿ç”¨è‡ªåŠ¨é…ç½®é€‰æ‹©ï¼ˆä¼˜å…ˆç”¨æˆ·é…ç½®æ–‡ä»¶ï¼‰")
                self.service = VertexFlowService()  # ä¸ä¼ é€’config_pathï¼Œè®©å®ƒè‡ªåŠ¨é€‰æ‹©
            else:
                config_path = os.path.abspath(config_path)
                logger.info(f"ä½¿ç”¨æŒ‡å®šé…ç½®è·¯å¾„: {config_path}")
                self.service = VertexFlowService(config_path)
            
            self.workflow_builder = DeepResearchWorkflow(self.service)
            self.current_workflow = None
            
            # åˆå§‹åŒ–é˜¶æ®µå†å²è®°å½•
            self.stage_history = {}
            self.completed_stages = set()
            self.current_research_topic = ""
            self.workflow_running = False
            
            self._initialize_llm()
            logger.info("Deep Research åº”ç”¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"Deep Research åº”ç”¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹"""
        try:
            self.llm_model = self.service.get_chatmodel()
            if self.llm_model is None:
                raise ValueError("æ— æ³•è·å–èŠå¤©æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            
            try:
                model_name = self.llm_model.model_name()
            except:
                model_name = str(self.llm_model)
            
            logger.info(f"æˆåŠŸåˆå§‹åŒ–èŠå¤©æ¨¡å‹: {model_name}")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {e}")
            raise
    
    def start_research(self, research_topic: str, save_intermediate: bool, save_final_report: bool, enable_stream: bool):
        """å¼€å§‹æ·±åº¦ç ”ç©¶"""
        if not research_topic.strip():
            yield "âŒ è¯·è¾“å…¥ç ”ç©¶ä¸»é¢˜", "", "", [], gr.update()
            return
        
        try:
            # é‡ç½®é˜¶æ®µå†å²è®°å½•
            self.stage_history = {}
            self.completed_stages = set()
            self.current_research_topic = research_topic.strip()
            self.workflow_running = True
            
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_data = {
                "content": research_topic.strip(),
                "stream": enable_stream,
                "save_intermediate": save_intermediate,
                "save_final_report": save_final_report,
                "env_vars": {},
                "user_vars": {}
            }
            
            logger.info(f"å¼€å§‹æ·±åº¦ç ”ç©¶: {research_topic}")
            
            # åˆ›å»ºå·¥ä½œæµ
            self.current_workflow = self.workflow_builder.create_workflow(input_data)
            
            if enable_stream:
                yield from self._execute_workflow_stream(input_data, research_topic)
            else:
                yield from self._execute_workflow_batch(input_data, research_topic)
                
        except Exception as e:
            error_msg = f"ç ”ç©¶æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield error_msg, "", "", [], gr.update()
    
    def _execute_workflow_stream(self, input_data: Dict[str, Any], research_topic: str):
        """æµå¼æ‰§è¡Œå·¥ä½œæµ"""
        try:
            # åˆå§‹åŒ–çŠ¶æ€
            self.workflow_running = True
            self.stage_history = {}
            
            # å‘é€å¼€å§‹çŠ¶æ€
            yield "ğŸš€ å¼€å§‹æ·±åº¦ç ”ç©¶åˆ†æ...", "å‡†å¤‡ä¸­...", "æ­£åœ¨åˆå§‹åŒ–å·¥ä½œæµ", [], gr.update()
            
            # è®¢é˜…å·¥ä½œæµäº‹ä»¶
            def on_vertex_complete(event_data):
                """å¤„ç†é¡¶ç‚¹å®Œæˆäº‹ä»¶ï¼ˆvaluesç±»å‹ï¼‰"""
                try:
                    vertex_id = event_data.get(VERTEX_ID_KEY)
                    output = event_data.get('output', '')
                    
                    if vertex_id and vertex_id not in self.stage_history:
                        # æ–°å®Œæˆçš„é˜¶æ®µ
                        stage_name = self.STAGE_MAPPING.get(vertex_id, (vertex_id, "ğŸ“"))[0]
                        stage_icon = self.STAGE_MAPPING.get(vertex_id, (vertex_id, "ğŸ“"))[1]
                        
                        self.stage_history[vertex_id] = {
                            'name': stage_name,
                            'icon': stage_icon,
                            'content': str(output),
                            'status': 'completed',
                            'cost_time': 0,
                            'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                        
                        logger.info(f"é˜¶æ®µå®Œæˆäº‹ä»¶: {vertex_id} - {stage_name}")
                except Exception as e:
                    logger.error(f"å¤„ç†é¡¶ç‚¹å®Œæˆäº‹ä»¶å¤±è´¥: {e}")
            
            def on_stream_message(event_data):
                """å¤„ç†æµå¼æ¶ˆæ¯äº‹ä»¶ï¼ˆmessagesç±»å‹ï¼‰"""
                try:
                    vertex_id = event_data.get(VERTEX_ID_KEY)
                    # ç»Ÿä¸€å¤„ç†ä¸åŒçš„æ¶ˆæ¯é”®åï¼Œæ”¯æŒå‘åå…¼å®¹
                    message = event_data.get(CONTENT_KEY) or event_data.get(MESSAGE_KEY) or ""
                    status = event_data.get('status')
                    message_type = event_data.get(TYPE_KEY, MESSAGE_TYPE_REGULAR)
                    
                    if status == 'end':
                        logger.info(f"é¡¶ç‚¹ {vertex_id} æµå¼è¾“å‡ºç»“æŸ")
                    elif message and vertex_id in self.STAGE_MAPPING:
                        # å®æ—¶æ˜¾ç¤ºæµå¼å†…å®¹
                        stage_name = self.STAGE_MAPPING[vertex_id][0]
                        logger.info(f"æµå¼æ¶ˆæ¯: {stage_name} - {message[:100]}...")
                        
                        # æ›´æ–°å½“å‰é˜¶æ®µçš„æµå¼å†…å®¹
                        if vertex_id not in self.stage_history:
                            self.stage_history[vertex_id] = {
                                'name': stage_name,
                                'icon': self.STAGE_MAPPING[vertex_id][1],
                                'content': message,
                                'status': 'streaming',
                                'cost_time': 0,
                                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            }
                        else:
                            # è¿½åŠ æµå¼å†…å®¹
                            self.stage_history[vertex_id]['content'] += message
                except Exception as e:
                    logger.error(f"å¤„ç†æµå¼æ¶ˆæ¯äº‹ä»¶å¤±è´¥: {e}")
            
            def on_workflow_update(event_data):
                """å¤„ç†å·¥ä½œæµæ›´æ–°äº‹ä»¶ï¼ˆupdatesç±»å‹ï¼‰"""
                try:
                    vertex_id = event_data.get(VERTEX_ID_KEY)
                    status = event_data.get('status')
                    
                    if status == 'failed':
                        logger.error(f"é¡¶ç‚¹æ‰§è¡Œå¤±è´¥: {vertex_id}")
                    elif status == 'workflow_complete':
                        logger.info("å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
                        self.workflow_running = False
                    elif status == 'workflow_failed':
                        logger.error("å·¥ä½œæµæ‰§è¡Œå¤±è´¥")
                        self.workflow_running = False
                except Exception as e:
                    logger.error(f"å¤„ç†å·¥ä½œæµæ›´æ–°äº‹ä»¶å¤±è´¥: {e}")
            
            # æ³¨å†Œäº‹ä»¶å›è°ƒ
            self.current_workflow.subscribe("values", on_vertex_complete)      # é¡¶ç‚¹å®Œæˆäº‹ä»¶
            self.current_workflow.subscribe("messages", on_stream_message)    # æµå¼æ¶ˆæ¯äº‹ä»¶
            self.current_workflow.subscribe("updates", on_workflow_update)    # å·¥ä½œæµçŠ¶æ€æ›´æ–°äº‹ä»¶
            
            # ä½¿ç”¨åŒæ­¥æ–¹å¼æ‰§è¡Œå·¥ä½œæµï¼Œä½†å¯ç”¨æµå¼æ¨¡å¼
            def run_workflow():
                try:
                    # æ‰§è¡Œå·¥ä½œæµï¼Œå¯ç”¨æµå¼æ¨¡å¼
                    self.current_workflow.execute_workflow(input_data, stream=True)
                    self.workflow_running = False
                except Exception as e:
                    logger.error(f"å·¥ä½œæµæ‰§è¡Œé”™è¯¯: {e}")
                    self.workflow_running = False
            
            # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨å·¥ä½œæµ
            workflow_thread = threading.Thread(target=run_workflow)
            workflow_thread.daemon = True
            workflow_thread.start()
            
            # æµå¼ç›‘æ§å·¥ä½œæµè¿›åº¦
            last_status = None
            last_stage_buttons = []
            last_progress = ""
            last_content = ""
            
            while self.workflow_running:
                try:
                    # åˆ›å»ºé˜¶æ®µæŒ‰é’®
                    current_stage_buttons = self._create_stage_buttons()
                    
                    # ç”Ÿæˆè¿›åº¦ä¿¡æ¯
                    completed_stages = len([s for s in self.stage_history.values() if s['status'] == 'completed'])
                    streaming_stages = len([s for s in self.stage_history.values() if s['status'] == 'streaming'])
                    total_stages = len(self.STAGE_ORDER)
                    progress_text = f"å·²å®Œæˆ {completed_stages}/{total_stages} ä¸ªé˜¶æ®µ"
                    
                    if streaming_stages > 0:
                        progress_text += f" (æ­£åœ¨æ‰§è¡Œ: {streaming_stages})"
                    
                    if self.stage_history:
                        # æ˜¾ç¤ºæœ€æ–°æ´»åŠ¨çš„é˜¶æ®µå†…å®¹
                        latest_stage_id = list(self.stage_history.keys())[-1]
                        latest_stage = self.stage_history[latest_stage_id]
                        current_content = self._format_content_for_display(
                            latest_stage['content'], 'markdown', False
                        )
                        
                        if latest_stage['status'] == 'completed':
                            status_msg = f"âœ… {latest_stage['name']} å®Œæˆ"
                        else:
                            status_msg = f"ğŸ”„ {latest_stage['name']} æ‰§è¡Œä¸­..."
                    else:
                        current_content = "æ­£åœ¨æ‰§è¡Œå·¥ä½œæµ..."
                        status_msg = "ğŸš€ å·¥ä½œæµæ‰§è¡Œä¸­..."
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç•Œé¢
                    if (status_msg != last_status or 
                        current_stage_buttons != last_stage_buttons or
                        progress_text != last_progress or
                        current_content != last_content):
                        
                        yield status_msg, current_content, progress_text, current_stage_buttons, gr.update()
                        last_status = status_msg
                        last_stage_buttons = current_stage_buttons.copy()
                        last_progress = progress_text
                        last_content = current_content
                    
                    time.sleep(0.3)  # æ¯300msæ£€æŸ¥ä¸€æ¬¡çŠ¶æ€ï¼Œæé«˜å“åº”é€Ÿåº¦
                    
                except Exception as e:
                    logger.error(f"ç›‘æ§å·¥ä½œæµçŠ¶æ€æ—¶å‡ºé”™: {e}")
                    time.sleep(1)
            
            # å·¥ä½œæµå®Œæˆï¼Œè·å–æœ€ç»ˆç»“æœ
            try:
                results = self.current_workflow.result()
                if results and 'sink' in results:
                    final_report = results['sink'].get('final_report', 'æ²¡æœ‰ç”ŸæˆæŠ¥å‘Š')
                    file_path = results['sink'].get('file_path', '')
                    
                    # æ ¼å¼åŒ–æœ€ç»ˆæŠ¥å‘Š
                    formatted_report = self._format_content_for_display(
                        final_report, 'markdown', True
                    )
                    
                    completion_msg = "âœ… æ·±åº¦ç ”ç©¶å®Œæˆ!"
                    if file_path:
                        completion_msg += f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}"
                    
                    yield completion_msg, formatted_report, f"ç ”ç©¶å®Œæˆï¼Œç”Ÿæˆäº† {len(final_report)} å­—ç¬¦çš„æŠ¥å‘Š", current_stage_buttons, gr.update()
                else:
                    yield "âŒ å·¥ä½œæµæ‰§è¡Œå®Œæˆä½†æ²¡æœ‰è·å–åˆ°ç»“æœ", "", "æ‰§è¡Œå®Œæˆä½†æ— ç»“æœ", current_stage_buttons, gr.update()
            except Exception as e:
                logger.error(f"è·å–ç»“æœå¤±è´¥: {e}")
                yield "âŒ è·å–ç»“æœå¤±è´¥", f"é”™è¯¯: {str(e)}", f"è·å–ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", current_stage_buttons, gr.update()
                
        except Exception as e:
            error_msg = f"âŒ æµå¼æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield error_msg, "æ‰§è¡Œå¤±è´¥", f"æ‰§è¡Œå¤±è´¥: {str(e)}", [], gr.update()
    
    def _execute_workflow_batch(self, input_data: Dict[str, Any], research_topic: str):
        """æ‰¹é‡æ‰§è¡Œå·¥ä½œæµ"""
        try:
            status_msg = "ğŸš€ å¼€å§‹æ·±åº¦ç ”ç©¶åˆ†æ..."
            yield status_msg, "å‡†å¤‡ä¸­...", "å¼€å§‹æ‰§è¡Œæ·±åº¦ç ”ç©¶å·¥ä½œæµ", [], gr.update()
            
            # æ‰§è¡Œå·¥ä½œæµ
            self.current_workflow.execute_workflow(input_data, stream=False)
            
            # è·å–ç»“æœ
            results = self.current_workflow.result()
            
            if results and 'sink' in results:
                final_report = results['sink'].get('final_report', 'æ²¡æœ‰ç”ŸæˆæŠ¥å‘Š')
                file_path = results['sink'].get('file_path', '')
                
                completion_msg = "âœ… æ·±åº¦ç ”ç©¶å®Œæˆ!"
                if file_path:
                    completion_msg += f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}"
                
                yield completion_msg, final_report, f"ç ”ç©¶å®Œæˆï¼Œç”Ÿæˆäº† {len(final_report)} å­—ç¬¦çš„æŠ¥å‘Š", [], gr.update()
            else:
                yield "âŒ å·¥ä½œæµæ‰§è¡Œå®Œæˆä½†æ²¡æœ‰è·å–åˆ°ç»“æœ", "", "æ‰§è¡Œå®Œæˆä½†ç»“æœä¸ºç©º", [], gr.update()
                
        except Exception as e:
            error_msg = f"âŒ æ‰¹é‡æ‰§è¡Œå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            yield error_msg, "", f"æ‰§è¡Œå¤±è´¥: {str(e)}", [], gr.update()
    
    def get_workflow_status(self):
        """è·å–å½“å‰å·¥ä½œæµçŠ¶æ€"""
        if self.current_workflow:
            try:
                status = self.current_workflow.status()
                return f"å·¥ä½œæµçŠ¶æ€: {status}"
            except:
                return "æ— æ³•è·å–å·¥ä½œæµçŠ¶æ€"
        return "æ²¡æœ‰æ­£åœ¨æ‰§è¡Œçš„å·¥ä½œæµ"
    
    def get_stage_content(self, stage_button_text: str) -> str:
        """æ ¹æ®é˜¶æ®µæŒ‰é’®æ–‡æœ¬è·å–é˜¶æ®µå†…å®¹"""
        logger.debug(f"è¯·æ±‚æŸ¥çœ‹é˜¶æ®µ: {stage_button_text}")
        logger.debug(f"å½“å‰é˜¶æ®µå†å²: {list(self.stage_history.keys())}")
        
        if not stage_button_text or not self.stage_history:
            return "è¯·å…ˆè¿è¡Œæ·±åº¦ç ”ç©¶å·¥ä½œæµ"
        
        # è§£ææŒ‰é’®æ–‡æœ¬ï¼Œæå–é˜¶æ®µåç§°
        for stage_id, stage_info in self.stage_history.items():
            expected_button_text = f"{stage_info['icon']} {stage_info['name']}"
            logger.debug(f"æ¯”è¾ƒ: '{expected_button_text}' vs '{stage_button_text}'")
            
            if expected_button_text == stage_button_text:
                logger.debug(f"æ‰¾åˆ°é˜¶æ®µå†…å®¹ï¼Œé•¿åº¦: {len(stage_info['content'])}")
                return self._format_stage_content(stage_info, include_metadata=True)
        
        # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
        for stage_id, stage_info in self.stage_history.items():
            if stage_info['name'] in stage_button_text or stage_button_text in stage_info['name']:
                logger.debug(f"æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ°é˜¶æ®µå†…å®¹: {stage_info['name']}")
                return self._format_stage_content(stage_info, include_metadata=True)
        
        # è¿”å›è°ƒè¯•ä¿¡æ¯
        available_stages = [f"{info['icon']} {info['name']}" for info in self.stage_history.values()]
        debug_info = f"æœªæ‰¾åˆ°é˜¶æ®µå†…å®¹: {stage_button_text}\n\n"
        debug_info += f"å¯ç”¨é˜¶æ®µ:\n" + "\n".join([f"- {stage}" for stage in available_stages])
        debug_info += f"\n\nå½“å‰é˜¶æ®µå†å²è®°å½•æ•°é‡: {len(self.stage_history)}"
        
        return debug_info
    
    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            providers = []
            for provider, provider_config in llm_config.items():
                if isinstance(provider_config, dict):
                    enabled = provider_config.get("enabled", False)
                    status = "âœ…" if enabled else "âŒ"
                    providers.append(f"{status} {provider}")
            return providers
        except Exception as e:
            logger.error(f"è·å–æä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def get_models_by_provider(self, provider: str) -> List[str]:
        """æ ¹æ®æä¾›å•†è·å–å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            provider_config = llm_config.get(provider, {})
            if not provider_config:
                return [f"æœªæ‰¾åˆ°æä¾›å•†: {provider}"]

            models = []
            provider_enabled = provider_config.get("enabled", False)
            
            # æ”¯æŒå¤šæ¨¡å‹ç»“æ„
            if "models" in provider_config:
                models_list = provider_config["models"]
                for model_config in models_list:
                    if isinstance(model_config, dict):
                        model_name = model_config.get("name", "unknown")
                        model_enabled = model_config.get("enabled", False)
                        is_default = model_config.get("default", False)
                        status = "âœ…" if (provider_enabled and model_enabled) else "âŒ"
                        default_mark = " (é»˜è®¤)" if is_default else ""
                        models.append(f"{status} {model_name}{default_mark}")
            else:
                # æ—§æ ¼å¼ï¼šä½¿ç”¨model-name
                model_name = provider_config.get("model-name", provider)
                status = "âœ…" if provider_enabled else "âŒ"
                models.append(f"{status} {model_name}")
            
            return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def switch_model_by_provider_and_name(self, provider: str, model_name: str = None) -> str:
        """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åç§°åˆ‡æ¢æ¨¡å‹"""
        try:
            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            new_model = self.service.get_chatmodel_by_provider(provider, model_name)
            if new_model:
                self.llm_model = new_model
                # é‡æ–°åˆå§‹åŒ–å·¥ä½œæµæ„å»ºå™¨
                self.workflow_builder = DeepResearchWorkflow(self.service)
                
                try:
                    actual_model_name = new_model.model_name()
                except:
                    actual_model_name = str(new_model)
                
                logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {provider} - {actual_model_name}")
                return f"âœ… å·²åˆ‡æ¢åˆ°: {provider} - {actual_model_name}"
            else:
                return f"âŒ æ— æ³•åˆ‡æ¢åˆ°æ¨¡å‹: {provider}"
        except Exception as e:
            logger.error(f"åˆ‡æ¢æ¨¡å‹å¤±è´¥: {e}")
            return f"âŒ åˆ‡æ¢å¤±è´¥: {str(e)}"

    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            models = []
            for provider, provider_config in llm_config.items():
                if isinstance(provider_config, dict):
                    enabled = provider_config.get("enabled", False)
                    
                    # æ”¯æŒå¤šæ¨¡å‹ç»“æ„
                    if "models" in provider_config:
                        models_list = provider_config["models"]
                        for model_config in models_list:
                            if isinstance(model_config, dict):
                                model_name = model_config.get("name", "unknown")
                                model_enabled = model_config.get("enabled", False)
                                status = "âœ…" if (enabled and model_enabled) else "âŒ"
                                models.append(f"{status} {provider}: {model_name}")
                    else:
                        # æ—§æ ¼å¼ï¼šä½¿ç”¨model-name
                        model_name = provider_config.get("model-name", provider)
                        status = "âœ…" if enabled else "âŒ"
                        models.append(f"{status} {provider}: {model_name}")
            return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def switch_model(self, provider: str) -> str:
        """åˆ‡æ¢æ¨¡å‹æä¾›å•†ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self.switch_model_by_provider_and_name(provider)
    
    def is_markdown_content(self, content: str) -> bool:
        """æ™ºèƒ½æ£€æµ‹å†…å®¹æ˜¯å¦åŒ…å«markdownæ ¼å¼"""
        if not content or len(content.strip()) < 10:
            return False
        
        # ä½¿ç”¨ç±»å¸¸é‡æ£€æµ‹markdownç‰¹å¾
        for pattern in self.MARKDOWN_PATTERNS:
            if re.search(pattern, content, re.MULTILINE):
                return True
        
        # æ£€æµ‹å¤šä¸ªæ¢è¡Œç¬¦ï¼ˆmarkdownæ–‡æ¡£ç‰¹å¾ï¼‰
        if content.count('\n\n') >= 2:
            return True
            
        return False
    
    def enhance_markdown_content(self, content: str) -> str:
        """å¢å¼ºmarkdownå†…å®¹çš„æ˜¾ç¤ºæ•ˆæœ"""
        if not content:
            return content
        
        # ä¸ºæ ‡é¢˜æ·»åŠ é€‚å½“çš„é—´è·
        content = re.sub(r'(?<!^)(\n)(#+\s)', r'\n\n\2', content, flags=re.MULTILINE)
        
        # ç¡®ä¿åˆ—è¡¨é¡¹æ ¼å¼æ­£ç¡®
        content = re.sub(r'(\n)([â€¢\-\*]\s)', r'\1\n\2', content)
        
        # ç¡®ä¿ç¼–å·åˆ—è¡¨æ ¼å¼æ­£ç¡®
        content = re.sub(r'(\n)(\d+\.\s)', r'\1\n\2', content)
        
        # ç‰¹æ®Šå¤„ç†Mermaidä»£ç å—
        def process_mermaid_blocks(match):
            lang = match.group(1) or ''
            code = match.group(2)
            
            if lang.lower() in ['mermaid', 'graph', 'flowchart', 'sequence', 'gantt', 'class', 'state', 'pie', 'journey', 'gitgraph']:
                # ä¸ºMermaidä»£ç å—æ·»åŠ ç‰¹æ®Šç±»åå’ŒID
                unique_id = f'mermaid-{hash(code) % 1000000}'
                return f'\n\n<div class="mermaid" id="{unique_id}">\n{code}\n</div>\n\n'
            else:
                # æ™®é€šä»£ç å—ä¿æŒåŸæ ·
                return match.group(0)
        
        # å¤„ç†Mermaidä»£ç å— - ä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
        content = re.sub(r'```(\w+)?\n(.*?)\n```', process_mermaid_blocks, content, flags=re.DOTALL)
        
        # ç¡®ä¿å…¶ä»–ä»£ç å—å‰åæœ‰ç©ºè¡Œ
        content = re.sub(r'(?<!^)(\n)(```)', r'\n\n\2', content)
        content = re.sub(r'(```.*?```\n)(?!\n)', r'\1\n', content, flags=re.DOTALL)
        
        return content.strip()
    
    def _format_stage_content(self, stage_info: Dict[str, str], include_metadata: bool = True) -> str:
        """æ ¼å¼åŒ–é˜¶æ®µå†…å®¹æ˜¾ç¤º"""
        content = stage_info['content']
        
        if include_metadata:
            formatted_content = f"## {stage_info['icon']} {stage_info['name']}\n\n"
            formatted_content += f"**ç ”ç©¶ä¸»é¢˜:** {self.current_research_topic}\n\n"
            formatted_content += f"**å®Œæˆæ—¶é—´:** {stage_info['timestamp']}\n\n"
            formatted_content += f"**å†…å®¹é•¿åº¦:** {len(content)} å­—ç¬¦\n\n"
            formatted_content += "---\n\n"
            formatted_content += content
            return formatted_content
        else:
            return content
    
    def _create_stage_buttons(self) -> List[str]:
        """åˆ›å»ºå·²å®Œæˆé˜¶æ®µçš„æŒ‰é’®åˆ—è¡¨"""
        buttons = []
        logger.debug(f"åˆ›å»ºé˜¶æ®µæŒ‰é’®ï¼Œé˜¶æ®µå†å²è®°å½•: {list(self.stage_history.keys())}")
        
        # æ›´æ–°completed_stagesé›†åˆ
        self.completed_stages = {stage_id for stage_id, stage_info in self.stage_history.items() 
                               if stage_info.get('status') == 'completed'}
        
        for stage_id in self.STAGE_ORDER:
            if stage_id in self.stage_history:
                stage_info = self.stage_history[stage_id]
                button_text = f"{stage_info['icon']} {stage_info['name']}"
                buttons.append(button_text)
                logger.debug(f"æ·»åŠ é˜¶æ®µæŒ‰é’®: {button_text}")
        
        return buttons
    
    def _should_update_stage_selector(self, previous_buttons: List[str], current_buttons: List[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°é˜¶æ®µé€‰æ‹©å™¨ï¼ˆåªæœ‰æ–°å¢é˜¶æ®µæ—¶æ‰æ›´æ–°ï¼‰"""
        return len(current_buttons) > len(previous_buttons)
    
    def _format_content_for_display(self, content: str, format_mode: str, is_final_report: bool = False) -> str:
        """æ ¼å¼åŒ–å†…å®¹ç”¨äºæ˜¾ç¤º"""
        if format_mode == "Markdownæ¸²æŸ“":
            if self.is_markdown_content(content):
                return self.enhance_markdown_content(content)
            elif is_final_report:
                return f"## ğŸ“„ ç ”ç©¶æŠ¥å‘Š\n\n```text\n{content}\n```"
            elif len(content) > self.LONG_TEXT_MIN_LENGTH:
                return f"```\n{content}\n```"
            else:
                return content
        else:
            return content
    
    def _create_gradio_update_tuple(self, status: str, content: str, progress: str, 
                                   stage_buttons: List[str], format_mode: str,
                                   is_final_report: bool = False) -> tuple:
        """åˆ›å»ºæ ‡å‡†çš„Gradioæ›´æ–°å…ƒç»„"""
        if is_final_report:
            formatted_content = self._format_content_for_display(content, format_mode, True)
            if format_mode == "Markdownæ¸²æŸ“":
                return (status, "âœ… ç ”ç©¶å®Œæˆ", progress, 
                       formatted_content, 
                       gr.update(value=content, visible=False),
                       gr.update(visible=True),
                       gr.update(visible=False),
                       gr.update(choices=stage_buttons))
            else:
                return (status, "âœ… ç ”ç©¶å®Œæˆ", progress, 
                       "è¯·åˆ‡æ¢åˆ°åŸå§‹æ–‡æœ¬æ¨¡å¼æŸ¥çœ‹å®Œæ•´å†…å®¹", 
                       gr.update(value=content, visible=True),
                       gr.update(visible=False),
                       gr.update(visible=True),
                       gr.update(choices=stage_buttons))
        else:
            stage_md_display = self._format_content_for_display(content, format_mode)
            return (status, stage_md_display, progress, 
                   gr.update(),  # research_report_md
                   gr.update(),  # research_report_text
                   gr.update(visible=True if format_mode == "Markdownæ¸²æŸ“" else False),
                   gr.update(value=content, visible=True if format_mode == "åŸå§‹æ–‡æœ¬" else False),
                   gr.update(choices=stage_buttons))


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆç”¨äºç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶ï¼‰"""
    parser = argparse.ArgumentParser(description="Deep Research Workflow Gradio App")
    parser.add_argument("--host", default="127.0.0.1", help="ä¸»æœºåœ°å€")
    parser.add_argument("--port", type=int, default=7861, help="ç«¯å£å·")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--config", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--dev", action="store_true", help="å¼€å‘æ¨¡å¼")
    return parser.parse_args()


def create_gradio_interface(app: DeepResearchApp):
    """åˆ›å»º Gradio ç•Œé¢"""
    
    with gr.Blocks(
        title="Deep Research - æ·±åº¦ç ”ç©¶å·¥ä½œæµ",
        theme=gr.themes.Soft(),
        head="""
        <script src="https://cdn.jsdelivr.net/npm/mermaid@10.6.1/dist/mermaid.min.js"></script>
        <script>
            // åˆå§‹åŒ–Mermaid
            mermaid.initialize({
                startOnLoad: true,
                theme: 'default',
                flowchart: {
                    useMaxWidth: true,
                    htmlLabels: true
                }
            });
            
            // è‡ªåŠ¨æ¸²æŸ“Mermaidå›¾è¡¨
            function renderMermaidCharts() {
                const mermaidElements = document.querySelectorAll('.mermaid');
                mermaidElements.forEach((element, index) => {
                    if (!element.hasAttribute('data-processed')) {
                        element.setAttribute('data-processed', 'true');
                        const id = 'mermaid-' + Date.now() + '-' + index;
                        element.id = id;
                        
                        try {
                            mermaid.render(id, element.textContent.trim()).then(({svg}) => {
                                element.innerHTML = svg;
                            }).catch(error => {
                                console.error('Mermaidæ¸²æŸ“é”™è¯¯:', error);
                                element.innerHTML = '<div style="color: red; padding: 10px; border: 1px solid red; background: #ffe6e6;">å›¾è¡¨æ¸²æŸ“å¤±è´¥: ' + error.message + '</div>';
                            });
                        } catch (error) {
                            console.error('Mermaidåˆå§‹åŒ–é”™è¯¯:', error);
                            element.innerHTML = '<div style="color: red; padding: 10px; border: 1px solid red; background: #ffe6e6;">å›¾è¡¨åˆå§‹åŒ–å¤±è´¥: ' + error.message + '</div>';
                        }
                    }
                });
            }
            
            // ç›‘å¬DOMå˜åŒ–ï¼Œè‡ªåŠ¨æ¸²æŸ“æ–°çš„Mermaidå›¾è¡¨
            const observer = new MutationObserver(function(mutations) {
                let shouldRender = false;
                mutations.forEach(function(mutation) {
                    if (mutation.type === 'childList') {
                        mutation.addedNodes.forEach(function(node) {
                            if (node.nodeType === 1 && (node.classList.contains('mermaid') || node.querySelector('.mermaid'))) {
                                shouldRender = true;
                            }
                        });
                    }
                });
                if (shouldRender) {
                    setTimeout(renderMermaidCharts, 100);
                }
            });
            
            observer.observe(document.body, {
                childList: true,
                subtree: true
            });
            
            // é¡µé¢åŠ è½½å®Œæˆååˆå§‹åŒ–
            document.addEventListener('DOMContentLoaded', function() {
                setTimeout(renderMermaidCharts, 500);
            });
            
            // å®šæœŸæ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„Mermaidå›¾è¡¨
            setInterval(renderMermaidCharts, 2000);
        </script>
        """,
        css="""
        .research-container { 
            max-height: 600px; 
            overflow-y: auto; 
            scroll-behavior: smooth;
        }
        .status-info { 
            background-color: #f0f8ff; 
            padding: 10px; 
            border-radius: 5px; 
            margin: 5px 0; 
        }
        .progress-info {
            background-color: #f5f5f5;
            padding: 10px;
            border-radius: 5px;
            font-family: monospace;
            white-space: pre-wrap;
        }
        .report-container {
            max-height: 500px;
            overflow-y: auto;
            border: 1px solid #ddd;
            padding: 15px;
            border-radius: 5px;
            background-color: #fafafa;
        }
        /* Markdownæ¸²æŸ“æ ·å¼ä¼˜åŒ– */
        .report-container h1, .report-container h2, .report-container h3 {
            color: #2563eb;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }
        .report-container h1 {
            border-bottom: 2px solid #e5e7eb;
            padding-bottom: 0.3em;
        }
        .report-container h2 {
            border-bottom: 1px solid #e5e7eb;
            padding-bottom: 0.2em;
        }
        .report-container ul, .report-container ol {
            padding-left: 1.5em;
            margin: 0.5em 0;
        }
        .report-container li {
            margin: 0.2em 0;
        }
        .report-container blockquote {
            border-left: 4px solid #e5e7eb;
            margin: 1em 0;
            padding-left: 1em;
            color: #6b7280;
        }
        .report-container code {
            background-color: #f3f4f6;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: 'Monaco', 'Consolas', monospace;
        }
        .report-container pre {
            background-color: #1f2937;
            color: #f9fafb;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
        }
        .report-container table {
            border-collapse: collapse;
            width: 100%;
            margin: 1em 0;
        }
        .report-container th, .report-container td {
            border: 1px solid #e5e7eb;
            padding: 0.5em;
            text-align: left;
        }
        .report-container th {
            background-color: #f3f4f6;
            font-weight: bold;
        }
        /* Mermaidå›¾è¡¨æ”¯æŒ */
        .mermaid {
            text-align: center;
            margin: 1em 0;
            padding: 1em;
            background-color: #f8fafc;
            border-radius: 8px;
            border: 1px solid #e2e8f0;
        }
        .mermaid svg {
            max-width: 100%;
            height: auto;
        }
        /* é˜¶æ®µé€‰æ‹©å™¨æ ·å¼ */
        .stage-selector {
            background-color: #f8fafc;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 8px;
        }
        .stage-selector label {
            font-weight: 600;
            color: #374151;
        }
        /* æ ‡ç­¾é¡µå†…å®¹æ ·å¼ä¼˜åŒ– */
        .gradio-tabs .gradio-tabitem {
            padding: 20px;
        }
        /* é˜¶æ®µè¯¦æƒ…æ˜¾ç¤ºæ ·å¼ */
        .stage-detail {
            background-color: #fefefe;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            max-height: 600px;
            overflow-y: auto;
        }
        /* æ ‡ç­¾é¡µä¸­çš„æŠ¥å‘Šå®¹å™¨æ ·å¼ */
        .gradio-tabs .report-container {
            max-height: 600px;
            overflow-y: auto;
            border: 1px solid #ddd;
            padding: 20px;
            border-radius: 8px;
            background-color: #fafafa;
            margin-top: 10px;
        }
        """,
    ) as demo:
        
        gr.Markdown("""
        # ğŸ”¬ Deep Research - æ·±åº¦ç ”ç©¶å·¥ä½œæµ
        
        **ä¸“ä¸šçš„è‡ªåŠ¨åŒ–æ·±åº¦ç ”ç©¶åˆ†æå·¥å…·**
        
        æœ¬å·¥å…·é€šè¿‡å…­ä¸ªè¿ç»­çš„åˆ†æé˜¶æ®µï¼Œå¯¹å¤æ‚ä¸»é¢˜è¿›è¡Œå…¨é¢ã€æ·±å…¥çš„ç ”ç©¶å’Œåˆ†æï¼š
        1. ğŸ“Š **ä¸»é¢˜åˆ†æ** - åˆ†æç ”ç©¶ä¸»é¢˜çš„æ ¸å¿ƒå†…å®¹å’Œç ”ç©¶èŒƒå›´
        2. ğŸ“‹ **ç ”ç©¶è§„åˆ’** - åˆ¶å®šè¯¦ç»†çš„ç ”ç©¶è®¡åˆ’å’Œåˆ†ææ¡†æ¶  
        3. ğŸ“š **ä¿¡æ¯æ”¶é›†** - ç³»ç»Ÿæ€§æ”¶é›†åŸºç¡€ä¿¡æ¯å’ŒèƒŒæ™¯èµ„æ–™
        4. ğŸ”¬ **æ·±åº¦åˆ†æ** - è¿›è¡Œè¶‹åŠ¿åˆ†æå’Œå…³è”åˆ†æ
        5. âœ… **äº¤å‰éªŒè¯** - éªŒè¯å…³é”®äº‹å®å’Œæ•°æ®å‡†ç¡®æ€§
        6. ğŸ“„ **æ€»ç»“æŠ¥å‘Š** - æ•´åˆæ‰€æœ‰ç ”ç©¶æˆæœç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                # ä¸»è¦ç ”ç©¶ç•Œé¢
                gr.Markdown("## ğŸ¯ ç ”ç©¶é…ç½®")
                
                research_topic = gr.Textbox(
                    label="ç ”ç©¶ä¸»é¢˜",
                    placeholder="è¯·è¾“å…¥æ‚¨æƒ³è¦æ·±åº¦ç ”ç©¶çš„ä¸»é¢˜ï¼Œä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä¸å‘å±•è¶‹åŠ¿",
                    lines=3,
                    max_lines=5
                )
                
                with gr.Row():
                    save_intermediate = gr.Checkbox(
                        label="ä¿å­˜ä¸­é—´æ–‡æ¡£",
                        value=True,
                        info="ä¿å­˜æ¯ä¸ªé˜¶æ®µçš„åˆ†æç»“æœ"
                    )
                    save_final_report = gr.Checkbox(
                        label="ä¿å­˜æœ€ç»ˆæŠ¥å‘Š",
                        value=True,
                        info="å°†æœ€ç»ˆæŠ¥å‘Šä¿å­˜ä¸ºæ–‡ä»¶"
                    )
                    enable_stream = gr.Checkbox(
                        label="æµå¼æ¨¡å¼",
                        value=True,
                        info="å®æ—¶æ˜¾ç¤ºåˆ†æè¿›åº¦"
                    )
                
                with gr.Row():
                    start_btn = gr.Button("ğŸš€ å¼€å§‹æ·±åº¦ç ”ç©¶", variant="primary", scale=2)
                    status_btn = gr.Button("ğŸ“Š æŸ¥çœ‹çŠ¶æ€", scale=1)
                
                # çŠ¶æ€æ˜¾ç¤º
                gr.Markdown("## ğŸ“ˆ æ‰§è¡ŒçŠ¶æ€")
                
                status_display = gr.Textbox(
                    label="å½“å‰çŠ¶æ€",
                    value="ç­‰å¾…å¼€å§‹ç ”ç©¶...",
                    interactive=False,
                    elem_classes="status-info"
                )
                
                # å½“å‰é˜¶æ®µæ˜¾ç¤ºï¼ˆæ”¯æŒmarkdownï¼‰
                current_stage_md = gr.Markdown(
                    value="**å½“å‰é˜¶æ®µ:** æœªå¼€å§‹",
                    visible=True
                )
                
                current_stage_text = gr.Textbox(
                    label="å½“å‰é˜¶æ®µ",
                    value="æœªå¼€å§‹",
                    interactive=False,
                    visible=False
                )
                
                progress_log = gr.Textbox(
                    label="è¿›åº¦æ—¥å¿—",
                    value="",
                    interactive=False,
                    lines=8,
                    elem_classes="progress-info"
                )
                
                # å·²å®Œæˆé˜¶æ®µå†å²æŸ¥çœ‹
                gr.Markdown("## ğŸ“‹ é˜¶æ®µå†å²")
                stage_selector = gr.Radio(
                    choices=[],
                    label="å·²å®Œæˆé˜¶æ®µ",
                    info="ç‚¹å‡»ç›´æ¥æŸ¥çœ‹é˜¶æ®µè¯¦ç»†å†…å®¹",
                    interactive=True,
                    elem_classes="stage-selector"
                )
            
            with gr.Column(scale=1):
                # æ˜¾ç¤ºæ ¼å¼æ§åˆ¶
                gr.Markdown("## ğŸ¨ æ˜¾ç¤ºè®¾ç½®")
                
                format_toggle = gr.Radio(
                    choices=["Markdownæ¸²æŸ“", "åŸå§‹æ–‡æœ¬"],
                    value="Markdownæ¸²æŸ“",
                    label="æŠ¥å‘Šæ˜¾ç¤ºæ ¼å¼",
                    info="é€‰æ‹©ç ”ç©¶æŠ¥å‘Šçš„æ˜¾ç¤ºæ ¼å¼"
                )
                
                # æ¨¡å‹é…ç½®å’Œç®¡ç†
                gr.Markdown("## âš™ï¸ æ¨¡å‹é…ç½®")
                
                # å®‰å…¨è·å–å½“å‰æ¨¡å‹åç§°
                try:
                    current_model_name = app.llm_model.model_name()
                except:
                    current_model_name = str(app.llm_model)
                
                model_info = gr.Markdown(f"**å½“å‰æ¨¡å‹:** {current_model_name}")
                
                # æ¨¡å‹åˆ‡æ¢ - å…ˆé€‰æ‹©æä¾›å•†ï¼Œå†é€‰æ‹©æ¨¡å‹
                gr.Markdown("#### é€‰æ‹©æä¾›å•†")
                provider_dropdown = gr.Dropdown(
                    label="æä¾›å•†",
                    choices=app.get_available_providers(),
                    interactive=True,
                    info="é€‰æ‹©æä¾›å•†åæ˜¾ç¤ºå¯¹åº”çš„æ¨¡å‹",
                    allow_custom_value=False
                )

                gr.Markdown("#### é€‰æ‹©æ¨¡å‹")
                model_dropdown = gr.Dropdown(
                    label="æ¨¡å‹",
                    choices=[],
                    interactive=True,
                    info="é€‰æ‹©è¦ä½¿ç”¨çš„å…·ä½“æ¨¡å‹",
                    allow_custom_value=False
                )

                with gr.Row():
                    switch_btn = gr.Button("åˆ‡æ¢æ¨¡å‹", variant="primary", scale=1)
                    refresh_btn = gr.Button("åˆ·æ–°", variant="secondary", scale=1)

                switch_result = gr.Textbox(
                    label="åˆ‡æ¢ç»“æœ",
                    interactive=False,
                    lines=2
                )

                # æ‰‹åŠ¨è¾“å…¥æ¨¡å¼ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                with gr.Accordion("ğŸ”§ æ‰‹åŠ¨è¾“å…¥æ¨¡å¼", open=False):
                    model_list = gr.Dropdown(
                        label="å¯ç”¨æ¨¡å‹ï¼ˆæ—§ç‰ˆæ ¼å¼ï¼‰",
                        choices=app.get_available_models(),
                        interactive=False,
                        info="ç³»ç»Ÿä¸­é…ç½®çš„æ‰€æœ‰æ¨¡å‹"
                    )
                    
                    provider_input = gr.Textbox(
                        placeholder="è¾“å…¥æä¾›å•†åç§°åˆ‡æ¢æ¨¡å‹",
                        label="åˆ‡æ¢æ¨¡å‹",
                        info="ä¾‹å¦‚: deepseek, openai, ollama"
                    )
                    
                    manual_switch_btn = gr.Button("æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹")
        
        # å†…å®¹æ˜¾ç¤ºåŒºåŸŸ - ä½¿ç”¨æ ‡ç­¾é¡µç»„ç»‡
        with gr.Tabs():
            with gr.TabItem("ğŸ“„ ç ”ç©¶æŠ¥å‘Š"):
                # Markdownæ¸²æŸ“æ˜¾ç¤ºåŒºåŸŸ
                research_report_md = gr.Markdown(
                    value="ç ”ç©¶æŠ¥å‘Šå°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                    elem_classes="report-container",
                    visible=True
                )
                
                # åŸå§‹æ–‡æœ¬æ˜¾ç¤ºåŒºåŸŸ  
                research_report_text = gr.Textbox(
                    label="æ·±åº¦ç ”ç©¶æŠ¥å‘Šï¼ˆåŸå§‹æ–‡æœ¬ï¼‰",
                    value="ç ”ç©¶æŠ¥å‘Šå°†åœ¨è¿™é‡Œæ˜¾ç¤º...",
                    interactive=False,
                    lines=20,
                    elem_classes="report-container",
                    visible=False
                )
            
            with gr.TabItem("ğŸ” é˜¶æ®µè¯¦æƒ…"):
                stage_detail_display = gr.Markdown(
                    value="ç‚¹å‡»å·¦ä¾§é˜¶æ®µæŒ‰é’®æŸ¥çœ‹è¯¦ç»†å†…å®¹...",
                    elem_classes="report-container"
                )
        
        # ä½¿ç”¨ç¤ºä¾‹
        with gr.Accordion("ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹", open=False):
            gr.Markdown("""
            ### ç ”ç©¶ä¸»é¢˜ç¤ºä¾‹ï¼š
            
            **ç§‘æŠ€ç±»ï¼š**
            - äººå·¥æ™ºèƒ½åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ä¸å‘å±•è¶‹åŠ¿
            - åŒºå—é“¾æŠ€æœ¯åœ¨é‡‘èç§‘æŠ€ä¸­çš„åˆ›æ–°åº”ç”¨
            - é‡å­è®¡ç®—æŠ€æœ¯çš„å‘å±•ç°çŠ¶ä¸æœªæ¥å±•æœ›
            - å¯æŒç»­èƒ½æºæŠ€æœ¯çš„å‘å±•è¶‹åŠ¿
            
            **å•†ä¸šç±»ï¼š**
            - æ–°é›¶å”®æ¨¡å¼çš„å‘å±•è¶‹åŠ¿ä¸æŒ‘æˆ˜
            - è¿œç¨‹åŠå…¬å¯¹ä¼ä¸šç®¡ç†æ¨¡å¼çš„å½±å“
            - æ•°å­—åŒ–è½¬å‹åœ¨ä¼ ç»Ÿåˆ¶é€ ä¸šçš„åº”ç”¨
            
            **ç¤¾ä¼šç±»ï¼š**
            - è€é¾„åŒ–ç¤¾ä¼šçš„æŒ‘æˆ˜ä¸åº”å¯¹ç­–ç•¥
            - æ•°å­—é¸¿æ²Ÿå¯¹ç¤¾ä¼šå…¬å¹³çš„å½±å“
            - åœ¨çº¿æ•™è‚²çš„å‘å±•è¶‹åŠ¿ä¸è´¨é‡ä¿éšœ
            
            ### ä½¿ç”¨å»ºè®®ï¼š
            1. ğŸ“ **æ˜ç¡®ä¸»é¢˜**ï¼šé€‰æ‹©å…·ä½“ã€æœ‰é’ˆå¯¹æ€§çš„ç ”ç©¶ä¸»é¢˜
            2. âš™ï¸ **é€‰æ‹©æ¨¡å¼**ï¼šå»ºè®®å¼€å¯æµå¼æ¨¡å¼æŸ¥çœ‹å®æ—¶è¿›åº¦
            3. ğŸ’¾ **ä¿å­˜è®¾ç½®**ï¼šå»ºè®®ä¿å­˜ä¸­é—´æ–‡æ¡£å’Œæœ€ç»ˆæŠ¥å‘Š
            4. ğŸ¨ **æ˜¾ç¤ºæ ¼å¼**ï¼šæ™ºèƒ½è¯†åˆ«Markdownæ ¼å¼ï¼Œè‡ªåŠ¨æ¸²æŸ“æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ç­‰
            5. â° **è€å¿ƒç­‰å¾…**ï¼šæ·±åº¦ç ”ç©¶éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
            
            ### ğŸ’¡ Markdownæ”¯æŒç‰¹æ€§ï¼š
            - âœ… **è‡ªåŠ¨æ£€æµ‹**ï¼šæ™ºèƒ½è¯†åˆ«å†…å®¹ä¸­çš„Markdownæ ¼å¼
            - âœ… **æ ·å¼æ¸²æŸ“**ï¼šæ”¯æŒæ ‡é¢˜ã€ç²—ä½“ã€æ–œä½“ã€åˆ—è¡¨ã€ä»£ç å—ç­‰
            - âœ… **è¡¨æ ¼æ”¯æŒ**ï¼šè‡ªåŠ¨æ¸²æŸ“è¡¨æ ¼æ ¼å¼
            - âœ… **ä»£ç é«˜äº®**ï¼šä»£ç å—è‡ªåŠ¨è¯­æ³•é«˜äº®
            - âœ… **åŒæ¨¡å¼**ï¼šå¯åœ¨Markdownæ¸²æŸ“å’ŒåŸå§‹æ–‡æœ¬é—´åˆ‡æ¢
            """)
        
        # äº‹ä»¶ç»‘å®š
        def handle_start_research(topic, save_inter, save_final, stream_mode, format_mode):
            """å¤„ç†å¼€å§‹ç ”ç©¶äº‹ä»¶"""
            if stream_mode:
                # æµå¼æ¨¡å¼ï¼šå®æ—¶æ›´æ–°ç•Œé¢
                for result in app.start_research(topic, save_inter, save_final, stream_mode):
                    if len(result) == 5:
                        status, content, progress, stage_buttons, _ = result
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ç»ˆæŠ¥å‘Š
                        if "å®Œæˆ!" in status and len(content) > 100:
                            # æœ€ç»ˆæŠ¥å‘Š
                            if format_mode == "Markdownæ¸²æŸ“":
                                if app.is_markdown_content(content):
                                    enhanced_content = app.enhance_markdown_content(content)
                                    yield (status, enhanced_content, progress, 
                                          enhanced_content, 
                                          gr.update(value=content, visible=False),
                                          gr.update(visible=True),
                                          gr.update(visible=False),
                                          gr.update(choices=stage_buttons))
                                else:
                                    formatted_content = f"## ğŸ“„ ç ”ç©¶æŠ¥å‘Š\n\n```text\n{content}\n```"
                                    yield (status, formatted_content, progress, 
                                          formatted_content, 
                                          gr.update(value=content, visible=False),
                                          gr.update(visible=True),
                                          gr.update(visible=False),
                                          gr.update(choices=stage_buttons))
                            else:
                                yield (status, "è¯·åˆ‡æ¢åˆ°åŸå§‹æ–‡æœ¬æ¨¡å¼æŸ¥çœ‹å®Œæ•´å†…å®¹", progress, 
                                      "è¯·åˆ‡æ¢åˆ°åŸå§‹æ–‡æœ¬æ¨¡å¼æŸ¥çœ‹å®Œæ•´å†…å®¹", 
                                      gr.update(value=content, visible=True),
                                      gr.update(visible=False),
                                      gr.update(visible=True),
                                      gr.update(choices=stage_buttons))
                        else:
                            # ä¸­é—´è¿›åº¦æ›´æ–°
                            if format_mode == "Markdownæ¸²æŸ“" and content:
                                if app.is_markdown_content(content):
                                    display_content = app.enhance_markdown_content(content)
                                elif len(content) > 100:
                                    display_content = f"```\n{content}\n```"
                                else:
                                    display_content = content
                            else:
                                display_content = content
                            
                            yield (status, display_content, progress, 
                                  gr.update(),  # research_report_md
                                  gr.update(),  # research_report_text
                                  gr.update(visible=True if format_mode == "Markdownæ¸²æŸ“" else False),  # current_stage_md
                                  gr.update(value=content, visible=True if format_mode == "åŸå§‹æ–‡æœ¬" else False),  # current_stage_text
                                  gr.update(choices=stage_buttons))
                    else:
                        # é”™è¯¯æƒ…å†µ
                        yield (result[0], "", "", 
                              gr.update(), gr.update(), 
                              gr.update(), gr.update(), gr.update())
            else:
                # æ‰¹é‡æ¨¡å¼ï¼šä¸€æ¬¡æ€§è¿”å›ç»“æœ
                results = list(app.start_research(topic, save_inter, save_final, stream_mode))
                if results:
                    final_result = results[-1]  # å–æœ€åä¸€ä¸ªç»“æœ
                    if len(final_result) == 5:
                        status, content, progress, stage_buttons, _ = final_result
                        
                        if format_mode == "Markdownæ¸²æŸ“":
                            if app.is_markdown_content(content):
                                enhanced_content = app.enhance_markdown_content(content)
                                yield (status, enhanced_content, progress, 
                                      enhanced_content, 
                                      gr.update(value=content, visible=False),
                                      gr.update(visible=True),
                                      gr.update(visible=False),
                                      gr.update(choices=stage_buttons))
                            else:
                                formatted_content = f"## ğŸ“„ ç ”ç©¶æŠ¥å‘Š\n\n```text\n{content}\n```"
                                yield (status, formatted_content, progress, 
                                      formatted_content, 
                                      gr.update(value=content, visible=False),
                                      gr.update(visible=True),
                                      gr.update(visible=False),
                                      gr.update(choices=stage_buttons))
                        else:
                            yield (status, "è¯·åˆ‡æ¢åˆ°åŸå§‹æ–‡æœ¬æ¨¡å¼æŸ¥çœ‹å®Œæ•´å†…å®¹", progress, 
                                  "è¯·åˆ‡æ¢åˆ°åŸå§‹æ–‡æœ¬æ¨¡å¼æŸ¥çœ‹å®Œæ•´å†…å®¹", 
                                  gr.update(value=content, visible=True),
                                  gr.update(visible=False),
                                  gr.update(visible=True),
                                  gr.update(choices=stage_buttons))
                    else:
                        yield (final_result[0], "", "", 
                              gr.update(), gr.update(), 
                              gr.update(), gr.update(), gr.update())
        
        def handle_status_check():
            """å¤„ç†çŠ¶æ€æŸ¥è¯¢äº‹ä»¶"""
            return app.get_workflow_status()
        
        def handle_model_switch(provider):
            """å¤„ç†æ¨¡å‹åˆ‡æ¢äº‹ä»¶"""
            if not provider.strip():
                return "âŒ è¯·è¾“å…¥æä¾›å•†åç§°", model_info.value
            
            result = app.switch_model(provider)
            
            # æ›´æ–°æ¨¡å‹ä¿¡æ¯
            try:
                new_model_name = app.llm_model.model_name()
            except:
                new_model_name = str(app.llm_model)
            
            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def update_models_by_provider(selected_provider):
            """æ ¹æ®é€‰æ‹©çš„æä¾›å•†æ›´æ–°æ¨¡å‹åˆ—è¡¨"""
            if not selected_provider:
                return gr.Dropdown(choices=[])
            
            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯æä¾›å•†åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            models = app.get_models_by_provider(provider)
            return gr.Dropdown(choices=models)

        def switch_model_by_provider_and_model(selected_provider, selected_model):
            """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åˆ‡æ¢"""
            if not selected_provider:
                return "âŒ è¯·å…ˆé€‰æ‹©æä¾›å•†", model_info.value
            
            if not selected_model:
                return "âŒ è¯·é€‰æ‹©æ¨¡å‹", model_info.value
            
            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            model = selected_model.replace("âœ… ", "").replace("âŒ ", "")
            
            # å¦‚æœæ¨¡å‹åç§°åŒ…å«"(é»˜è®¤)"æ ‡è®°ï¼Œç§»é™¤å®ƒ
            if " (é»˜è®¤)" in model:
                model = model.replace(" (é»˜è®¤)", "")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if not selected_model.startswith("âœ…"):
                return f"âŒ æ¨¡å‹ {model} å½“å‰ä¸å¯ç”¨", model_info.value
            
            result = app.switch_model_by_provider_and_name(provider, model)
            
            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def refresh_provider_list():
            """åˆ·æ–°æä¾›å•†åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_available_providers())

        def manual_switch_model(manual_provider):
            """æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹ï¼ˆå…¼å®¹æ€§ï¼‰"""
            if not manual_provider:
                return "âŒ è¯·è¾“å…¥æä¾›å•†åç§°", model_info.value
            
            result = app.switch_model_by_provider_and_name(manual_provider)
            
            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def handle_format_toggle(format_mode, current_md_content, current_text_content):
            """å¤„ç†æ ¼å¼åˆ‡æ¢äº‹ä»¶"""
            if format_mode == "Markdownæ¸²æŸ“":
                return (gr.update(visible=True), gr.update(visible=False),
                       gr.update(visible=True), gr.update(visible=False))
            else:
                return (gr.update(visible=False), gr.update(visible=True),
                       gr.update(visible=False), gr.update(visible=True))
        
        def handle_stage_selection(stage_selection):
            """å¤„ç†é˜¶æ®µé€‰æ‹©äº‹ä»¶ï¼Œç›´æ¥æ˜¾ç¤ºå†…å®¹"""
            logger.info(f"é˜¶æ®µé€‰æ‹©äº‹ä»¶è§¦å‘ï¼Œé€‰æ‹©: {stage_selection}, å·¥ä½œæµè¿è¡ŒçŠ¶æ€: {app.workflow_running}")
            
            if not stage_selection:
                return "ç‚¹å‡»å·¦ä¾§é˜¶æ®µæŒ‰é’®æŸ¥çœ‹è¯¦ç»†å†…å®¹..."
            
            # æ— è®ºå·¥ä½œæµæ˜¯å¦è¿è¡Œï¼Œéƒ½å…è®¸æŸ¥çœ‹å·²å®Œæˆçš„é˜¶æ®µå†…å®¹
            content = app.get_stage_content(stage_selection)
            logger.info(f"è·å–åˆ°é˜¶æ®µå†…å®¹ï¼Œé•¿åº¦: {len(content)}")
            
            # å¦‚æœå·¥ä½œæµæ­£åœ¨è¿è¡Œï¼Œåœ¨å†…å®¹é¡¶éƒ¨æ·»åŠ æç¤ºä¿¡æ¯
            if app.workflow_running:
                status_note = "## ğŸ” æŸ¥çœ‹å·²å®Œæˆé˜¶æ®µå†…å®¹\n\n> â³ **æç¤º:** å·¥ä½œæµæ­£åœ¨è¿è¡Œä¸­ï¼Œä»¥ä¸‹æ˜¯å·²å®Œæˆé˜¶æ®µçš„å†…å®¹\n\n---\n\n"
                content = status_note + content
            
            return content

        # ç»‘å®šäº‹ä»¶
        start_btn.click(
            handle_start_research,
            inputs=[research_topic, save_intermediate, save_final_report, enable_stream, format_toggle],
            outputs=[status_display, current_stage_md, progress_log, research_report_md, research_report_text, current_stage_md, current_stage_text, stage_selector],
            show_progress="minimal"
        )
        
        status_btn.click(
            handle_status_check,
            outputs=[status_display]
        )
        
        # ç»‘å®šæä¾›å•†é€‰æ‹©äº‹ä»¶ - æ›´æ–°æ¨¡å‹åˆ—è¡¨
        provider_dropdown.change(
            update_models_by_provider, inputs=[provider_dropdown], outputs=[model_dropdown]
        )

        # ç»‘å®šæ¨¡å‹åˆ‡æ¢äº‹ä»¶
        switch_btn.click(
            switch_model_by_provider_and_model, inputs=[provider_dropdown, model_dropdown], outputs=[switch_result, model_info]
        )

        # ç»‘å®šåˆ·æ–°äº‹ä»¶
        refresh_btn.click(
            refresh_provider_list, outputs=[provider_dropdown]
        )

        # ç»‘å®šæ‰‹åŠ¨åˆ‡æ¢äº‹ä»¶
        manual_switch_btn.click(
            manual_switch_model, inputs=[provider_input], outputs=[switch_result, model_info]
        )

        # ç»‘å®šæ ¼å¼åˆ‡æ¢äº‹ä»¶
        format_toggle.change(
            handle_format_toggle,
            inputs=[format_toggle, research_report_md, research_report_text],
            outputs=[research_report_md, research_report_text, current_stage_md, current_stage_text]
        )
        
        # é˜¶æ®µé€‰æ‹©äº‹ä»¶ - ç›´æ¥è§¦å‘å†…å®¹æ˜¾ç¤º
        stage_selector.change(
            handle_stage_selection,
            inputs=[stage_selector],
            outputs=[stage_detail_display]
        )
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    try:
        # åˆå§‹åŒ–åº”ç”¨
        logger.info("æ­£åœ¨åˆå§‹åŒ– Deep Research åº”ç”¨...")
        app = DeepResearchApp(args.config)
        
        # åˆ›å»º Gradio ç•Œé¢
        demo = create_gradio_interface(app)
        
        # å¯åŠ¨åº”ç”¨
        logger.info(f"å¯åŠ¨ Deep Research åº”ç”¨åœ¨ {args.host}:{args.port}")
        demo.launch(
            server_name=args.host,
            server_port=args.port,
            share=args.share,
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡® (vertex_flow/config/llm.yml)")
        print("2. æ˜¯å¦æœ‰å¯ç”¨çš„ LLM æä¾›å•†")
        print("3. API å¯†é’¥æ˜¯å¦é…ç½®æ­£ç¡®")
        return 1


if __name__ == "__main__":
    exit(main()) 