#!/usr/bin/env python3
"""
åŸºäº Workflow LLM Vertex çš„ä¸»åº”ç”¨å…¥å£
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿå’Œ LLM Vertex å®ç°èŠå¤©åŠŸèƒ½
"""

import argparse
from typing import List, Tuple

import gradio as gr

from vertex_flow.utils.logger import setup_logger
from vertex_flow.workflow.constants import ENABLE_STREAM, SYSTEM, USER
from vertex_flow.workflow.service import VertexFlowService
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex
from vertex_flow.workflow.workflow import WorkflowContext

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


class WorkflowChatApp:
    """åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨"""

    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–åº”ç”¨"""
        logger.info(f" workflow chat app {config_path}")          
        self.service = VertexFlowService(config_file=config_path) if config_path else VertexFlowService()
        self.llm_model = None
        self.context = WorkflowContext()
        self.tools_enabled = False
        self.available_tools = []
        self._initialize_llm()
        self._initialize_tools()

    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹å’Œ Vertex"""
        try:
            # è·å–èŠå¤©æ¨¡å‹
            self.llm_model = self.service.get_chatmodel()
            if self.llm_model is None:
                raise ValueError("æ— æ³•è·å–èŠå¤©æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

            # å®‰å…¨è·å–æ¨¡å‹åç§°
            try:
                model_name = self.llm_model.model_name()
            except:
                model_name = str(self.llm_model)

            logger.info(f"æˆåŠŸåˆå§‹åŒ–èŠå¤©æ¨¡å‹: {model_name}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {e}")
            raise

    def _initialize_tools(self):
        """åˆå§‹åŒ–å¯ç”¨çš„å·¥å…·"""
        try:
            # åˆå§‹åŒ–å‘½ä»¤è¡Œå·¥å…·
            command_line_tool = self.service.get_command_line_tool()
            self.available_tools.append(command_line_tool)

            # åˆå§‹åŒ–Webæœç´¢å·¥å…· - å°è¯•å¤šç§æœç´¢æœåŠ¡
            web_search_tool = self._initialize_web_search_tool()
            if web_search_tool:
                self.available_tools.append(web_search_tool)

            logger.info(f"å·²åˆå§‹åŒ– {len(self.available_tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")
            self.available_tools = []

    def _initialize_web_search_tool(self):
        """åˆå§‹åŒ–Webæœç´¢å·¥å…·ï¼Œå°è¯•å¤šç§æœç´¢æœåŠ¡"""
        # ä¼˜å…ˆçº§åˆ—è¡¨ï¼šserpapi -> duckduckgo(å…è´¹) -> bocha -> searchapi -> bing
        search_providers = ["serpapi", "duckduckgo", "bocha", "searchapi", "bing"]
        
        for provider in search_providers:
            try:
                web_search_tool = self.service.get_web_search_tool(provider)
                logger.info(f"Webæœç´¢å·¥å…·å·²å¯ç”¨ - ä½¿ç”¨{provider}æœåŠ¡")
                return web_search_tool
            except Exception as e:
                logger.debug(f"{provider}æœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                continue
        
        # å¦‚æœæ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥ï¼Œè®°å½•è­¦å‘Š
        logger.warning("æ‰€æœ‰Webæœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–å¯ç”¨è‡³å°‘ä¸€ä¸ªæœç´¢æœåŠ¡")
        return None



    def _create_llm_vertex(self, system_prompt: str, enable_reasoning: bool = False, show_reasoning: bool = True):
        """åˆ›å»º LLM Vertex å®ä¾‹"""
        if self.llm_model is None:
            raise ValueError("LLMæ¨¡å‹æœªåˆå§‹åŒ–")

        # æ ¹æ®å·¥å…·å¯ç”¨çŠ¶æ€å†³å®šæ˜¯å¦ä¼ é€’å·¥å…·
        tools = self.available_tools if self.tools_enabled else []

        return LLMVertex(
            id="chat_llm",
            name="èŠå¤©LLM",
            model=self.llm_model,
            params={
                SYSTEM: system_prompt,
                USER: [],  # ç©ºçš„ç”¨æˆ·æ¶ˆæ¯åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šé€šè¿‡ conversation_history ä¼ é€’
                ENABLE_STREAM: True,  # å¯ç”¨æµæ¨¡å¼
                "enable_reasoning": enable_reasoning,  # å¯ç”¨æ€è€ƒè¿‡ç¨‹
                "show_reasoning": show_reasoning,  # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            },
            tools=tools,  # ä¼ é€’å·¥å…·åˆ—è¡¨
        )

    def chat_with_vertex(self, message, history, system_prompt, enable_reasoning=False, show_reasoning=True):
        """ä½¿ç”¨ LLM Vertex è¿›è¡ŒèŠå¤©ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œæ€è€ƒè¿‡ç¨‹"""
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
        try:
            current_model_name = self.llm_model.model_name() if self.llm_model else "æœªçŸ¥"
            logger.info(f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {current_model_name}")
        except:
            logger.info(f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {self.llm_model}")
        
        # æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼šmessageå¯ä»¥æ˜¯stræˆ–dict
        if isinstance(message, dict):
            # å¤šæ¨¡æ€è¾“å…¥
            text = message.get("text", "")
            image_url = message.get("image_url")
            if not text and not image_url:
                yield "", history
                return
            inputs = {
                "conversation_history": history,
                "current_message": text,
            }
            if image_url:
                inputs["image_url"] = image_url
        else:
            # å…¼å®¹åŸæœ‰å­—ç¬¦ä¸²è¾“å…¥
            if not message.strip():
                yield "", history
                return
            inputs = {
                "conversation_history": history,
                "current_message": message,
            }
        try:
            # åˆ›å»ºæ–°çš„ LLM Vertex å®ä¾‹ï¼ˆæ¯æ¬¡å¯¹è¯ä½¿ç”¨æ–°å®ä¾‹é¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
            llm_vertex = self._create_llm_vertex(system_prompt, enable_reasoning, show_reasoning)
            # å…ˆè¿›è¡Œæ¶ˆæ¯é‡å®šå‘å¤„ç†
            llm_vertex.messages_redirect(inputs, self.context)
            # ä½¿ç”¨æµå¼èŠå¤©æ–¹æ³•
            for chunk in self._stream_chat_with_gradio_format(llm_vertex, inputs, self.context, message, history):
                yield chunk
        except Exception as e:
            error_msg = f"èŠå¤©é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            import traceback
            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            new_history = history + [(str(message), error_msg)]
            yield "", new_history

    def _stream_chat_with_gradio_format(self, llm_vertex, inputs, context, message, history):
        """ç»Ÿä¸€çš„æµå¼èŠå¤©æ–¹æ³•ï¼Œè¿”å›Gradioæ ¼å¼çš„ç»“æœ"""
        response_parts = []
        # ç¡®ä¿ä¼ é€’ç»™Gradioçš„æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        if isinstance(message, dict):
            display_message = message.get("text", "")
            if message.get("image_url"):
                display_message += " [å›¾ç‰‡]"
        else:
            display_message = str(message)
        
        # ç¡®ä¿historyä¸ä¸ºNone
        if history is None:
            history = []
        
        new_history = history + [(display_message, "")]

        try:
            # ç›´æ¥ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼
            logger.info("ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼")
            chunk_count = 0
            for chunk in llm_vertex.chat_stream_generator(inputs, context):
                if chunk:
                    chunk_count += 1
                    response_parts.append(chunk)
                    current_response = "".join(response_parts)
                    new_history[-1] = (display_message, current_response)
                    yield "", new_history
            logger.info(f"æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")

        except Exception as e:
            logger.error(f"æµå¼èŠå¤©é”™è¯¯: {str(e)}")
            import traceback

            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            error_msg = f"èŠå¤©å¤„ç†é”™è¯¯: {str(e)}"
            new_history[-1] = (display_message, error_msg)
            yield "", new_history

        final_response = "".join(response_parts) if response_parts else new_history[-1][1]
        logger.info(f"ç”¨æˆ·: {display_message[:150]}... | åŠ©æ‰‹: {final_response[:150]}...")

    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            providers = []
            for provider, provider_config in llm_config.items():
                if isinstance(provider_config, dict):
                    enabled = provider_config.get("enabled", False)
                    status = "âœ…" if enabled else "âŒ"
                    providers.append(f"{status} {provider}")
            return providers
        except Exception as e:
            logger.error(f"è·å–æä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def get_models_by_provider(self, provider: str) -> List[str]:
        """æ ¹æ®æä¾›å•†è·å–å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            provider_config = llm_config.get(provider, {})
            if not provider_config:
                return [f"æœªæ‰¾åˆ°æä¾›å•†: {provider}"]

            models = []
            provider_enabled = provider_config.get("enabled", False)
            
            # æ”¯æŒå¤šæ¨¡å‹ç»“æ„
            if "models" in provider_config:
                models_list = provider_config["models"]
                for model_config in models_list:
                    if isinstance(model_config, dict):
                        model_name = model_config.get("name", "unknown")
                        model_enabled = model_config.get("enabled", False)
                        is_default = model_config.get("default", False)
                        status = "âœ…" if (provider_enabled and model_enabled) else "âŒ"
                        default_mark = " (é»˜è®¤)" if is_default else ""
                        models.append(f"{status} {model_name}{default_mark}")
            else:
                # æ—§æ ¼å¼ï¼šä½¿ç”¨model-name
                model_name = provider_config.get("model-name", provider)
                status = "âœ…" if provider_enabled else "âŒ"
                models.append(f"{status} {model_name}")
            
            return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def switch_model_by_provider_and_name(self, provider: str, model_name: str = None) -> str:
        """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åç§°åˆ‡æ¢æ¨¡å‹"""
        try:
            # å¯¹äºOllamaï¼Œå…ˆæ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if provider.lower() == "ollama":
                if not self._check_ollama_service():
                    return "âŒ OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œå¹¶ç›‘å¬åœ¨http://localhost:11434"

            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            new_model = self.service.get_chatmodel_by_provider(provider, model_name)
            if new_model:
                self.llm_model = new_model

                # å®‰å…¨è·å–æ¨¡å‹åç§°
                try:
                    actual_model_name = new_model.model_name()
                except:
                    actual_model_name = str(new_model)

                logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {provider} - {actual_model_name}")
                return f"âœ… å·²åˆ‡æ¢åˆ°: {provider} - {actual_model_name}"
            else:
                return f"âŒ æ— æ³•åˆ‡æ¢åˆ°æ¨¡å‹: {provider}"
        except Exception as e:
            error_msg = f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg

    def get_ollama_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            if not self._check_ollama_service():
                return ["OllamaæœåŠ¡ä¸å¯ç”¨"]

            import requests

            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = []
                for model in data.get("models", []):
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_mb = size / (1024 * 1024) if size else 0
                    models.append(f"{name} ({size_mb:.1f}MB)")
                return models if models else ["æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹"]
            else:
                return ["æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨"]
        except Exception as e:
            logger.error(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return [f"è·å–å¤±è´¥: {str(e)}"]

    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests

            response = requests.get("http://localhost:11434/api/version", timeout=3)
            return response.status_code == 200
        except:
            return False


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨")
    parser.add_argument("--port", type=int, default=7860, help="Gradio Web UI ç«¯å£")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Web UI ä¸»æœºåœ°å€")
    parser.add_argument("--share", action="store_true", help="å¯ç”¨ Gradio åˆ†äº«é“¾æ¥")
    parser.add_argument("--config", "-c", help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
    return parser.parse_args()


def create_gradio_interface(app: WorkflowChatApp):
    """åˆ›å»º Gradio ç•Œé¢"""

    # é»˜è®¤ç³»ç»Ÿæç¤º
    default_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€èªæ˜ä¸”ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"
        "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
        "å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"
        "\n\nä½ å¯ä»¥ä½¿ç”¨ç½‘ç»œæœç´¢å·¥å…·æ¥è·å–æœ€æ–°ä¿¡æ¯ï¼š"
        "\n- å½“ç”¨æˆ·è¯¢é—®æœ€æ–°æ–°é—»ã€å®æ—¶ä¿¡æ¯ã€è‚¡ä»·ã€å¤©æ°”ç­‰æ—¶ï¼Œè¯·ä¸»åŠ¨ä½¿ç”¨æœç´¢åŠŸèƒ½"
        "\n- å½“éœ€è¦æŸ¥è¯äº‹å®ã€è·å–å‡†ç¡®æ•°æ®æ—¶ï¼Œå»ºè®®è¿›è¡Œç½‘ç»œæœç´¢"
        "\n- æœç´¢åè¯·åŸºäºæœç´¢ç»“æœæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”"
    )

    with gr.Blocks(
        title="Vertex Chat - åŸºäº Workflow LLM",
        theme=gr.themes.Soft(),
        css="""
        .chat-container { 
            max-height: 600px; 
            overflow-y: auto; 
            scroll-behavior: smooth;
        }
        .model-info { 
            background-color: #f0f0f0; 
            padding: 10px; 
            border-radius: 5px; 
            margin: 5px 0; 
        }
        /* è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨çš„æ ·å¼ */
        .chatbot { 
            height: 500px !important; 
            overflow-y: auto !important;
            scroll-behavior: smooth !important;
        }
        .chatbot .wrap {
            height: 100% !important;
        }
        .chatbot .message-wrap {
            scroll-margin-bottom: 20px;
        }
        /* å¼ºåˆ¶æ»šåŠ¨çš„CSSåŠ¨ç”» */
        @keyframes scrollToBottom {
            to {
                scroll-behavior: smooth;
                overflow-anchor: none;
            }
        }
        .auto-scroll {
            animation: scrollToBottom 0.3s ease-out;
        }
        """,
        js="""
        function() {
            console.log('ğŸš€ åˆå§‹åŒ–æµå¼èŠå¤©è‡ªåŠ¨æ»šåŠ¨åŠŸèƒ½...');
            
            let scrollContainer = null;
            let isUserScrolling = false;
            let scrollTimeout = null;
            
            // æŸ¥æ‰¾å¹¶ç¼“å­˜èŠå¤©æ»šåŠ¨å®¹å™¨
            function findScrollContainer() {
                if (scrollContainer && document.contains(scrollContainer)) {
                    return scrollContainer;
                }
                
                console.log('ğŸ” å¼€å§‹æœç´¢æ»šåŠ¨å®¹å™¨...');
                
                // é¦–å…ˆæ‰“å°æ‰€æœ‰èŠå¤©æ¡†å…ƒç´ ï¼Œå¸®åŠ©è°ƒè¯•
                const chatbotElements = document.querySelectorAll('.chatbot');
                console.log('ğŸ“‹ æ‰¾åˆ°èŠå¤©æ¡†å…ƒç´ æ•°é‡:', chatbotElements.length);
                chatbotElements.forEach((el, index) => {
                    console.log(`èŠå¤©æ¡† ${index}:`, el, 'innerHTMLé•¿åº¦:', el.innerHTML.length);
                });
                
                // æ‰©å±•çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šå¯èƒ½æ€§
                const selectors = [
                    // Gradio 4.x å¸¸è§ç»“æ„
                    '.chatbot > div > div.overflow-y-auto',
                    '.chatbot .overflow-y-auto',
                    '.chatbot > div:first-child > div:first-child',
                    '.chatbot > div:first-child',
                    '.chatbot > div',
                    '.chatbot div[class*="overflow"]',
                    '.chatbot div[style*="overflow"]',
                    '.chatbot div[style*="scroll"]',
                    
                    // é€šç”¨å®¹å™¨
                    'gradio-chatbot .overflow-y-auto',
                    'gradio-chatbot > div',
                    '[data-testid="chatbot"] .overflow-y-auto',
                    '[data-testid="chatbot"] > div',
                    
                    // é«˜åº¦ç›¸å…³
                    '.chatbot .h-full',
                    '.chatbot div[style*="height"]',
                    
                    // æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                    '.chatbot',
                    'gradio-chatbot'
                ];
                
                console.log('ğŸ§­ å°†å°è¯•ä»¥ä¸‹é€‰æ‹©å™¨:', selectors);
                
                for (let i = 0; i < selectors.length; i++) {
                    const selector = selectors[i];
                    const elements = document.querySelectorAll(selector);
                    console.log(`é€‰æ‹©å™¨ "${selector}" æ‰¾åˆ° ${elements.length} ä¸ªå…ƒç´ `);
                    
                    for (let j = 0; j < elements.length; j++) {
                        const element = elements[j];
                        if (element) {
                            const hasScroll = element.scrollHeight > element.clientHeight;
                            const computedStyle = window.getComputedStyle(element);
                            const overflowY = computedStyle.overflowY;
                            
                            console.log(`  å…ƒç´  ${j}:`, {
                                tagName: element.tagName,
                                className: element.className,
                                scrollHeight: element.scrollHeight,
                                clientHeight: element.clientHeight,
                                hasScroll: hasScroll,
                                overflowY: overflowY,
                                element: element
                            });
                            
                            // æ›´å®½æ¾çš„æ¡ä»¶ï¼šæœ‰æ»šåŠ¨æˆ–è€…æ˜¯å¯æ»šåŠ¨å®¹å™¨
                            if (hasScroll || overflowY === 'auto' || overflowY === 'scroll' || element.scrollHeight > 100) {
                                scrollContainer = element;
                                console.log('âœ… æ‰¾åˆ°æ»šåŠ¨å®¹å™¨:', selector, 'å…ƒç´ :', element);
                                console.log('ğŸ“ å®¹å™¨å°ºå¯¸:', {
                                    scrollHeight: element.scrollHeight,
                                    clientHeight: element.clientHeight,
                                    scrollTop: element.scrollTop,
                                    overflowY: overflowY
                                });
                                return scrollContainer;
                            }
                        }
                    }
                }
                
                // å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾èŠå¤©æ¡†å†…çš„ä»»ä½•div
                console.log('ğŸš¨ å¸¸è§„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾èŠå¤©æ¡†å†…çš„æ‰€æœ‰div...');
                const allChatDivs = document.querySelectorAll('.chatbot div, gradio-chatbot div');
                console.log('æ‰¾åˆ°èŠå¤©æ¡†å†…divæ•°é‡:', allChatDivs.length);
                
                for (let i = 0; i < allChatDivs.length; i++) {
                    const div = allChatDivs[i];
                    if (div && div.scrollHeight > 50) { // éå¸¸å®½æ¾çš„æ¡ä»¶
                        console.log('ğŸ¯ å¤‡ç”¨æ–¹æ¡ˆæ‰¾åˆ°å®¹å™¨:', div);
                        scrollContainer = div;
                        return scrollContainer;
                    }
                }
                
                console.log('âŒ å®Œå…¨æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨');
                console.log('ğŸ”§ DOMç»“æ„è°ƒè¯•ä¿¡æ¯:');
                console.log('document.body:', document.body);
                console.log('æ‰€æœ‰å¸¦classçš„å…ƒç´ :', document.querySelectorAll('[class]').length);
                return null;
            }
            
            // å¼ºåˆ¶æ»šåŠ¨åˆ°åº•éƒ¨
            function forceScrollToBottom() {
                const container = findScrollContainer();
                if (container) {
                    container.scrollTop = container.scrollHeight;
                    console.log('ğŸ“œ æ‰§è¡Œæ»šåŠ¨:', container.scrollTop, '/', container.scrollHeight);
                    return true;
                }
                return false;
            }
            
            // å¹³æ»‘æ»šåŠ¨åˆ°åº•éƒ¨
            function smoothScrollToBottom() {
                const container = findScrollContainer();
                if (container) {
                    container.scrollTo({
                        top: container.scrollHeight,
                        behavior: 'smooth'
                    });
                    // å¤‡ç”¨å¼ºåˆ¶æ»šåŠ¨
                    setTimeout(() => {
                        if (container.scrollTop < container.scrollHeight - container.clientHeight - 50) {
                            container.scrollTop = container.scrollHeight;
                        }
                    }, 300);
                    return true;
                }
                return false;
            }
            
            // æ£€æŸ¥æ˜¯å¦åº”è¯¥è‡ªåŠ¨æ»šåŠ¨
            function shouldAutoScroll() {
                if (isUserScrolling) {
                    console.log('ğŸ¤š ç”¨æˆ·æ­£åœ¨æ»šåŠ¨ï¼Œè·³è¿‡è‡ªåŠ¨æ»šåŠ¨');
                    return false;
                }
                
                const container = findScrollContainer();
                if (!container) return false;
                
                // å¦‚æœå·²ç»åœ¨åº•éƒ¨é™„è¿‘ï¼Œåˆ™è‡ªåŠ¨æ»šåŠ¨
                const isNearBottom = container.scrollTop >= container.scrollHeight - container.clientHeight - 100;
                return isNearBottom;
            }
            
            // ç›‘å¬å†…å®¹å˜åŒ–çš„Observer
            function setupContentObserver() {
                const observer = new MutationObserver(function(mutations) {
                    let contentChanged = false;
                    
                    mutations.forEach(function(mutation) {
                        // æ£€æµ‹æ–‡æœ¬å†…å®¹å˜åŒ–ï¼ˆæµå¼æ›´æ–°ï¼‰
                        if (mutation.type === 'characterData') {
                            contentChanged = true;
                        }
                        // æ£€æµ‹å­å…ƒç´ å˜åŒ–ï¼ˆæ–°æ¶ˆæ¯ï¼‰
                        else if (mutation.type === 'childList' && mutation.addedNodes.length > 0) {
                            contentChanged = true;
                        }
                        // æ£€æµ‹å±æ€§å˜åŒ–ï¼ˆå¯èƒ½å½±å“æ»šåŠ¨ï¼‰
                        else if (mutation.type === 'attributes') {
                            contentChanged = true;
                        }
                    });
                    
                    if (contentChanged) {
                        console.log('ğŸ”„ å†…å®¹å˜åŒ–æ£€æµ‹ï¼Œæ‰§è¡Œè‡ªåŠ¨æ»šåŠ¨');
                        // ç«‹å³æ»šåŠ¨ - ä½¿ç”¨å¤šç§æ–¹æ³•
                        const scrolled = forceScrollToBottom() || bruteForceScroll();
                        
                        if (!scrolled) {
                            console.log('âš ï¸ ç«‹å³æ»šåŠ¨å¤±è´¥ï¼Œå»¶è¿Ÿé‡è¯•...');
                            // å»¶è¿Ÿæ»šåŠ¨ä½œä¸ºå¤‡ç”¨
                            setTimeout(() => {
                                const retryScrolled = forceScrollToBottom() || bruteForceScroll() || fallbackScroll();
                                if (retryScrolled) {
                                    console.log('âœ… å»¶è¿Ÿæ»šåŠ¨æˆåŠŸ');
                                } else {
                                    console.log('âŒ æ‰€æœ‰æ»šåŠ¨æ–¹æ³•éƒ½å¤±è´¥äº†');
                                }
                            }, 100);
                        } else {
                            console.log('âœ… ç«‹å³æ»šåŠ¨æˆåŠŸ');
                        }
                    }
                });
                
                // ç›‘å¬æ•´ä¸ªèŠå¤©æ¡†åŒºåŸŸ
                const chatbotElements = document.querySelectorAll('.chatbot, gradio-chatbot, [data-testid="chatbot"]');
                chatbotElements.forEach(element => {
                    observer.observe(element, {
                        childList: true,
                        subtree: true,
                        characterData: true,
                        attributes: false // å‡å°‘ä¸å¿…è¦çš„è§¦å‘
                    });
                    console.log('ğŸ“‹ å¼€å§‹ç›‘å¬èŠå¤©æ¡†:', element.tagName);
                });
                
                return observer;
            }
            
            // ç›‘å¬ç”¨æˆ·æ»šåŠ¨è¡Œä¸º
            function setupScrollListener() {
                document.addEventListener('scroll', function(e) {
                    if (e.target.closest && e.target.closest('.chatbot')) {
                        isUserScrolling = true;
                        console.log('ğŸ‘† ç”¨æˆ·æ‰‹åŠ¨æ»šåŠ¨');
                        
                        // æ¸…é™¤ä¹‹å‰çš„è¶…æ—¶
                        if (scrollTimeout) {
                            clearTimeout(scrollTimeout);
                        }
                        
                        // 3ç§’åæ¢å¤è‡ªåŠ¨æ»šåŠ¨
                        scrollTimeout = setTimeout(() => {
                            isUserScrolling = false;
                            console.log('âœ… æ¢å¤è‡ªåŠ¨æ»šåŠ¨');
                        }, 3000);
                    }
                }, true);
            }
            
            // å®šæ—¶å¼ºåˆ¶æ»šåŠ¨ï¼ˆæµå¼èŠå¤©çš„å¼ºåŠ›ä¿éšœï¼‰
            function setupPeriodicScroll() {
                setInterval(() => {
                    if (!isUserScrolling) {
                        // å°è¯•å¤šç§æ»šåŠ¨æ–¹æ³•
                        const scrolled = forceScrollToBottom() || 
                                       bruteForceScroll() || 
                                       fallbackScroll();
                        
                        if (scrolled) {
                            console.log('ğŸ¯ å®šæ—¶æ»šåŠ¨æˆåŠŸ');
                        }
                    }
                }, 500); // æ¯500msæ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿æµå¼å†…å®¹åŠæ—¶æ»šåŠ¨
            }
            
            // æš´åŠ›æ»šåŠ¨æ–¹æ³•ï¼šç›´æ¥æ»šåŠ¨æ‰€æœ‰å¯èƒ½çš„å®¹å™¨
            function bruteForceScroll() {
                console.log('ğŸ’ª æ‰§è¡Œæš´åŠ›æ»šåŠ¨...');
                let scrolled = false;
                
                // è·å–æ‰€æœ‰å¯èƒ½åŒ…å«æ»šåŠ¨å†…å®¹çš„å…ƒç´ 
                const allElements = [
                    ...document.querySelectorAll('.chatbot'),
                    ...document.querySelectorAll('.chatbot *'),
                    ...document.querySelectorAll('gradio-chatbot'),
                    ...document.querySelectorAll('gradio-chatbot *'),
                    ...document.querySelectorAll('[class*="chat"]'),
                    ...document.querySelectorAll('div')
                ];
                
                for (const element of allElements) {
                    if (element && element.scrollHeight > element.clientHeight) {
                        const oldScrollTop = element.scrollTop;
                        element.scrollTop = element.scrollHeight;
                        if (element.scrollTop !== oldScrollTop) {
                            console.log('ğŸ’ª æš´åŠ›æ»šåŠ¨æˆåŠŸ:', element);
                            scrolled = true;
                        }
                    }
                }
                
                return scrolled;
            }
            
            // æœ€åçš„å¤‡ç”¨æ»šåŠ¨æ–¹æ³•
            function fallbackScroll() {
                console.log('ğŸ†˜ æ‰§è¡Œå¤‡ç”¨æ»šåŠ¨æ–¹æ³•...');
                
                // å°è¯•æ»šåŠ¨çª—å£æœ¬èº«
                const oldScrollY = window.scrollY;
                window.scrollTo(0, document.body.scrollHeight);
                if (window.scrollY !== oldScrollY) {
                    console.log('ğŸ†˜ çª—å£æ»šåŠ¨æˆåŠŸ');
                    return true;
                }
                
                // å°è¯•æ»šåŠ¨bodyå’Œhtml
                const targets = [document.body, document.documentElement];
                for (const target of targets) {
                    if (target) {
                        target.scrollTop = target.scrollHeight;
                        console.log('ğŸ†˜ å°è¯•æ»šåŠ¨:', target.tagName);
                    }
                }
                
                return false;
            }
            
            // åˆå§‹åŒ–æ‰€æœ‰åŠŸèƒ½
            function initialize() {
                console.log('âš™ï¸ åˆå§‹åŒ–è‡ªåŠ¨æ»šåŠ¨ç³»ç»Ÿ...');
                console.log('ğŸ“… æ—¶é—´:', new Date().toLocaleTimeString());
                console.log('ğŸŒ document.readyState:', document.readyState);
                console.log('ğŸ“„ DOMå…ƒç´ æ€»æ•°:', document.querySelectorAll('*').length);
                
                // æŸ¥æ‰¾æ»šåŠ¨å®¹å™¨
                const foundContainer = findScrollContainer();
                
                if (foundContainer) {
                    console.log('ğŸ‰ æˆåŠŸæ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œç»§ç»­åˆå§‹åŒ–...');
                    
                    // è®¾ç½®å†…å®¹ç›‘å¬
                    setupContentObserver();
                    
                    // è®¾ç½®æ»šåŠ¨ç›‘å¬
                    setupScrollListener();
                    
                    // è®¾ç½®å®šæ—¶æ»šåŠ¨
                    setupPeriodicScroll();
                    
                    // åˆå§‹æ»šåŠ¨
                    setTimeout(() => {
                        forceScrollToBottom();
                    }, 500);
                    
                    console.log('âœ… è‡ªåŠ¨æ»šåŠ¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
                } else {
                    console.log('â³ æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œå°†åœ¨2ç§’åé‡è¯•...');
                    setTimeout(() => {
                        console.log('ğŸ”„ é‡è¯•åˆå§‹åŒ–æ»šåŠ¨ç³»ç»Ÿ...');
                        initialize();
                    }, 2000);
                }
            }
            
            // æ‰‹åŠ¨æŸ¥æ‰¾å®¹å™¨çš„å‡½æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            function debugFindContainer() {
                console.log('ğŸ› æ‰‹åŠ¨è°ƒè¯•æŸ¥æ‰¾å®¹å™¨...');
                findScrollContainer();
            }
            
            // æš´éœ²åˆ°å…¨å±€ï¼Œä¾¿äºæ‰‹åŠ¨è°ƒè¯•
            window.debugScrollContainer = debugFindContainer;
            
            // ç­‰å¾…DOMå‡†å¤‡å°±ç»ª
            if (document.readyState === 'loading') {
                document.addEventListener('DOMContentLoaded', initialize);
            } else {
                setTimeout(initialize, 500);
            }
            
            // çª—å£å¤§å°å˜åŒ–æ—¶é‡æ–°æ»šåŠ¨
            window.addEventListener('resize', () => {
                setTimeout(forceScrollToBottom, 200);
            });
        }
        """,
    ) as demo:

        gr.Markdown(
            """
        # ğŸ¤– Vertex Chat
        ### åŸºäº Workflow LLM Vertex çš„æ™ºèƒ½èŠå¤©åŠ©æ‰‹
        
        ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§ LLM æä¾›å•†
        """
        )

        with gr.Row():
            with gr.Column(scale=3):
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="å¯¹è¯",
                    height=500,
                    container=True,
                    elem_classes=["chat-container"],
                    show_copy_button=True,  # æ˜¾ç¤ºå¤åˆ¶æŒ‰é’®
                    bubble_full_width=False,  # æ¶ˆæ¯æ°”æ³¡ä¸å æ»¡å®½åº¦ï¼Œæ›´ç¾è§‚
                )

                with gr.Row():
                    msg = gr.Textbox(placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=1, scale=4, container=False)
                    image_url_input = gr.Textbox(placeholder="ç²˜è´´å›¾ç‰‡URLï¼ˆå¯é€‰ï¼‰", label="å›¾ç‰‡URL", lines=1, scale=3)
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1, size="sm")

                with gr.Row():
                    clear_btn = gr.Button("æ¸…é™¤å¯¹è¯", variant="secondary")

            with gr.Column(scale=1):
                # é…ç½®é¢æ¿
                gr.Markdown("### âš™ï¸ é…ç½®")

                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º", value=default_system_prompt, lines=4, placeholder="è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯..."
                )

                # æ¨¡å‹ä¿¡æ¯
                gr.Markdown("### ğŸ”§ æ¨¡å‹ä¿¡æ¯")

                # å®‰å…¨è·å–å½“å‰æ¨¡å‹åç§°
                current_model = "æœªçŸ¥"
                if app.llm_model:
                    try:
                        current_model = app.llm_model.model_name()
                    except:
                        current_model = str(app.llm_model)

                model_info = gr.Markdown(f"**å½“å‰æ¨¡å‹:** {current_model}", elem_classes=["model-info"])

                # æ¨¡å‹åˆ‡æ¢ - å…ˆé€‰æ‹©æä¾›å•†ï¼Œå†é€‰æ‹©æ¨¡å‹
                gr.Markdown("#### é€‰æ‹©æä¾›å•†")
                provider_dropdown = gr.Dropdown(
                    label="æä¾›å•†",
                    choices=app.get_available_providers(),
                    interactive=True,
                    info="é€‰æ‹©æä¾›å•†åæ˜¾ç¤ºå¯¹åº”çš„æ¨¡å‹",
                    allow_custom_value=False
                )

                gr.Markdown("#### é€‰æ‹©æ¨¡å‹")
                model_dropdown = gr.Dropdown(
                    label="æ¨¡å‹",
                    choices=[],
                    interactive=True,
                    info="é€‰æ‹©è¦ä½¿ç”¨çš„å…·ä½“æ¨¡å‹",
                    allow_custom_value=False
                )

                with gr.Row():
                    switch_btn = gr.Button("åˆ‡æ¢æ¨¡å‹", variant="primary", scale=1)
                    refresh_btn = gr.Button("åˆ·æ–°", variant="secondary", scale=1)

                switch_result = gr.Textbox(label="åˆ‡æ¢ç»“æœ", interactive=False, lines=2)

                # æ‰‹åŠ¨è¾“å…¥æ¨¡å¼ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                with gr.Accordion("ğŸ”§ æ‰‹åŠ¨è¾“å…¥æ¨¡å¼", open=False):
                    provider_input = gr.Textbox(
                        placeholder="è¾“å…¥æä¾›å•†åç§° (å¦‚: deepseek)", 
                        label="æ‰‹åŠ¨è¾“å…¥æä¾›å•†", 
                        scale=4
                    )
                    manual_switch_btn = gr.Button("æ‰‹åŠ¨åˆ‡æ¢", scale=1)

                # Ollamaæœ¬åœ°æ¨¡å‹ç®¡ç†
                gr.Markdown("### ğŸ  æœ¬åœ°æ¨¡å‹(Ollama)")

                with gr.Row():
                    refresh_ollama_btn = gr.Button("åˆ·æ–°æ¨¡å‹åˆ—è¡¨", scale=1)

                ollama_models = gr.Dropdown(
                    label="å¯ç”¨çš„Ollamaæ¨¡å‹",
                    choices=app.get_ollama_models(),
                    interactive=False,
                    info="å·²å®‰è£…çš„æœ¬åœ°æ¨¡å‹",
                )

                # å·¥å…·ç®¡ç†
                gr.Markdown("### ğŸ› ï¸ å·¥å…·ç®¡ç†")

                tools_enabled = gr.Checkbox(
                    label="å¯ç”¨Function Tools", value=app.tools_enabled, info="å…è®¸AIåŠ©æ‰‹ä½¿ç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡"
                )

                available_tools_display = gr.Dropdown(
                    label="å¯ç”¨å·¥å…·",
                    choices=[f"{tool.name}: {tool.description}" for tool in app.available_tools],
                    interactive=False,
                    info=f"å…±æœ‰ {len(app.available_tools)} ä¸ªå·¥å…·å¯ç”¨",
                )

                # æ€è€ƒè¿‡ç¨‹ç®¡ç†
                gr.Markdown("### ğŸ¤” æ€è€ƒè¿‡ç¨‹")

                enable_reasoning = gr.Checkbox(
                    label="å¯ç”¨æ€è€ƒè¿‡ç¨‹", value=False, info="è®©AIæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼ˆæ”¯æŒDeepSeek R1ç­‰æ¨¡å‹ï¼‰"
                )

                show_reasoning = gr.Checkbox(
                    label="æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹", value=True, info="æ˜¯å¦åœ¨å¯¹è¯ä¸­æ˜¾ç¤ºAIçš„æ€è€ƒè¿‡ç¨‹"
                )

                # å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•åŒºåŸŸ
                with gr.Accordion("ğŸ–¥ï¸ å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•", open=False):
                    cmd_input = gr.Textbox(label="å‘½ä»¤", placeholder="ä¾‹å¦‚: ls -la, python --version, pwd", lines=1)
                    cmd_execute_btn = gr.Button("æ‰§è¡Œå‘½ä»¤", variant="secondary")
                    cmd_result = gr.JSON(label="æ‰§è¡Œç»“æœ", visible=True)

        # äº‹ä»¶ç»‘å®š
        def respond(message, history, sys_prompt, image_url, enable_reasoning_val, show_reasoning_val):
            multimodal_inputs = {}
            # æ–‡æœ¬
            if message:
                multimodal_inputs["text"] = message
            # å›¾ç‰‡URLå¤„ç†
            if image_url and image_url.strip():
                # éªŒè¯å›¾ç‰‡URL
                url = image_url.strip()
                if "discordapp.com" in url or "discord.com" in url:
                    # Discordå›¾ç‰‡å¯èƒ½ä¸è¢«æ”¯æŒï¼Œç»™å‡ºæç¤º
                    yield "âš ï¸ æ£€æµ‹åˆ°Discordå›¾ç‰‡é“¾æ¥ï¼Œå¯èƒ½ä¸è¢«æ”¯æŒã€‚å»ºè®®ï¼š\n1. ä½¿ç”¨å…¶ä»–å›¾ç‰‡æ‰˜ç®¡æœåŠ¡\n2. ç›´æ¥ç²˜è´´å›¾ç‰‡URL", history + [(message or "", "âš ï¸ Discordå›¾ç‰‡é“¾æ¥å¯èƒ½ä¸è¢«æ”¯æŒï¼Œè¯·å°è¯•å…¶ä»–æ–¹å¼ã€‚")]
                    return
                elif "cdn.discordapp.com" in url:
                    # Discord CDNå›¾ç‰‡
                    yield "âš ï¸ Discord CDNå›¾ç‰‡é“¾æ¥å¯èƒ½ä¸è¢«æ”¯æŒã€‚å»ºè®®ä½¿ç”¨å…¶ä»–å›¾ç‰‡æ‰˜ç®¡æœåŠ¡ã€‚", history + [(message or "", "âš ï¸ Discord CDNå›¾ç‰‡é“¾æ¥å¯èƒ½ä¸è¢«æ”¯æŒï¼Œè¯·å°è¯•å…¶ä»–æ–¹å¼ã€‚")]
                    return
                else:
                    multimodal_inputs["image_url"] = url
            
            # ä¼ é€’ç»™chat_with_vertex
            try:
                for result in app.chat_with_vertex(multimodal_inputs, history, sys_prompt, enable_reasoning_val, show_reasoning_val):
                    yield result
            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                if "500" in str(e) and multimodal_inputs.get("image_url"):
                    error_msg = "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡æ ¼å¼ä¸æ”¯æŒæˆ–é“¾æ¥æ— æ•ˆã€‚è¯·å°è¯•ï¼š\n1. ä½¿ç”¨å…¶ä»–å›¾ç‰‡\n2. æ£€æŸ¥å›¾ç‰‡é“¾æ¥æ˜¯å¦æœ‰æ•ˆ\n3. ç¡®ä¿å›¾ç‰‡æ ¼å¼ä¸ºå¸¸è§æ ¼å¼ï¼ˆJPGã€PNGç­‰ï¼‰"
                yield error_msg, history + [(message or "", error_msg)]

        def clear_conversation():
            return []

        def update_models_by_provider(selected_provider):
            """æ ¹æ®é€‰æ‹©çš„æä¾›å•†æ›´æ–°æ¨¡å‹åˆ—è¡¨"""
            if not selected_provider:
                return gr.Dropdown(choices=[])
            
            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯æä¾›å•†åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            models = app.get_models_by_provider(provider)
            return gr.Dropdown(choices=models)

        def switch_model_by_provider_and_model(selected_provider, selected_model):
            """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åˆ‡æ¢"""
            if not selected_provider:
                return "âŒ è¯·å…ˆé€‰æ‹©æä¾›å•†", model_info.value
            
            if not selected_model:
                return "âŒ è¯·é€‰æ‹©æ¨¡å‹", model_info.value
            
            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            model = selected_model.replace("âœ… ", "").replace("âŒ ", "")
            
            # å¦‚æœæ¨¡å‹åç§°åŒ…å«"(é»˜è®¤)"æ ‡è®°ï¼Œç§»é™¤å®ƒ
            if " (é»˜è®¤)" in model:
                model = model.replace(" (é»˜è®¤)", "")
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if not selected_model.startswith("âœ…"):
                return f"âŒ æ¨¡å‹ {model} å½“å‰ä¸å¯ç”¨", model_info.value
            
            result = app.switch_model_by_provider_and_name(provider, model)
            
            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def refresh_provider_list():
            """åˆ·æ–°æä¾›å•†åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_available_providers())

        def manual_switch_model(manual_provider):
            """æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹ï¼ˆå…¼å®¹æ€§ï¼‰"""
            if not manual_provider:
                return "âŒ è¯·è¾“å…¥æä¾›å•†åç§°", model_info.value
            
            result = app.switch_model_by_provider_and_name(manual_provider)
            
            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def refresh_ollama_models():
            """åˆ·æ–°Ollamaæ¨¡å‹åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_ollama_models())

        def toggle_tools(enabled):
            """åˆ‡æ¢å·¥å…·å¯ç”¨çŠ¶æ€"""
            app.tools_enabled = enabled
            status = "âœ… å·²å¯ç”¨" if enabled else "âŒ å·²ç¦ç”¨"
            logger.info(f"å·¥å…·çŠ¶æ€å·²æ›´æ”¹: {status}")
            return f"å·¥å…·çŠ¶æ€: {status}"

        def execute_command_test(command):
            """æµ‹è¯•æ‰§è¡Œå‘½ä»¤"""
            if not command.strip():
                return {"error": "è¯·è¾“å…¥å‘½ä»¤"}

            try:
                # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
                if app.available_tools:
                    cmd_tool = app.available_tools[0]  # ç¬¬ä¸€ä¸ªå·¥å…·åº”è¯¥æ˜¯å‘½ä»¤è¡Œå·¥å…·
                    result = cmd_tool.execute({"command": command})
                    return result
                else:
                    return {"error": "å‘½ä»¤è¡Œå·¥å…·æœªåˆå§‹åŒ–"}
            except Exception as e:
                return {"error": f"æ‰§è¡Œå¤±è´¥: {str(e)}"}

        # ç»‘å®šå‘é€æ¶ˆæ¯äº‹ä»¶ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        msg.submit(respond, inputs=[msg, chatbot, system_prompt, image_url_input, enable_reasoning, show_reasoning], outputs=[msg, chatbot], show_progress="minimal")
        send_btn.click(respond, inputs=[msg, chatbot, system_prompt, image_url_input, enable_reasoning, show_reasoning], outputs=[msg, chatbot], show_progress="minimal")

        # ç»‘å®šæ¸…é™¤å¯¹è¯äº‹ä»¶
        clear_btn.click(clear_conversation, outputs=[chatbot])

        # ç»‘å®šæä¾›å•†é€‰æ‹©äº‹ä»¶ - æ›´æ–°æ¨¡å‹åˆ—è¡¨
        provider_dropdown.change(
            update_models_by_provider, inputs=[provider_dropdown], outputs=[model_dropdown]
        )

        # ç»‘å®šæ¨¡å‹åˆ‡æ¢äº‹ä»¶
        switch_btn.click(
            switch_model_by_provider_and_model, inputs=[provider_dropdown, model_dropdown], outputs=[switch_result, model_info]
        )

        # ç»‘å®šåˆ·æ–°äº‹ä»¶
        refresh_btn.click(
            refresh_provider_list, outputs=[provider_dropdown]
        )

        # ç»‘å®šæ‰‹åŠ¨åˆ‡æ¢äº‹ä»¶
        manual_switch_btn.click(
            manual_switch_model, inputs=[provider_input], outputs=[switch_result, model_info]
        )

        # ç»‘å®šOllamaæ¨¡å‹åˆ·æ–°äº‹ä»¶
        refresh_ollama_btn.click(refresh_ollama_models, outputs=[ollama_models])

        # ç»‘å®šå·¥å…·å¯ç”¨åˆ‡æ¢äº‹ä»¶
        tools_enabled.change(toggle_tools, inputs=[tools_enabled], outputs=[])

        # ç»‘å®šå‘½ä»¤æ‰§è¡Œäº‹ä»¶
        cmd_execute_btn.click(execute_command_test, inputs=[cmd_input], outputs=[cmd_result])

    return demo


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    try:
        # åˆå§‹åŒ–åº”ç”¨
        logger.info("æ­£åœ¨åˆå§‹åŒ– Vertex Chat åº”ç”¨...")
        app = WorkflowChatApp(config_path=args.config)

        # åˆ›å»º Gradio ç•Œé¢
        demo = create_gradio_interface(app)

        # å¯åŠ¨åº”ç”¨
        logger.info(f"å¯åŠ¨ Vertex Chat åº”ç”¨åœ¨ {args.host}:{args.port}")
        demo.launch(server_name=args.host, server_port=args.port, share=args.share, show_error=True)

    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡® (vertex_flow/config/llm.yml)")
        print("2. æ˜¯å¦æœ‰å¯ç”¨çš„ LLM æä¾›å•†")
        print("3. API å¯†é’¥æ˜¯å¦é…ç½®æ­£ç¡®")
        return 1


if __name__ == "__main__":
    exit(main())
