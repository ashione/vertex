#!/usr/bin/env python3
"""
åŸºäº Workflow LLM Vertex çš„ä¸»åº”ç”¨å…¥å£
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿå’Œ LLM Vertex å®ç°èŠå¤©åŠŸèƒ½
"""

import argparse
from typing import List, Tuple

import gradio as gr

from vertex_flow.utils.logger import setup_logger
from vertex_flow.workflow.service import VertexFlowService
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex
from vertex_flow.workflow.constants import SYSTEM, USER, ENABLE_STREAM
from vertex_flow.workflow.workflow import WorkflowContext

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


class WorkflowChatApp:
    """åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–åº”ç”¨"""
        self.service = VertexFlowService()
        self.llm_model = None
        self.context = WorkflowContext()
        self.tools_enabled = False
        self.available_tools = []
        self._initialize_llm()
        self._initialize_tools()
        
    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹å’Œ Vertex"""
        try:
            # è·å–èŠå¤©æ¨¡å‹
            self.llm_model = self.service.get_chatmodel()
            if self.llm_model is None:
                raise ValueError("æ— æ³•è·å–èŠå¤©æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            
            # å®‰å…¨è·å–æ¨¡å‹åç§°
            try:
                model_name = self.llm_model.model_name()
            except:
                model_name = str(self.llm_model)
            
            logger.info(f"æˆåŠŸåˆå§‹åŒ–èŠå¤©æ¨¡å‹: {model_name}")
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {e}")
            raise
    
    def _initialize_tools(self):
        """åˆå§‹åŒ–å¯ç”¨çš„å·¥å…·"""
        try:
            # åˆå§‹åŒ–å‘½ä»¤è¡Œå·¥å…·
            command_line_tool = self.service.get_command_line_tool()
            self.available_tools.append(command_line_tool)
            
            # å¯ä»¥æ·»åŠ å…¶ä»–å·¥å…·
            # web_search_tool = self.service.get_web_search_tool()
            # self.available_tools.append(web_search_tool)
            
            # finance_tool = self.service.get_finance_tool()
            # self.available_tools.append(finance_tool)
            
            logger.info(f"å·²åˆå§‹åŒ– {len(self.available_tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")
            self.available_tools = []
    
    def _create_llm_vertex(self, system_prompt: str):
        """åˆ›å»º LLM Vertex å®ä¾‹"""
        if self.llm_model is None:
            raise ValueError("LLMæ¨¡å‹æœªåˆå§‹åŒ–")
        
        # æ ¹æ®å·¥å…·å¯ç”¨çŠ¶æ€å†³å®šæ˜¯å¦ä¼ é€’å·¥å…·
        tools = self.available_tools if self.tools_enabled else []
        
        return LLMVertex(
            id="chat_llm",
            name="èŠå¤©LLM",
            model=self.llm_model,
            params={
                SYSTEM: system_prompt,
                USER: [],  # ç©ºçš„ç”¨æˆ·æ¶ˆæ¯åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šé€šè¿‡ conversation_history ä¼ é€’
                ENABLE_STREAM: True,  # å¯ç”¨æµæ¨¡å¼
            },
            tools=tools  # ä¼ é€’å·¥å…·åˆ—è¡¨
        )
    
    def chat_with_vertex(self, message: str, history: List[Tuple[str, str]], system_prompt: str):
        """ä½¿ç”¨ LLM Vertex è¿›è¡ŒèŠå¤©ï¼ˆæµå¼è¾“å‡ºï¼‰"""
        if not message.strip():
            yield "", history
            return
        
        try:
            # åˆ›å»ºæ–°çš„ LLM Vertex å®ä¾‹ï¼ˆæ¯æ¬¡å¯¹è¯ä½¿ç”¨æ–°å®ä¾‹é¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
            llm_vertex = self._create_llm_vertex(system_prompt)
            
            # ç›´æ¥ä¼ é€’å¯¹è¯å†å²å’Œå½“å‰æ¶ˆæ¯ç»™ LLM Vertex
            inputs = {
                "conversation_history": history,  # ä¼ é€’å¯¹è¯å†å²åˆ—è¡¨
                "current_message": message        # ä¼ é€’å½“å‰ç”¨æˆ·æ¶ˆæ¯
            }
            
            # å…ˆè¿›è¡Œæ¶ˆæ¯é‡å®šå‘å¤„ç†
            llm_vertex.messages_redirect(inputs, self.context)
            
            # ä½¿ç”¨ç»Ÿä¸€çš„æµå¼èŠå¤©æ–¹æ³•
            response_parts = []
            new_history = history + [(message, "")]
            
            for chunk in self._stream_chat_with_gradio_format(llm_vertex, inputs, self.context, message, history):
                yield chunk
            
        except Exception as e:
            error_msg = f"èŠå¤©é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            new_history = history + [(message, error_msg)]
            yield "", new_history
    
    def _stream_chat_with_gradio_format(self, llm_vertex, inputs, context, message, history):
        """ç»Ÿä¸€çš„æµå¼èŠå¤©æ–¹æ³•ï¼Œè¿”å›Gradioæ ¼å¼çš„ç»“æœ"""
        response_parts = []
        new_history = history + [(message, "")]
        
        for chunk in llm_vertex.chat_stream_generator(inputs, context):
            if chunk:
                response_parts.append(chunk)
                current_response = "".join(response_parts)
                new_history[-1] = (message, current_response)
                yield "", new_history
        
        final_response = "".join(response_parts)
        logger.info(f"ç”¨æˆ·: {message[:50]}... | åŠ©æ‰‹: {final_response[:50]}...")
    

    
    def get_available_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]
                
            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]
                
            models = []
            for provider, provider_config in llm_config.items():
                if isinstance(provider_config, dict):
                    model_name = provider_config.get("model-name", provider)
                    enabled = provider_config.get("enabled", False)
                    status = "âœ…" if enabled else "âŒ"
                    models.append(f"{status} {provider}: {model_name}")
            return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]
    
    def switch_model(self, provider: str) -> str:
        """åˆ‡æ¢æ¨¡å‹æä¾›å•†"""
        try:
            # å¯¹äºOllamaï¼Œå…ˆæ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if provider.lower() == "ollama":
                if not self._check_ollama_service():
                    return "âŒ OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œå¹¶ç›‘å¬åœ¨http://localhost:11434"
            
            new_model = self.service.get_chatmodel_by_provider(provider)
            if new_model:
                self.llm_model = new_model
                
                # å®‰å…¨è·å–æ¨¡å‹åç§°
                try:
                    model_name = new_model.model_name()
                except:
                    model_name = str(new_model)
                
                logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {provider} - {model_name}")
                return f"âœ… å·²åˆ‡æ¢åˆ°: {provider} - {model_name}"
            else:
                return f"âŒ æ— æ³•åˆ‡æ¢åˆ°æ¨¡å‹: {provider}"
        except Exception as e:
            error_msg = f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg
    
    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests
            response = requests.get("http://localhost:11434/api/version", timeout=3)
            return response.status_code == 200
        except:
            return False
    
    def get_ollama_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            if not self._check_ollama_service():
                return ["OllamaæœåŠ¡ä¸å¯ç”¨"]
            
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = []
                for model in data.get("models", []):
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_mb = size / (1024 * 1024) if size else 0
                    models.append(f"{name} ({size_mb:.1f}MB)")
                return models if models else ["æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹"]
            else:
                return ["æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨"]
        except Exception as e:
            logger.error(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return [f"è·å–å¤±è´¥: {str(e)}"]


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨")
    parser.add_argument("--port", type=int, default=7860, help="Gradio Web UI ç«¯å£")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="Web UI ä¸»æœºåœ°å€")
    parser.add_argument("--share", action="store_true", help="å¯ç”¨ Gradio åˆ†äº«é“¾æ¥")
    return parser.parse_args()


def create_gradio_interface(app: WorkflowChatApp):
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # é»˜è®¤ç³»ç»Ÿæç¤º
    default_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€èªæ˜ä¸”ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"
        "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
        "å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"
    )
    
    with gr.Blocks(
        title="Vertex Chat - åŸºäº Workflow LLM",
        theme=gr.themes.Soft(),
        css="""
        .chat-container { max-height: 600px; overflow-y: auto; }
        .model-info { background-color: #f0f0f0; padding: 10px; border-radius: 5px; margin: 5px 0; }
        """
    ) as demo:
        
        gr.Markdown("""
        # ğŸ¤– Vertex Chat
        ### åŸºäº Workflow LLM Vertex çš„æ™ºèƒ½èŠå¤©åŠ©æ‰‹
        
        ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§ LLM æä¾›å•†
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="å¯¹è¯",
                    height=500,
                    container=True,
                    elem_classes=["chat-container"]
                )
                
                with gr.Row():
                    msg = gr.Textbox(
                        placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...",
                        lines=2,
                        scale=4,
                        container=False
                    )
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                
                with gr.Row():
                    clear_btn = gr.Button("æ¸…é™¤å¯¹è¯", variant="secondary")
                    
            with gr.Column(scale=1):
                # é…ç½®é¢æ¿
                gr.Markdown("### âš™ï¸ é…ç½®")
                
                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º",
                    value=default_system_prompt,
                    lines=4,
                    placeholder="è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯..."
                )
                
                # æ¨¡å‹ä¿¡æ¯
                gr.Markdown("### ğŸ”§ æ¨¡å‹ä¿¡æ¯")
                
                # å®‰å…¨è·å–å½“å‰æ¨¡å‹åç§°
                current_model = "æœªçŸ¥"
                if app.llm_model:
                    try:
                        current_model = app.llm_model.model_name()
                    except:
                        current_model = str(app.llm_model)
                
                model_info = gr.Markdown(
                    f"**å½“å‰æ¨¡å‹:** {current_model}",
                    elem_classes=["model-info"]
                )
                
                # å¯ç”¨æ¨¡å‹åˆ—è¡¨
                available_models = gr.Dropdown(
                    label="å¯ç”¨æ¨¡å‹",
                    choices=app.get_available_models(),
                    interactive=False,
                    info="é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ¨¡å‹"
                )
                
                # æ¨¡å‹åˆ‡æ¢ - ä½¿ç”¨ä¸‹æ‹‰é€‰æ‹© + æ‰‹åŠ¨è¾“å…¥ä¸¤ç§æ–¹å¼
                with gr.Row():
                    provider_dropdown = gr.Dropdown(
                        label="é€‰æ‹©æä¾›å•†",
                        choices=["deepseek", "openrouter", "tongyi", "moonshoot", "ollama"],
                        scale=2,
                        allow_custom_value=True
                    )
                    switch_btn = gr.Button("åˆ‡æ¢", scale=1)
                
                with gr.Row():
                    provider_input = gr.Textbox(
                        placeholder="æˆ–æ‰‹åŠ¨è¾“å…¥æä¾›å•†åç§° (å¦‚: deepseek)",
                        label="æ‰‹åŠ¨è¾“å…¥æä¾›å•†",
                        scale=4,
                        visible=True
                    )
                
                switch_result = gr.Textbox(
                    label="åˆ‡æ¢ç»“æœ",
                    interactive=False,
                    lines=2
                )
                
                # Ollamaæœ¬åœ°æ¨¡å‹ç®¡ç†
                gr.Markdown("### ğŸ  æœ¬åœ°æ¨¡å‹(Ollama)")
                
                with gr.Row():
                    refresh_ollama_btn = gr.Button("åˆ·æ–°æ¨¡å‹åˆ—è¡¨", scale=1)
                
                ollama_models = gr.Dropdown(
                    label="å¯ç”¨çš„Ollamaæ¨¡å‹",
                    choices=app.get_ollama_models(),
                    interactive=False,
                    info="å·²å®‰è£…çš„æœ¬åœ°æ¨¡å‹"
                )
                
                # å·¥å…·ç®¡ç†
                gr.Markdown("### ğŸ› ï¸ å·¥å…·ç®¡ç†")
                
                tools_enabled = gr.Checkbox(
                    label="å¯ç”¨Function Tools",
                    value=app.tools_enabled,
                    info="å…è®¸AIåŠ©æ‰‹ä½¿ç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡"
                )
                
                available_tools_display = gr.Dropdown(
                    label="å¯ç”¨å·¥å…·",
                    choices=[f"{tool.name}: {tool.description}" for tool in app.available_tools],
                    interactive=False,
                    info=f"å…±æœ‰ {len(app.available_tools)} ä¸ªå·¥å…·å¯ç”¨"
                )
                
                # å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•åŒºåŸŸ
                with gr.Accordion("ğŸ–¥ï¸ å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•", open=False):
                    cmd_input = gr.Textbox(
                        label="å‘½ä»¤",
                        placeholder="ä¾‹å¦‚: ls -la, python --version, pwd",
                        lines=1
                    )
                    cmd_execute_btn = gr.Button("æ‰§è¡Œå‘½ä»¤", variant="secondary")
                    cmd_result = gr.JSON(
                        label="æ‰§è¡Œç»“æœ",
                        visible=True
                    )
        
        # äº‹ä»¶ç»‘å®š
        def respond(message, history, sys_prompt):
            # For streaming chat, we need to iterate through the generator
            for result in app.chat_with_vertex(message, history, sys_prompt):
                yield result
        
        def clear_conversation():
            return []
        
        def switch_model_handler(dropdown_provider, manual_provider):
            # ä¼˜å…ˆä½¿ç”¨ä¸‹æ‹‰é€‰æ‹©çš„å€¼ï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨æ‰‹åŠ¨è¾“å…¥çš„å€¼
            provider = dropdown_provider if dropdown_provider else manual_provider
            if not provider:
                return "âŒ è¯·é€‰æ‹©æˆ–è¾“å…¥æä¾›å•†åç§°", model_info.value
            
            result = app.switch_model(provider)
            
            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)
            
            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info
        
        def refresh_ollama_models():
            """åˆ·æ–°Ollamaæ¨¡å‹åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_ollama_models())
        
        def toggle_tools(enabled):
            """åˆ‡æ¢å·¥å…·å¯ç”¨çŠ¶æ€"""
            app.tools_enabled = enabled
            status = "âœ… å·²å¯ç”¨" if enabled else "âŒ å·²ç¦ç”¨"
            logger.info(f"å·¥å…·çŠ¶æ€å·²æ›´æ”¹: {status}")
            return f"å·¥å…·çŠ¶æ€: {status}"
        
        def execute_command_test(command):
            """æµ‹è¯•æ‰§è¡Œå‘½ä»¤"""
            if not command.strip():
                return {"error": "è¯·è¾“å…¥å‘½ä»¤"}
            
            try:
                # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
                if app.available_tools:
                    cmd_tool = app.available_tools[0]  # ç¬¬ä¸€ä¸ªå·¥å…·åº”è¯¥æ˜¯å‘½ä»¤è¡Œå·¥å…·
                    result = cmd_tool.execute({"command": command})
                    return result
                else:
                    return {"error": "å‘½ä»¤è¡Œå·¥å…·æœªåˆå§‹åŒ–"}
            except Exception as e:
                return {"error": f"æ‰§è¡Œå¤±è´¥: {str(e)}"}
        
        # ç»‘å®šå‘é€æ¶ˆæ¯äº‹ä»¶ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        msg.submit(
            respond,
            inputs=[msg, chatbot, system_prompt],
            outputs=[msg, chatbot],
            show_progress="minimal"
        )
        
        send_btn.click(
            respond,
            inputs=[msg, chatbot, system_prompt],
            outputs=[msg, chatbot],
            show_progress="minimal"
        )
        
        # ç»‘å®šæ¸…é™¤å¯¹è¯äº‹ä»¶
        clear_btn.click(
            clear_conversation,
            outputs=[chatbot]
        )
        
        # ç»‘å®šæ¨¡å‹åˆ‡æ¢äº‹ä»¶ - æ”¯æŒä¸¤ç§è¾“å…¥æ–¹å¼
        switch_btn.click(
            switch_model_handler,
            inputs=[provider_dropdown, provider_input],
            outputs=[switch_result, model_info]
        )
        
        # ç»‘å®šOllamaæ¨¡å‹åˆ·æ–°äº‹ä»¶
        refresh_ollama_btn.click(
            refresh_ollama_models,
            outputs=[ollama_models]
        )
        
        # ç»‘å®šå·¥å…·å¯ç”¨åˆ‡æ¢äº‹ä»¶
        tools_enabled.change(
            toggle_tools,
            inputs=[tools_enabled],
            outputs=[]
        )
        
        # ç»‘å®šå‘½ä»¤æ‰§è¡Œäº‹ä»¶
        cmd_execute_btn.click(
            execute_command_test,
            inputs=[cmd_input],
            outputs=[cmd_result]
        )
    
    return demo


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    try:
        # åˆå§‹åŒ–åº”ç”¨
        logger.info("æ­£åœ¨åˆå§‹åŒ– Vertex Chat åº”ç”¨...")
        app = WorkflowChatApp()
        
        # åˆ›å»º Gradio ç•Œé¢
        demo = create_gradio_interface(app)
        
        # å¯åŠ¨åº”ç”¨
        logger.info(f"å¯åŠ¨ Vertex Chat åº”ç”¨åœ¨ {args.host}:{args.port}")
        demo.launch(
            server_name=args.host,
            server_port=args.port,
            share=args.share,
            show_error=True
        )
        
    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡® (vertex_flow/config/llm.yml)")
        print("2. æ˜¯å¦æœ‰å¯ç”¨çš„ LLM æä¾›å•†")
        print("3. API å¯†é’¥æ˜¯å¦é…ç½®æ­£ç¡®")
        return 1


if __name__ == "__main__":
    exit(main()) 