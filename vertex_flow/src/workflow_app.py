#!/usr/bin/env python3
"""
åŸºäº Workflow LLM Vertex çš„ä¸»åº”ç”¨å…¥å£
ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿå’Œ LLM Vertex å®ç°èŠå¤©åŠŸèƒ½
"""

import argparse
import asyncio
import threading
import time
from typing import List, Tuple

import gradio as gr

from vertex_flow.utils.logger import setup_logger
from vertex_flow.workflow.constants import ENABLE_STREAM, SHOW_REASONING, SHOW_REASONING_KEY, SYSTEM, USER
from vertex_flow.workflow.service import VertexFlowService
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex
from vertex_flow.workflow.workflow import WorkflowContext

# MCP support imports
try:
    from vertex_flow.workflow.vertex.mcp_llm_vertex import MCPLLMVertex

    MCP_AVAILABLE = True
except ImportError as e:
    MCP_AVAILABLE = False
    print(f"MCPåŠŸèƒ½ä¸å¯ç”¨: {e}")

# é…ç½®æ—¥å¿—
logger = setup_logger(__name__)


class WorkflowChatApp:
    """åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨"""

    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–åº”ç”¨"""
        logger.info(f" workflow chat app {config_path}")
        self.service = VertexFlowService(config_file=config_path) if config_path else VertexFlowService()
        self.llm_model = None
        self.context = WorkflowContext()
        self.tools_enabled = False
        self.available_tools = []
        self.mcp_enabled = False
        self.mcp_manager = None

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._initialize_llm()
        self._initialize_tools()
        self._initialize_mcp()

    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM æ¨¡å‹å’Œ Vertex"""
        try:
            # è·å–èŠå¤©æ¨¡å‹
            self.llm_model = self.service.get_chatmodel()
            if self.llm_model is None:
                raise ValueError("æ— æ³•è·å–èŠå¤©æ¨¡å‹ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")

            # å®‰å…¨è·å–æ¨¡å‹åç§°
            try:
                model_name = self.llm_model.model_name()
            except:
                model_name = str(self.llm_model)

            logger.info(f"æˆåŠŸåˆå§‹åŒ–èŠå¤©æ¨¡å‹: {model_name}")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ– LLM å¤±è´¥: {e}")
            raise

    def _initialize_tools(self):
        """åˆå§‹åŒ–å¯ç”¨çš„å·¥å…·"""
        try:
            # åˆå§‹åŒ–å‘½ä»¤è¡Œå·¥å…·
            command_line_tool = self.service.get_command_line_tool()
            self.available_tools.append(command_line_tool)

            # åˆå§‹åŒ–Webæœç´¢å·¥å…· - å°è¯•å¤šç§æœç´¢æœåŠ¡
            web_search_tool = self._initialize_web_search_tool()
            if web_search_tool:
                self.available_tools.append(web_search_tool)

            logger.info(f"å·²åˆå§‹åŒ– {len(self.available_tools)} ä¸ªå·¥å…·")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å·¥å…·å¤±è´¥: {e}")
            self.available_tools = []

    def _initialize_web_search_tool(self):
        """åˆå§‹åŒ–Webæœç´¢å·¥å…·ï¼Œå°è¯•å¤šç§æœç´¢æœåŠ¡"""
        # ä¼˜å…ˆçº§åˆ—è¡¨ï¼šserpapi -> duckduckgo(å…è´¹) -> bocha -> searchapi -> bing
        search_providers = ["serpapi", "duckduckgo", "bocha", "searchapi", "bing"]

        for provider in search_providers:
            try:
                web_search_tool = self.service.get_web_search_tool(provider)
                logger.info(f"Webæœç´¢å·¥å…·å·²å¯ç”¨ - ä½¿ç”¨{provider}æœåŠ¡")
                return web_search_tool
            except Exception as e:
                logger.debug(f"{provider}æœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                continue

        # å¦‚æœæ‰€æœ‰æœåŠ¡éƒ½å¤±è´¥ï¼Œè®°å½•è­¦å‘Š
        logger.warning("æ‰€æœ‰Webæœç´¢æœåŠ¡åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–å¯ç”¨è‡³å°‘ä¸€ä¸ªæœç´¢æœåŠ¡")
        return None

    def _initialize_mcp(self):
        """åˆå§‹åŒ–MCPåŠŸèƒ½"""
        try:
            # æ£€æŸ¥serviceé…ç½®æ˜¯å¦å¯ç”¨MCP
            if self.service.is_mcp_enabled():
                logger.info("æ£€æµ‹åˆ°MCPé…ç½®å·²å¯ç”¨ï¼Œå¼€å§‹åˆå§‹åŒ–MCP...")

                # ç«‹å³åˆå§‹åŒ–MCPç®¡ç†å™¨
                self.mcp_manager = self.service.get_mcp_manager()

                if self.mcp_manager:
                    self.mcp_enabled = True
                    logger.info("MCPåŠŸèƒ½åˆå§‹åŒ–æˆåŠŸ")

                    # å¯åŠ¨æ—¶æ£€æŸ¥MCPå·¥å…·å¯ç”¨æ€§
                    self._check_mcp_tools_startup()
                else:
                    logger.warning("MCPç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥")
            else:
                logger.info("MCPåŠŸèƒ½æœªå¯ç”¨")

        except Exception as e:
            logger.error(f"MCPåˆå§‹åŒ–å¤±è´¥: {e}")
            self.mcp_enabled = False
            self.mcp_manager = None

    def _check_mcp_tools_startup(self):
        """æ£€æŸ¥MCPå·¥å…·åœ¨å¯åŠ¨æ—¶çš„å¯ç”¨æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not self.mcp_enabled or not self.mcp_manager:
            return

        try:

            def run_mcp_check():
                """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡ŒMCPæ£€æŸ¥"""
                try:
                    # ç­‰å¾…MCPç®¡ç†å™¨å®Œå…¨åˆå§‹åŒ–
                    max_init_wait = 10  # æœ€å¤šç­‰å¾…10ç§’åˆå§‹åŒ–
                    init_wait_interval = 0.5
                    init_waited = 0

                    while not self.mcp_manager._initialized and init_waited < max_init_wait:
                        logger.info(f"ç­‰å¾…MCPç®¡ç†å™¨åˆå§‹åŒ–... ({init_waited:.1f}s/{max_init_wait}s)")
                        time.sleep(init_wait_interval)
                        init_waited += init_wait_interval

                    if not self.mcp_manager._initialized:
                        logger.warning("MCPç®¡ç†å™¨åˆå§‹åŒ–è¶…æ—¶")
                        return

                    logger.info("MCPç®¡ç†å™¨å·²åˆå§‹åŒ– âœ…")

                    # ç®€å•æ£€æŸ¥è¿æ¥çŠ¶æ€ï¼Œä¸è·å–å·¥å…·
                    connected_clients = self.mcp_manager.get_connected_clients()

                    if connected_clients:
                        logger.info(
                            f"âœ… å¯åŠ¨æ£€æŸ¥æˆåŠŸ - å‘ç° {len(connected_clients)} ä¸ªå·²è¿æ¥çš„MCPå®¢æˆ·ç«¯: {', '.join(connected_clients)}"
                        )
                        logger.info("ğŸ’¡ MCPå·¥å…·å·²å‡†å¤‡å°±ç»ªï¼Œå¯åœ¨èŠå¤©ä¸­ä½¿ç”¨")
                    else:
                        logger.warning("âš ï¸ æœªå‘ç°å·²è¿æ¥çš„MCPå®¢æˆ·ç«¯")

                except Exception as e:
                    logger.error(f"MCPå·¥å…·å¯åŠ¨æ£€æŸ¥å¤±è´¥: {e}")

            # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œæ£€æŸ¥ï¼Œé¿å…é˜»å¡ä¸»çº¿ç¨‹
            check_thread = threading.Thread(target=run_mcp_check, daemon=True)
            check_thread.start()

        except Exception as e:
            logger.error(f"å¯åŠ¨MCPå·¥å…·æ£€æŸ¥å¤±è´¥: {e}")

    def check_mcp_availability(self) -> bool:
        """æ£€æŸ¥MCPåŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return self.mcp_enabled

    def get_mcp_manager(self):
        """è·å–MCPç®¡ç†å™¨"""
        return self.mcp_manager

    def _create_llm_vertex(
        self,
        system_prompt: str,
        enable_reasoning: bool = False,
        show_reasoning: bool = SHOW_REASONING,
        enable_mcp: bool = False,
    ):
        """åˆ›å»º LLM Vertex å®ä¾‹ï¼Œæ”¯æŒMCPå¢å¼º"""
        if self.llm_model is None:
            raise ValueError("LLMæ¨¡å‹æœªåˆå§‹åŒ–")

        # æ ¹æ®å·¥å…·å¯ç”¨çŠ¶æ€å†³å®šæ˜¯å¦ä¼ é€’å·¥å…·
        tools = self.available_tools if self.tools_enabled else []

        # è®°å½•MCPçŠ¶æ€
        mcp_manager = self.get_mcp_manager()
        logger.info(
            f"åˆ›å»ºLLM Vertex - MCPå¯ç”¨: {enable_mcp}, MCPå¯ç”¨: {MCP_AVAILABLE}, MCPç®¡ç†å™¨: {mcp_manager is not None}"
        )

        # å¦‚æœå¯ç”¨MCPä¸”MCPåŠŸèƒ½å¯ç”¨ï¼Œä½¿ç”¨MCPLLMVertex
        if enable_mcp and MCP_AVAILABLE and mcp_manager:
            try:
                return MCPLLMVertex(
                    id="mcp_chat_llm",
                    name="MCPå¢å¼ºèŠå¤©LLM",
                    model=self.llm_model,
                    params={
                        SYSTEM: system_prompt,
                        USER: [],  # ç©ºçš„ç”¨æˆ·æ¶ˆæ¯åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šé€šè¿‡ conversation_history ä¼ é€’
                        ENABLE_STREAM: True,  # å¯ç”¨æµæ¨¡å¼
                        "enable_reasoning": enable_reasoning,  # å¯ç”¨æ€è€ƒè¿‡ç¨‹
                        SHOW_REASONING_KEY: show_reasoning,  # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
                    },
                    tools=tools,  # ä¼ é€’å·¥å…·åˆ—è¡¨
                )
            except Exception as e:
                logger.warning(f"åˆ›å»ºMCPå¢å¼ºLLMå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†LLM: {e}")
                # å›é€€åˆ°æ ‡å‡†LLM
                pass

        # ä½¿ç”¨æ ‡å‡†LLM Vertex
        return LLMVertex(
            id="chat_llm",
            name="èŠå¤©LLM",
            model=self.llm_model,
            params={
                SYSTEM: system_prompt,
                USER: [],  # ç©ºçš„ç”¨æˆ·æ¶ˆæ¯åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬ä¼šé€šè¿‡ conversation_history ä¼ é€’
                ENABLE_STREAM: True,  # å¯ç”¨æµæ¨¡å¼
                "enable_reasoning": enable_reasoning,  # å¯ç”¨æ€è€ƒè¿‡ç¨‹
                SHOW_REASONING_KEY: show_reasoning,  # æ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            },
            tools=tools,  # ä¼ é€’å·¥å…·åˆ—è¡¨
        )

    def chat_with_vertex(self, message, history, system_prompt, enable_reasoning=False, show_reasoning=SHOW_REASONING):
        """ä½¿ç”¨ LLM Vertex è¿›è¡ŒèŠå¤©ï¼ˆæµå¼è¾“å‡ºï¼‰ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥å’Œæ€è€ƒè¿‡ç¨‹"""
        # MCPå¯ç”¨çŠ¶æ€ä½¿ç”¨é¢„åˆå§‹åŒ–çš„ç»“æœ
        enable_mcp = self.mcp_enabled

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå½“å‰ä½¿ç”¨çš„æ¨¡å‹
        try:
            current_model_name = self.llm_model.model_name() if self.llm_model else "æœªçŸ¥"
            logger.info(f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {current_model_name}, MCPå¯ç”¨: {enable_mcp}")
        except:
            logger.info(f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹: {self.llm_model}, MCPå¯ç”¨: {enable_mcp}")

        # æ”¯æŒå¤šæ¨¡æ€è¾“å…¥ï¼šmessageå¯ä»¥æ˜¯stræˆ–dict
        if isinstance(message, dict):
            # å¤šæ¨¡æ€è¾“å…¥
            text = message.get("text", "")
            image_url = message.get("image_url")
            if not text and not image_url:
                yield "", history
                return
            inputs = {
                "conversation_history": history,
                "current_message": text,
            }
            if image_url:
                inputs["image_url"] = image_url
        else:
            # å…¼å®¹åŸæœ‰å­—ç¬¦ä¸²è¾“å…¥
            if not message.strip():
                yield "", history
                return
            inputs = {
                "conversation_history": history,
                "current_message": message,
            }
        try:
            # åˆ›å»ºæ–°çš„ LLM Vertex å®ä¾‹ï¼ˆæ¯æ¬¡å¯¹è¯ä½¿ç”¨æ–°å®ä¾‹é¿å…çŠ¶æ€æ±¡æŸ“ï¼‰
            llm_vertex = self._create_llm_vertex(system_prompt, enable_reasoning, show_reasoning, enable_mcp)
            # å…ˆè¿›è¡Œæ¶ˆæ¯é‡å®šå‘å¤„ç†
            llm_vertex.messages_redirect(inputs, self.context)
            # ä½¿ç”¨æµå¼èŠå¤©æ–¹æ³•
            for chunk in self._stream_chat_with_gradio_format(llm_vertex, inputs, self.context, message, history):
                yield chunk
        except Exception as e:
            error_msg = f"èŠå¤©é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            import traceback

            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            new_history = history + [(str(message), error_msg)]
            yield "", new_history

    def _stream_chat_with_gradio_format(self, llm_vertex, inputs, context, message, history):
        """ç»Ÿä¸€çš„æµå¼èŠå¤©æ–¹æ³•ï¼Œè¿”å›Gradioæ ¼å¼çš„ç»“æœ"""
        response_parts = []
        # ç¡®ä¿ä¼ é€’ç»™Gradioçš„æ¶ˆæ¯æ ¼å¼æ­£ç¡®
        if isinstance(message, dict):
            display_message = message.get("text", "")
            if message.get("image_url"):
                display_message += " [å›¾ç‰‡]"
        else:
            display_message = str(message)

        # ç¡®ä¿historyä¸ä¸ºNone
        if history is None:
            history = []

        new_history = history + [(display_message, "")]

        try:
            # ç›´æ¥ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼
            logger.info("ä½¿ç”¨æµå¼è¾“å‡ºæ¨¡å¼")
            chunk_count = 0
            reasoning_header_added = False
            answer_header_added = False
            is_reasoning_phase = True

            for chunk in llm_vertex.chat_stream_generator(inputs, context):
                if chunk:
                    chunk_count += 1

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æ¨ç†æ¨¡å¼ä¸”éœ€è¦æ˜¾ç¤ºæ¨ç†è¿‡ç¨‹
                    enable_reasoning = getattr(llm_vertex, "enable_reasoning", False)
                    show_reasoning = getattr(llm_vertex, "show_reasoning", False)

                    if enable_reasoning and show_reasoning and not reasoning_header_added:
                        # æ·»åŠ æ¨ç†æ¨¡å¼å¤´éƒ¨
                        response_parts.append("ğŸ§  **å¯ç”¨æ¨ç†æ¨¡å¼** - æ‚¨å°†çœ‹åˆ°AIçš„å®Œæ•´æ€è€ƒè¿‡ç¨‹\n\n")
                        response_parts.append("ğŸ¤” **æ€è€ƒè¿‡ç¨‹ï¼š**\n")
                        reasoning_header_added = True

                    # æ£€æµ‹æ˜¯å¦ä»æ¨ç†é˜¶æ®µè½¬æ¢åˆ°å›ç­”é˜¶æ®µ
                    # ç®€å•çš„å¯å‘å¼ï¼šå¦‚æœchunkåŒ…å«è¾ƒå¤šè¿ç»­çš„éç‰¹æ®Šå­—ç¬¦ï¼Œå¯èƒ½æ˜¯æœ€ç»ˆç­”æ¡ˆçš„å¼€å§‹
                    if (
                        enable_reasoning
                        and show_reasoning
                        and is_reasoning_phase
                        and reasoning_header_added
                        and not answer_header_added
                        and chunk_count > 10
                        and len(chunk.strip()) > 5
                        and not any(marker in chunk for marker in ["æ€è€ƒ", "åˆ†æ", "è€ƒè™‘", "æ¨ç†"])
                    ):
                        # æ·»åŠ åˆ†éš”ç¬¦å’Œæœ€ç»ˆç­”æ¡ˆå¤´éƒ¨
                        response_parts.append("\n\n" + "=" * 50 + "\n")
                        response_parts.append("ğŸ’¡ **æœ€ç»ˆå›ç­”ï¼š**\n\n")
                        answer_header_added = True
                        is_reasoning_phase = False

                    response_parts.append(chunk)
                    current_response = "".join(response_parts)
                    new_history[-1] = (display_message, current_response)
                    yield "", new_history
            logger.info(f"æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")

        except Exception as e:
            logger.error(f"æµå¼èŠå¤©é”™è¯¯: {str(e)}")
            import traceback

            logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
            error_msg = f"èŠå¤©å¤„ç†é”™è¯¯: {str(e)}"
            new_history[-1] = (display_message, error_msg)
            yield "", new_history

        final_response = "".join(response_parts) if response_parts else new_history[-1][1]
        logger.info(f"ç”¨æˆ·: {display_message[:150]}... | åŠ©æ‰‹: {final_response[:150]}...")

    def get_available_providers(self) -> List[str]:
        """è·å–å¯ç”¨çš„æä¾›å•†åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            providers = []
            for provider, provider_config in llm_config.items():
                if isinstance(provider_config, dict):
                    enabled = provider_config.get("enabled", False)
                    status = "âœ…" if enabled else "âŒ"
                    providers.append(f"{status} {provider}")
            return providers
        except Exception as e:
            logger.error(f"è·å–æä¾›å•†åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def get_models_by_provider(self, provider: str) -> List[str]:
        """æ ¹æ®æä¾›å•†è·å–å¯¹åº”çš„æ¨¡å‹åˆ—è¡¨"""
        try:
            config = self.service._config
            if not isinstance(config, dict):
                return ["é…ç½®æ ¼å¼é”™è¯¯"]

            llm_config = config.get("llm", {})
            if not isinstance(llm_config, dict):
                return ["LLMé…ç½®æ ¼å¼é”™è¯¯"]

            provider_config = llm_config.get(provider, {})
            if not provider_config:
                return [f"æœªæ‰¾åˆ°æä¾›å•†: {provider}"]

            models = []
            provider_enabled = provider_config.get("enabled", False)

            # æ”¯æŒå¤šæ¨¡å‹ç»“æ„
            if "models" in provider_config:
                models_list = provider_config["models"]
                for model_config in models_list:
                    if isinstance(model_config, dict):
                        model_name = model_config.get("name", "unknown")
                        model_enabled = model_config.get("enabled", False)
                        is_default = model_config.get("default", False)
                        status = "âœ…" if (provider_enabled and model_enabled) else "âŒ"
                        default_mark = " (é»˜è®¤)" if is_default else ""
                        models.append(f"{status} {model_name}{default_mark}")
            else:
                # æ—§æ ¼å¼ï¼šä½¿ç”¨model-name
                model_name = provider_config.get("model-name", provider)
                status = "âœ…" if provider_enabled else "âŒ"
                models.append(f"{status} {model_name}")

            return models
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["é…ç½®åŠ è½½å¤±è´¥"]

    def switch_model_by_provider_and_name(self, provider: str, model_name: str = None) -> str:
        """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åç§°åˆ‡æ¢æ¨¡å‹"""
        try:
            # å¯¹äºOllamaï¼Œå…ˆæ£€æŸ¥æœåŠ¡æ˜¯å¦å¯ç”¨
            if provider.lower() == "ollama":
                if not self._check_ollama_service():
                    return "âŒ OllamaæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œå¹¶ç›‘å¬åœ¨http://localhost:11434"

            # å¦‚æœæŒ‡å®šäº†æ¨¡å‹åç§°ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            new_model = self.service.get_chatmodel_by_provider(provider, model_name)
            if new_model:
                self.llm_model = new_model

                # å®‰å…¨è·å–æ¨¡å‹åç§°
                try:
                    actual_model_name = new_model.model_name()
                except:
                    actual_model_name = str(new_model)

                logger.info(f"å·²åˆ‡æ¢åˆ°æ¨¡å‹: {provider} - {actual_model_name}")
                return f"âœ… å·²åˆ‡æ¢åˆ°: {provider} - {actual_model_name}"
            else:
                return f"âŒ æ— æ³•åˆ‡æ¢åˆ°æ¨¡å‹: {provider}"
        except Exception as e:
            error_msg = f"âŒ åˆ‡æ¢æ¨¡å‹å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return error_msg

    def get_ollama_models(self) -> List[str]:
        """è·å–å¯ç”¨çš„Ollamaæ¨¡å‹åˆ—è¡¨"""
        try:
            if not self._check_ollama_service():
                return ["OllamaæœåŠ¡ä¸å¯ç”¨"]

            import requests

            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = []
                for model in data.get("models", []):
                    name = model.get("name", "unknown")
                    size = model.get("size", 0)
                    size_mb = size / (1024 * 1024) if size else 0
                    models.append(f"{name} ({size_mb:.1f}MB)")
                return models if models else ["æ²¡æœ‰æ‰¾åˆ°å·²å®‰è£…çš„æ¨¡å‹"]
            else:
                return ["æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨"]
        except Exception as e:
            logger.error(f"è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return [f"è·å–å¤±è´¥: {str(e)}"]

    def _check_ollama_service(self) -> bool:
        """æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            import requests

            response = requests.get("http://localhost:11434/api/version", timeout=3)
            return response.status_code == 200
        except:
            return False


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="åŸºäº Workflow LLM Vertex çš„èŠå¤©åº”ç”¨")
    parser.add_argument("--port", type=int, default=7860, help="Gradio Web UI ç«¯å£")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Web UI ä¸»æœºåœ°å€")
    parser.add_argument("--share", action="store_true", help="å¯ç”¨ Gradio åˆ†äº«é“¾æ¥")
    parser.add_argument("--config", "-c", help="æŒ‡å®šé…ç½®æ–‡ä»¶è·¯å¾„")
    return parser.parse_args()


def create_gradio_interface(app: WorkflowChatApp):
    """åˆ›å»º Gradio ç•Œé¢"""

    # é»˜è®¤ç³»ç»Ÿæç¤º
    default_system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€èªæ˜ä¸”ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚"
        "è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"
        "å¦‚æœä¸ç¡®å®šç­”æ¡ˆï¼Œè¯·è¯šå®åœ°è¯´æ˜ã€‚"
        "\n\nğŸ› ï¸ ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥ååŠ©ç”¨æˆ·ï¼š"
        "\n\nğŸ“¡ **ç½‘ç»œæœç´¢å·¥å…· (Web Search)**"
        "\n- å½“ç”¨æˆ·è¯¢é—®æœ€æ–°æ–°é—»ã€å®æ—¶ä¿¡æ¯ã€è‚¡ä»·ã€å¤©æ°”ç­‰æ—¶ï¼Œè¯·ä¸»åŠ¨ä½¿ç”¨æœç´¢åŠŸèƒ½"
        "\n- å½“éœ€è¦æŸ¥è¯äº‹å®ã€è·å–å‡†ç¡®æ•°æ®æ—¶ï¼Œå»ºè®®è¿›è¡Œç½‘ç»œæœç´¢"
        "\n- æœç´¢åè¯·åŸºäºæœç´¢ç»“æœæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”"
        "\n- æ”¯æŒå¤šç§æœç´¢å¼•æ“ï¼šDuckDuckGoã€SerpAPIã€SearchAPIç­‰"
        "\n\nğŸ’» **å‘½ä»¤è¡Œå·¥å…· (Command Line)**"
        "\n- å¯ä»¥æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ã€æŸ¥çœ‹æ–‡ä»¶ã€è¿è¡Œè„šæœ¬ç­‰"
        "\n- æ”¯æŒçš„å‘½ä»¤å¦‚ï¼šls, pwd, python, git, ps, grepç­‰"
        "\n- å…·æœ‰å®‰å…¨é˜²æŠ¤ï¼Œä¼šè‡ªåŠ¨æ‹¦æˆªå±é™©å‘½ä»¤"
        "\n- é€‚ç”¨äºï¼šæ–‡ä»¶æ“ä½œã€ç³»ç»ŸæŸ¥è¯¢ã€å¼€å‘è°ƒè¯•ã€ç¯å¢ƒæ£€æŸ¥"
        "\n\nğŸ’° **é‡‘èå·¥å…· (Finance)**"
        "\n- æŸ¥è¯¢è‚¡ç¥¨ä»·æ ¼ã€å¸‚åœºæ•°æ®ã€è´¢ç»æ–°é—»"
        "\n- æ”¯æŒè‚¡ç¥¨ä»£ç æŸ¥è¯¢ã€ä»·æ ¼èµ°åŠ¿åˆ†æ"
        "\n- è·å–å®æ—¶é‡‘èå¸‚åœºä¿¡æ¯"
        "\n\nğŸ§® **æ•°å­¦è®¡ç®—å·¥å…· (Calculate)**"
        "\n- æ‰§è¡Œæ•°å­¦è¿ç®—å’Œè¡¨è¾¾å¼è®¡ç®—"
        "\n- æ”¯æŒåŸºæœ¬å››åˆ™è¿ç®—ã€æ‹¬å·ä¼˜å…ˆçº§"
        "\n- å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼æ±‚å€¼"
        "\n\nğŸ“ **æ–‡æœ¬å¤„ç†å·¥å…· (Text Processing)**"
        "\n- æ–‡æœ¬ç»Ÿè®¡ï¼šå­—æ•°ç»Ÿè®¡ã€å­—ç¬¦è®¡æ•°"
        "\n- æ–‡æœ¬è½¬æ¢ï¼šå¤§å°å†™è½¬æ¢ã€æ–‡æœ¬åè½¬"
        "\n- æ–‡æœ¬åˆ†æå’Œæ ¼å¼åŒ–å¤„ç†"
        "\n\nğŸ”— **MCPå·¥å…·é›†æˆ (Model Context Protocol)**"
        "\n- å¦‚æœé…ç½®äº†MCPå®¢æˆ·ç«¯ï¼Œå¯ä»¥è®¿é—®é¢å¤–çš„å·¥å…·å’Œèµ„æº"
        "\n- æ”¯æŒæ–‡ä»¶ç³»ç»Ÿè®¿é—®ã€æ•°æ®åº“æŸ¥è¯¢ã€å¤–éƒ¨APIè°ƒç”¨ç­‰"
        "\n- MCPå·¥å…·ä¼šè‡ªåŠ¨åŠ è½½å¹¶å¯é€šè¿‡function callingä½¿ç”¨"
        "\n\nğŸ’¡ **ä½¿ç”¨å»ºè®®ï¼š**"
        "\n- æ ¹æ®ç”¨æˆ·éœ€æ±‚ä¸»åŠ¨é€‰æ‹©æœ€é€‚åˆçš„å·¥å…·"
        "\n- å¤šä¸ªå·¥å…·å¯ä»¥ç»„åˆä½¿ç”¨è§£å†³å¤æ‚é—®é¢˜"
        "\n- ä½¿ç”¨å·¥å…·å‰å¯ä»¥ç®€å•è¯´æ˜å°†è¦æ‰§è¡Œçš„æ“ä½œ"
        "\n- å·¥å…·æ‰§è¡Œåè¯·è§£é‡Šç»“æœå¹¶æä¾›æœ‰ç”¨çš„è§è§£"
    )

    with gr.Blocks(
        title="Vertex Chat - åŸºäº Workflow LLM",
        theme=gr.themes.Soft(),
        css="""
        .chat-container { 
            max-height: 600px; 
            overflow-y: auto; 
            scroll-behavior: smooth;
        }
        .model-info { 
            background-color: #f0f0f0; 
            padding: 10px; 
            border-radius: 5px; 
            margin: 5px 0; 
        }
        /* è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨çš„æ ·å¼ */
        .chatbot { 
            height: 500px !important; 
            overflow-y: auto !important;
            scroll-behavior: smooth !important;
        }
        .chatbot .wrap {
            height: 100% !important;
        }
        .chatbot .message-wrap {
            scroll-margin-bottom: 20px;
        }
        /* å¼ºåˆ¶æ»šåŠ¨çš„CSSåŠ¨ç”» */
        @keyframes scrollToBottom {
            to {
                scroll-behavior: smooth;
                overflow-anchor: none;
            }
        }
        .auto-scroll {
            animation: scrollToBottom 0.3s ease-out;
        }
        """,
        js="""
        function() {
            console.log('ğŸš€ åˆå§‹åŒ–æµå¼èŠå¤©è‡ªåŠ¨æ»šåŠ¨åŠŸèƒ½...');
            
            let scrollContainer = null;
            let isUserScrolling = false;
            let scrollTimeout = null;
            let autoScrollEnabled = true; // æ§åˆ¶è‡ªåŠ¨æ»šåŠ¨æ˜¯å¦å¯ç”¨
            
            // æŸ¥æ‰¾å¹¶ç¼“å­˜èŠå¤©æ»šåŠ¨å®¹å™¨
            function findScrollContainer() {
                if (scrollContainer && document.contains(scrollContainer)) {
                    return scrollContainer;
                }
                
                console.log('ğŸ” å¼€å§‹æœç´¢æ»šåŠ¨å®¹å™¨...');
                
                // é¦–å…ˆæ‰“å°æ‰€æœ‰èŠå¤©æ¡†å…ƒç´ ï¼Œå¸®åŠ©è°ƒè¯•
                const chatbotElements = document.querySelectorAll('.chatbot');
                console.log('ğŸ“‹ æ‰¾åˆ°èŠå¤©æ¡†å…ƒç´ æ•°é‡:', chatbotElements.length);
                chatbotElements.forEach((el, index) => {
                    console.log(`èŠå¤©æ¡† ${index}:`, el, 'innerHTMLé•¿åº¦:', el.innerHTML.length);
                });
                
                // æ‰©å±•çš„é€‰æ‹©å™¨åˆ—è¡¨ï¼ŒåŒ…å«æ›´å¤šå¯èƒ½æ€§
                const selectors = [
                    // Gradio 4.x å¸¸è§ç»“æ„
                    '.chatbot > div > div.overflow-y-auto',
                    '.chatbot .overflow-y-auto',
                    '.chatbot > div:first-child > div:first-child',
                    '.chatbot > div:first-child',
                    '.chatbot > div',
                    '.chatbot div[class*="overflow"]',
                    '.chatbot div[style*="overflow"]',
                    '.chatbot div[style*="scroll"]',
                    
                    // é€šç”¨å®¹å™¨
                    'gradio-chatbot .overflow-y-auto',
                    'gradio-chatbot > div',
                    '[data-testid="chatbot"] .overflow-y-auto',
                    '[data-testid="chatbot"] > div',
                    
                    // é«˜åº¦ç›¸å…³
                    '.chatbot .h-full',
                    '.chatbot div[style*="height"]',
                    
                    // æœ€åçš„å¤‡é€‰æ–¹æ¡ˆ
                    '.chatbot',
                    'gradio-chatbot'
                ];
                
                console.log('ğŸ§­ å°†å°è¯•ä»¥ä¸‹é€‰æ‹©å™¨:', selectors);
                
                for (let i = 0; i < selectors.length; i++) {
                    const selector = selectors[i];
                    const elements = document.querySelectorAll(selector);
                    console.log(`é€‰æ‹©å™¨ "${selector}" æ‰¾åˆ° ${elements.length} ä¸ªå…ƒç´ `);
                    
                    for (let j = 0; j < elements.length; j++) {
                        const element = elements[j];
                        if (element) {
                            const hasScroll = element.scrollHeight > element.clientHeight;
                            const computedStyle = window.getComputedStyle(element);
                            const overflowY = computedStyle.overflowY;
                            
                            console.log(`  å…ƒç´  ${j}:`, {
                                tagName: element.tagName,
                                className: element.className,
                                scrollHeight: element.scrollHeight,
                                clientHeight: element.clientHeight,
                                hasScroll: hasScroll,
                                overflowY: overflowY,
                                element: element
                            });
                            
                            // æ›´å®½æ¾çš„æ¡ä»¶ï¼šæœ‰æ»šåŠ¨æˆ–è€…æ˜¯å¯æ»šåŠ¨å®¹å™¨
                            if (hasScroll || overflowY === 'auto' || overflowY === 'scroll' || element.scrollHeight > 100) {
                                scrollContainer = element;
                                console.log('âœ… æ‰¾åˆ°æ»šåŠ¨å®¹å™¨:', selector, 'å…ƒç´ :', element);
                                console.log('ğŸ“ å®¹å™¨å°ºå¯¸:', {
                                    scrollHeight: element.scrollHeight,
                                    clientHeight: element.clientHeight,
                                    scrollTop: element.scrollTop,
                                    overflowY: overflowY
                                });
                                return scrollContainer;
                            }
                        }
                    }
                }
                
                // å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾èŠå¤©æ¡†å†…çš„ä»»ä½•div
                console.log('ğŸš¨ å¸¸è§„æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æŸ¥æ‰¾èŠå¤©æ¡†å†…çš„æ‰€æœ‰div...');
                const allChatDivs = document.querySelectorAll('.chatbot div, gradio-chatbot div');
                console.log('æ‰¾åˆ°èŠå¤©æ¡†å†…divæ•°é‡:', allChatDivs.length);
                
                for (let i = 0; i < allChatDivs.length; i++) {
                    const div = allChatDivs[i];
                    if (div && div.scrollHeight > 50) { // éå¸¸å®½æ¾çš„æ¡ä»¶
                        console.log('ğŸ¯ å¤‡ç”¨æ–¹æ¡ˆæ‰¾åˆ°å®¹å™¨:', div);
                        scrollContainer = div;
                        return scrollContainer;
                    }
                }
                
                console.log('âŒ å®Œå…¨æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨');
                console.log('ğŸ”§ DOMç»“æ„è°ƒè¯•ä¿¡æ¯:');
                console.log('document.body:', document.body);
                console.log('æ‰€æœ‰å¸¦classçš„å…ƒç´ :', document.querySelectorAll('[class]').length);
                return null;
            }
            
            // å¼ºåˆ¶æ»šåŠ¨åˆ°åº•éƒ¨
            function forceScrollToBottom() {
                const container = findScrollContainer();
                if (container) {
                    container.scrollTop = container.scrollHeight;
                    console.log('ğŸ“œ æ‰§è¡Œæ»šåŠ¨:', container.scrollTop, '/', container.scrollHeight);
                    return true;
                }
                return false;
            }
            
            // å¼ºåˆ¶æ»šåŠ¨åˆ°é¡¶éƒ¨ï¼ˆç”¨äºæ¸…é™¤å¯¹è¯ï¼‰
            function forceScrollToTop() {
                const container = findScrollContainer();
                if (container) {
                    container.scrollTop = 0;
                    console.log('ğŸ“œ æ‰§è¡Œæ»šåŠ¨åˆ°é¡¶éƒ¨');
                    return true;
                }
                return false;
            }
            
            // å¹³æ»‘æ»šåŠ¨åˆ°åº•éƒ¨
            function smoothScrollToBottom() {
                const container = findScrollContainer();
                if (container) {
                    container.scrollTo({
                        top: container.scrollHeight,
                        behavior: 'smooth'
                    });
                    // å¤‡ç”¨å¼ºåˆ¶æ»šåŠ¨
                    setTimeout(() => {
                        if (container.scrollTop < container.scrollHeight - container.clientHeight - 50) {
                            container.scrollTop = container.scrollHeight;
                        }
                    }, 300);
                    return true;
                }
                return false;
            }
            
            // æ£€æŸ¥æ˜¯å¦åº”è¯¥è‡ªåŠ¨æ»šåŠ¨
            function shouldAutoScroll() {
                if (!autoScrollEnabled) {
                    console.log('ğŸ›‘ è‡ªåŠ¨æ»šåŠ¨å·²ç¦ç”¨');
                    return false;
                }
                
                if (isUserScrolling) {
                    console.log('ğŸ¤š ç”¨æˆ·æ­£åœ¨æ»šåŠ¨ï¼Œè·³è¿‡è‡ªåŠ¨æ»šåŠ¨');
                    return false;
                }
                
                const container = findScrollContainer();
                if (!container) {
                    console.log('âŒ æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨');
                    return false;
                }
                
                // ä¿®å¤ï¼šåœ¨æµå¼è¾“å‡ºæ—¶å§‹ç»ˆè‡ªåŠ¨æ»šåŠ¨ï¼Œé™¤éç”¨æˆ·æ˜ç¡®å‘ä¸Šæ»šåŠ¨äº†å¾ˆå¤š
                const scrollTop = container.scrollTop;
                const scrollHeight = container.scrollHeight;
                const clientHeight = container.clientHeight;
                const maxScroll = scrollHeight - clientHeight;
                
                console.log('ğŸ“ å®¹å™¨å°ºå¯¸ä¿¡æ¯:', {
                    scrollTop,
                    scrollHeight,
                    clientHeight,
                    maxScroll,
                    containerTag: container.tagName,
                    containerClass: container.className
                });
                
                // å¦‚æœå†…å®¹é«˜åº¦å°äºå®¹å™¨é«˜åº¦ï¼Œä»ç„¶å…è®¸æ»šåŠ¨ï¼ˆæµå¼è¾“å‡ºä¸­å†…å®¹ä¼šå¢åŠ ï¼‰
                if (maxScroll <= 0) {
                    console.log('ğŸ“ å†…å®¹æœªè¶…å‡ºå®¹å™¨ï¼Œä½†å…è®¸æ»šåŠ¨ï¼ˆæµå¼è¾“å‡ºï¼‰');
                    return true; // æ”¹ä¸ºtrueï¼Œå…è®¸æµå¼è¾“å‡ºæ—¶çš„æ»šåŠ¨
                }
                
                // å¦‚æœç”¨æˆ·æ»šåŠ¨åˆ°äº†å¾ˆä¸Šé¢ï¼ˆè¶…è¿‡30%ï¼‰ï¼Œåˆ™æš‚åœè‡ªåŠ¨æ»šåŠ¨
                const scrollPercentage = scrollTop / maxScroll;
                const shouldPause = scrollPercentage < 0.7; // å¦‚æœæ»šåŠ¨ä½ç½®åœ¨å‰70%ï¼Œæš‚åœè‡ªåŠ¨æ»šåŠ¨
                
                if (shouldPause) {
                    console.log('ğŸ“ ç”¨æˆ·æ»šåŠ¨åˆ°è¾ƒä¸Šæ–¹ä½ç½®ï¼Œæš‚åœè‡ªåŠ¨æ»šåŠ¨', {
                        scrollTop,
                        maxScroll,
                        percentage: Math.round(scrollPercentage * 100) + '%'
                    });
                    return false;
                }
                
                console.log('âœ… å…è®¸è‡ªåŠ¨æ»šåŠ¨', {
                    scrollTop,
                    maxScroll,
                    percentage: Math.round(scrollPercentage * 100) + '%'
                });
                return true;
            }
            
            // ç›‘å¬å†…å®¹å˜åŒ–çš„Observer
            function setupContentObserver() {
                const observer = new MutationObserver(function(mutations) {
                    let contentChanged = false;
                    
                    mutations.forEach(function(mutation) {
                        // æ£€æµ‹æ–‡æœ¬å†…å®¹å˜åŒ–ï¼ˆæµå¼æ›´æ–°ï¼‰
                        if (mutation.type === 'characterData') {
                            contentChanged = true;
                        }
                        // æ£€æµ‹å­å…ƒç´ å˜åŒ–ï¼ˆæ–°æ¶ˆæ¯ï¼‰
                        else if (mutation.type === 'childList' && mutation.addedNodes.length > 0) {
                            contentChanged = true;
                        }
                        // æ£€æµ‹å±æ€§å˜åŒ–ï¼ˆå¯èƒ½å½±å“æ»šåŠ¨ï¼‰
                        else if (mutation.type === 'attributes') {
                            contentChanged = true;
                        }
                    });
                    
                    if (contentChanged && autoScrollEnabled && shouldAutoScroll()) {
                        console.log('ğŸ”„ å†…å®¹å˜åŒ–æ£€æµ‹ï¼Œæ‰§è¡Œè‡ªåŠ¨æ»šåŠ¨');
                        // ç«‹å³æ»šåŠ¨ - ä½¿ç”¨å¤šç§æ–¹æ³•
                        const scrolled = forceScrollToBottom() || bruteForceScroll();
                        
                        if (!scrolled) {
                            console.log('âš ï¸ ç«‹å³æ»šåŠ¨å¤±è´¥ï¼Œå»¶è¿Ÿé‡è¯•...');
                            // å»¶è¿Ÿæ»šåŠ¨ä½œä¸ºå¤‡ç”¨
                            setTimeout(() => {
                                if (autoScrollEnabled && shouldAutoScroll()) {
                                    const retryScrolled = forceScrollToBottom() || bruteForceScroll() || fallbackScroll();
                                    if (retryScrolled) {
                                        console.log('âœ… å»¶è¿Ÿæ»šåŠ¨æˆåŠŸ');
                                    } else {
                                        console.log('âŒ æ‰€æœ‰æ»šåŠ¨æ–¹æ³•éƒ½å¤±è´¥äº†');
                                    }
                                }
                            }, 100);
                        } else {
                            console.log('âœ… ç«‹å³æ»šåŠ¨æˆåŠŸ');
                        }
                    } else if (contentChanged && !autoScrollEnabled) {
                        console.log('ğŸ›‘ å†…å®¹å˜åŒ–æ£€æµ‹ï¼Œä½†è‡ªåŠ¨æ»šåŠ¨å·²ç¦ç”¨');
                    }
                });
                
                // ç›‘å¬æ•´ä¸ªèŠå¤©æ¡†åŒºåŸŸ
                const chatbotElements = document.querySelectorAll('.chatbot, gradio-chatbot, [data-testid="chatbot"]');
                chatbotElements.forEach(element => {
                    observer.observe(element, {
                        childList: true,
                        subtree: true,
                        characterData: true,
                        attributes: false // å‡å°‘ä¸å¿…è¦çš„è§¦å‘
                    });
                    console.log('ğŸ“‹ å¼€å§‹ç›‘å¬èŠå¤©æ¡†:', element.tagName);
                });
                
                return observer;
            }
            
            // ç›‘å¬ç”¨æˆ·æ»šåŠ¨è¡Œä¸º
            function setupScrollListener() {
                document.addEventListener('scroll', function(e) {
                    if (e.target.closest && e.target.closest('.chatbot')) {
                        isUserScrolling = true;
                        console.log('ğŸ‘† ç”¨æˆ·æ‰‹åŠ¨æ»šåŠ¨');
                        
                        // æ¸…é™¤ä¹‹å‰çš„è¶…æ—¶
                        if (scrollTimeout) {
                            clearTimeout(scrollTimeout);
                        }
                        
                        // 3ç§’åæ¢å¤è‡ªåŠ¨æ»šåŠ¨
                        scrollTimeout = setTimeout(() => {
                            isUserScrolling = false;
                            console.log('âœ… æ¢å¤è‡ªåŠ¨æ»šåŠ¨');
                        }, 3000);
                    }
                }, true);
            }
            
            // å®šæ—¶å¼ºåˆ¶æ»šåŠ¨ï¼ˆæµå¼èŠå¤©çš„å¼ºåŠ›ä¿éšœï¼‰
            function setupPeriodicScroll() {
                setInterval(() => {
                    if (autoScrollEnabled && !isUserScrolling && shouldAutoScroll()) {
                        // å°è¯•å¤šç§æ»šåŠ¨æ–¹æ³•
                        const scrolled = forceScrollToBottom() || 
                                       bruteForceScroll() || 
                                       fallbackScroll();
                        
                        if (scrolled) {
                            console.log('ğŸ¯ å®šæ—¶æ»šåŠ¨æˆåŠŸ');
                        }
                    }
                }, 500); // æ¯500msæ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿æµå¼å†…å®¹åŠæ—¶æ»šåŠ¨
            }
            
            // æš´åŠ›æ»šåŠ¨æ–¹æ³•ï¼šç›´æ¥æ»šåŠ¨æ‰€æœ‰å¯èƒ½çš„å®¹å™¨
            function bruteForceScroll() {
                console.log('ğŸ’ª æ‰§è¡Œæš´åŠ›æ»šåŠ¨...');
                let scrolled = false;
                
                // è·å–æ‰€æœ‰å¯èƒ½åŒ…å«æ»šåŠ¨å†…å®¹çš„å…ƒç´ 
                const allElements = [
                    ...document.querySelectorAll('.chatbot'),
                    ...document.querySelectorAll('.chatbot *'),
                    ...document.querySelectorAll('gradio-chatbot'),
                    ...document.querySelectorAll('gradio-chatbot *'),
                    ...document.querySelectorAll('[class*="chat"]'),
                    ...document.querySelectorAll('div')
                ];
                
                for (const element of allElements) {
                    if (element && element.scrollHeight > element.clientHeight) {
                        const oldScrollTop = element.scrollTop;
                        element.scrollTop = element.scrollHeight;
                        if (element.scrollTop !== oldScrollTop) {
                            console.log('ğŸ’ª æš´åŠ›æ»šåŠ¨æˆåŠŸ:', element);
                            scrolled = true;
                        }
                    }
                }
                
                return scrolled;
            }
            
            // æœ€åçš„å¤‡ç”¨æ»šåŠ¨æ–¹æ³•
            function fallbackScroll() {
                console.log('ğŸ†˜ æ‰§è¡Œå¤‡ç”¨æ»šåŠ¨æ–¹æ³•...');
                
                // å°è¯•æ»šåŠ¨çª—å£æœ¬èº«
                const oldScrollY = window.scrollY;
                window.scrollTo(0, document.body.scrollHeight);
                if (window.scrollY !== oldScrollY) {
                    console.log('ğŸ†˜ çª—å£æ»šåŠ¨æˆåŠŸ');
                    return true;
                }
                
                // å°è¯•æ»šåŠ¨bodyå’Œhtml
                const targets = [document.body, document.documentElement];
                for (const target of targets) {
                    if (target) {
                        target.scrollTop = target.scrollHeight;
                        console.log('ğŸ†˜ å°è¯•æ»šåŠ¨:', target.tagName);
                    }
                }
                
                return false;
            }
            
            // åˆå§‹åŒ–æ‰€æœ‰åŠŸèƒ½
            function initialize() {
                console.log('âš™ï¸ åˆå§‹åŒ–è‡ªåŠ¨æ»šåŠ¨ç³»ç»Ÿ...');
                console.log('ğŸ“… æ—¶é—´:', new Date().toLocaleTimeString());
                console.log('ğŸŒ document.readyState:', document.readyState);
                console.log('ğŸ“„ DOMå…ƒç´ æ€»æ•°:', document.querySelectorAll('*').length);
                
                // æŸ¥æ‰¾æ»šåŠ¨å®¹å™¨
                const foundContainer = findScrollContainer();
                
                if (foundContainer) {
                    console.log('ğŸ‰ æˆåŠŸæ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œç»§ç»­åˆå§‹åŒ–...');
                    
                    // è®¾ç½®å†…å®¹ç›‘å¬
                    setupContentObserver();
                    
                    // è®¾ç½®æ»šåŠ¨ç›‘å¬
                    setupScrollListener();
                    
                    // è®¾ç½®å®šæ—¶æ»šåŠ¨
                    setupPeriodicScroll();
                    
                    // åˆå§‹æ»šåŠ¨
                    setTimeout(() => {
                        forceScrollToBottom();
                    }, 500);
                    
                    console.log('âœ… è‡ªåŠ¨æ»šåŠ¨ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ');
                } else {
                    console.log('â³ æœªæ‰¾åˆ°æ»šåŠ¨å®¹å™¨ï¼Œå°†åœ¨2ç§’åé‡è¯•...');
                    setTimeout(() => {
                        console.log('ğŸ”„ é‡è¯•åˆå§‹åŒ–æ»šåŠ¨ç³»ç»Ÿ...');
                        initialize();
                    }, 2000);
                }
            }
            
            // æ‰‹åŠ¨æŸ¥æ‰¾å®¹å™¨çš„å‡½æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            function debugFindContainer() {
                console.log('ğŸ› æ‰‹åŠ¨è°ƒè¯•æŸ¥æ‰¾å®¹å™¨...');
                findScrollContainer();
            }
            
            // ç¦ç”¨è‡ªåŠ¨æ»šåŠ¨ï¼ˆæ¸…é™¤å¯¹è¯æ—¶ä½¿ç”¨ï¼‰
            function disableAutoScroll() {
                autoScrollEnabled = false;
                console.log('ğŸ›‘ è‡ªåŠ¨æ»šåŠ¨å·²ç¦ç”¨');
            }
            
            // å¯ç”¨è‡ªåŠ¨æ»šåŠ¨ï¼ˆå¼€å§‹æ–°å¯¹è¯æ—¶ä½¿ç”¨ï¼‰
            function enableAutoScroll() {
                autoScrollEnabled = true;
                console.log('âœ… è‡ªåŠ¨æ»šåŠ¨å·²å¯ç”¨');
                // å¯ç”¨åç«‹å³æ»šåŠ¨åˆ°åº•éƒ¨
                setTimeout(() => {
                    if (autoScrollEnabled) {
                        forceScrollToBottom();
                    }
                }, 100);
            }
            
            // æš´éœ²åˆ°å…¨å±€ï¼Œä¾¿äºæ‰‹åŠ¨è°ƒè¯•å’Œæ¸…é™¤å¯¹è¯æ—¶ä½¿ç”¨
            window.debugScrollContainer = debugFindContainer;
            window.scrollToTop = forceScrollToTop;
            window.scrollToBottom = forceScrollToBottom;
            window.disableAutoScroll = disableAutoScroll;
            window.enableAutoScroll = enableAutoScroll;
            
            // ç­‰å¾…DOMå‡†å¤‡å°±ç»ª
            if (document.readyState === 'loading') {
                document.addEventListener('DOMContentLoaded', initialize);
            } else {
                setTimeout(initialize, 500);
            }
            
            // çª—å£å¤§å°å˜åŒ–æ—¶é‡æ–°æ»šåŠ¨
            window.addEventListener('resize', () => {
                setTimeout(forceScrollToBottom, 200);
            });
        }
        """,
    ) as demo:

        gr.Markdown(
            """
        # ğŸ¤– Vertex Chat
        ### åŸºäº Workflow LLM Vertex çš„æ™ºèƒ½èŠå¤©åŠ©æ‰‹
        
        ä½¿ç”¨ç»Ÿä¸€é…ç½®ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§ LLM æä¾›å•†
        """
        )

        with gr.Row():
            with gr.Column(scale=3):
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="å¯¹è¯",
                    height=500,
                    container=True,
                    elem_classes=["chat-container"],
                    show_copy_button=True,  # æ˜¾ç¤ºå¤åˆ¶æŒ‰é’®
                    bubble_full_width=False,  # æ¶ˆæ¯æ°”æ³¡ä¸å æ»¡å®½åº¦ï¼Œæ›´ç¾è§‚
                )

                with gr.Row():
                    msg = gr.Textbox(placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...", lines=1, scale=4, container=False)
                    image_url_input = gr.Textbox(placeholder="ç²˜è´´å›¾ç‰‡URLï¼ˆå¯é€‰ï¼‰", label="å›¾ç‰‡URL", lines=1, scale=3)
                    send_btn = gr.Button("å‘é€", variant="primary", scale=1, size="sm")

                with gr.Row():
                    clear_btn = gr.Button("æ¸…é™¤å¯¹è¯", variant="secondary")

            with gr.Column(scale=1):
                # é…ç½®é¢æ¿
                gr.Markdown("### âš™ï¸ é…ç½®")

                system_prompt = gr.Textbox(
                    label="ç³»ç»Ÿæç¤º", value=default_system_prompt, lines=4, placeholder="è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯..."
                )

                # æ¨¡å‹ä¿¡æ¯
                gr.Markdown("### ğŸ”§ æ¨¡å‹ä¿¡æ¯")

                # å®‰å…¨è·å–å½“å‰æ¨¡å‹åç§°
                current_model = "æœªçŸ¥"
                if app.llm_model:
                    try:
                        current_model = app.llm_model.model_name()
                    except:
                        current_model = str(app.llm_model)

                model_info = gr.Markdown(f"**å½“å‰æ¨¡å‹:** {current_model}", elem_classes=["model-info"])

                # æ¨¡å‹åˆ‡æ¢ - å…ˆé€‰æ‹©æä¾›å•†ï¼Œå†é€‰æ‹©æ¨¡å‹
                gr.Markdown("#### é€‰æ‹©æä¾›å•†")
                provider_dropdown = gr.Dropdown(
                    label="æä¾›å•†",
                    choices=app.get_available_providers(),
                    interactive=True,
                    info="é€‰æ‹©æä¾›å•†åæ˜¾ç¤ºå¯¹åº”çš„æ¨¡å‹",
                    allow_custom_value=False,
                )

                gr.Markdown("#### é€‰æ‹©æ¨¡å‹")
                model_dropdown = gr.Dropdown(
                    label="æ¨¡å‹", choices=[], interactive=True, info="é€‰æ‹©è¦ä½¿ç”¨çš„å…·ä½“æ¨¡å‹", allow_custom_value=False
                )

                with gr.Row():
                    switch_btn = gr.Button("åˆ‡æ¢æ¨¡å‹", variant="primary", scale=1)
                    refresh_btn = gr.Button("åˆ·æ–°", variant="secondary", scale=1)

                switch_result = gr.Textbox(label="åˆ‡æ¢ç»“æœ", interactive=False, lines=2)

                # æ‰‹åŠ¨è¾“å…¥æ¨¡å¼ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
                with gr.Accordion("ğŸ”§ æ‰‹åŠ¨è¾“å…¥æ¨¡å¼", open=False):
                    provider_input = gr.Textbox(
                        placeholder="è¾“å…¥æä¾›å•†åç§° (å¦‚: deepseek)", label="æ‰‹åŠ¨è¾“å…¥æä¾›å•†", scale=4
                    )
                    manual_switch_btn = gr.Button("æ‰‹åŠ¨åˆ‡æ¢", scale=1)

                # Ollamaæœ¬åœ°æ¨¡å‹ç®¡ç†
                gr.Markdown("### ğŸ  æœ¬åœ°æ¨¡å‹(Ollama)")

                with gr.Row():
                    refresh_ollama_btn = gr.Button("åˆ·æ–°æ¨¡å‹åˆ—è¡¨", scale=1)

                ollama_models = gr.Dropdown(
                    label="å¯ç”¨çš„Ollamaæ¨¡å‹",
                    choices=app.get_ollama_models(),
                    interactive=False,
                    info="å·²å®‰è£…çš„æœ¬åœ°æ¨¡å‹",
                )

                # å·¥å…·ç®¡ç†
                gr.Markdown("### ğŸ› ï¸ å·¥å…·ç®¡ç†")

                tools_enabled = gr.Checkbox(
                    label="å¯ç”¨Function Tools", value=app.tools_enabled, info="å…è®¸AIåŠ©æ‰‹ä½¿ç”¨å·¥å…·æ‰§è¡Œä»»åŠ¡"
                )

                available_tools_display = gr.Dropdown(
                    label="å¯ç”¨å·¥å…·",
                    choices=[f"{tool.name}: {tool.description}" for tool in app.available_tools],
                    interactive=False,
                    info=f"å…±æœ‰ {len(app.available_tools)} ä¸ªå·¥å…·å¯ç”¨",
                )

                # æ€è€ƒè¿‡ç¨‹ç®¡ç†
                gr.Markdown("### ğŸ¤” æ€è€ƒè¿‡ç¨‹")

                enable_reasoning = gr.Checkbox(
                    label="å¯ç”¨æ€è€ƒè¿‡ç¨‹", value=False, info="è®©AIæ˜¾ç¤ºæ¨ç†è¿‡ç¨‹ï¼ˆæ”¯æŒDeepSeek R1ç­‰æ¨¡å‹ï¼‰"
                )

                # MCPç®¡ç†
                gr.Markdown("### ğŸ”— MCPé›†æˆ")

                # MCPå¯ç”¨æ€§æ£€æŸ¥
                mcp_available = app.check_mcp_availability() if MCP_AVAILABLE else False
                mcp_status_text = "âœ… MCPåŠŸèƒ½å¯ç”¨" if mcp_available else "âŒ MCPåŠŸèƒ½ä¸å¯ç”¨"
                if not MCP_AVAILABLE:
                    mcp_status_text += " (æ¨¡å—æœªå®‰è£…)"
                elif not mcp_available:
                    mcp_status_text += " (é…ç½®ç¼ºå¤±)"

                mcp_status = gr.Markdown(f"**çŠ¶æ€:** {mcp_status_text}")

                # å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•åŒºåŸŸ
                with gr.Accordion("ğŸ–¥ï¸ å‘½ä»¤è¡Œå·¥å…·æµ‹è¯•", open=False):
                    cmd_input = gr.Textbox(label="å‘½ä»¤", placeholder="ä¾‹å¦‚: ls -la, python --version, pwd", lines=1)
                    cmd_execute_btn = gr.Button("æ‰§è¡Œå‘½ä»¤", variant="secondary")
                    cmd_result = gr.JSON(label="æ‰§è¡Œç»“æœ", visible=True)

        # äº‹ä»¶ç»‘å®š
        def respond(message, history, sys_prompt, image_url, enable_reasoning_val):
            multimodal_inputs = {}
            # æ–‡æœ¬
            if message:
                multimodal_inputs["text"] = message
            # å›¾ç‰‡URLå¤„ç†
            if image_url and image_url.strip():
                # éªŒè¯å›¾ç‰‡URL
                url = image_url.strip()
                if "discordapp.com" in url or "discord.com" in url:
                    # Discordå›¾ç‰‡å¯èƒ½ä¸è¢«æ”¯æŒï¼Œç»™å‡ºæç¤º
                    yield "", history + [(message or "", "âš ï¸ Discordå›¾ç‰‡é“¾æ¥å¯èƒ½ä¸è¢«æ”¯æŒï¼Œè¯·å°è¯•å…¶ä»–æ–¹å¼ã€‚")]
                    return
                elif "cdn.discordapp.com" in url:
                    # Discord CDNå›¾ç‰‡
                    yield "", history + [(message or "", "âš ï¸ Discord CDNå›¾ç‰‡é“¾æ¥å¯èƒ½ä¸è¢«æ”¯æŒï¼Œè¯·å°è¯•å…¶ä»–æ–¹å¼ã€‚")]
                    return
                else:
                    multimodal_inputs["image_url"] = url

            # ä¼ é€’ç»™chat_with_vertex - MCPçŠ¶æ€ç”±é…ç½®è‡ªåŠ¨å†³å®š
            try:
                for result in app.chat_with_vertex(
                    multimodal_inputs, history, sys_prompt, enable_reasoning_val, enable_reasoning_val
                ):
                    # ç¡®ä¿è¾“å…¥æ¡†å§‹ç»ˆä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä¿æŒå¯è¾“å…¥çŠ¶æ€
                    if isinstance(result, tuple) and len(result) == 2:
                        yield "", result[1]  # è¾“å…¥æ¡†æ¸…ç©ºï¼Œæ›´æ–°èŠå¤©å†å²
                    else:
                        yield "", result  # å…¼å®¹å…¶ä»–æ ¼å¼
            except Exception as e:
                error_msg = f"å¤„ç†å¤±è´¥: {str(e)}"
                if "500" in str(e) and multimodal_inputs.get("image_url"):
                    error_msg = "å›¾ç‰‡å¤„ç†å¤±è´¥ï¼Œå¯èƒ½æ˜¯å›¾ç‰‡æ ¼å¼ä¸æ”¯æŒæˆ–é“¾æ¥æ— æ•ˆã€‚è¯·å°è¯•ï¼š\n1. ä½¿ç”¨å…¶ä»–å›¾ç‰‡\n2. æ£€æŸ¥å›¾ç‰‡é“¾æ¥æ˜¯å¦æœ‰æ•ˆ\n3. ç¡®ä¿å›¾ç‰‡æ ¼å¼ä¸ºå¸¸è§æ ¼å¼ï¼ˆJPGã€PNGç­‰ï¼‰"
                yield "", history + [(message or "", error_msg)]

        def clear_conversation():
            """æ¸…é™¤å¯¹è¯å†å²å¹¶é‡ç½®æ»šåŠ¨ä½ç½®"""
            return [], ""  # åŒæ—¶æ¸…ç©ºèŠå¤©å†å²å’Œè¾“å…¥æ¡†

        def update_models_by_provider(selected_provider):
            """æ ¹æ®é€‰æ‹©çš„æä¾›å•†æ›´æ–°æ¨¡å‹åˆ—è¡¨"""
            if not selected_provider:
                return gr.Dropdown(choices=[])

            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯æä¾›å•†åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            models = app.get_models_by_provider(provider)
            return gr.Dropdown(choices=models)

        def switch_model_by_provider_and_model(selected_provider, selected_model):
            """æ ¹æ®æä¾›å•†å’Œæ¨¡å‹åˆ‡æ¢"""
            if not selected_provider:
                return "âŒ è¯·å…ˆé€‰æ‹©æä¾›å•†", model_info.value

            if not selected_model:
                return "âŒ è¯·é€‰æ‹©æ¨¡å‹", model_info.value

            # ç§»é™¤çŠ¶æ€å›¾æ ‡è·å–çº¯åç§°
            provider = selected_provider.replace("âœ… ", "").replace("âŒ ", "")
            model = selected_model.replace("âœ… ", "").replace("âŒ ", "")

            # å¦‚æœæ¨¡å‹åç§°åŒ…å«"(é»˜è®¤)"æ ‡è®°ï¼Œç§»é™¤å®ƒ
            if " (é»˜è®¤)" in model:
                model = model.replace(" (é»˜è®¤)", "")

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯ç”¨
            if not selected_model.startswith("âœ…"):
                return f"âŒ æ¨¡å‹ {model} å½“å‰ä¸å¯ç”¨", model_info.value

            result = app.switch_model_by_provider_and_name(provider, model)

            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def refresh_provider_list():
            """åˆ·æ–°æä¾›å•†åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_available_providers())

        def manual_switch_model(manual_provider):
            """æ‰‹åŠ¨åˆ‡æ¢æ¨¡å‹ï¼ˆå…¼å®¹æ€§ï¼‰"""
            if not manual_provider:
                return "âŒ è¯·è¾“å…¥æä¾›å•†åç§°", model_info.value

            result = app.switch_model_by_provider_and_name(manual_provider)

            # å®‰å…¨è·å–æ–°æ¨¡å‹åç§°
            new_model_name = "æœªçŸ¥"
            if app.llm_model:
                try:
                    new_model_name = app.llm_model.model_name()
                except:
                    new_model_name = str(app.llm_model)

            new_model_info = f"**å½“å‰æ¨¡å‹:** {new_model_name}"
            return result, new_model_info

        def refresh_ollama_models():
            """åˆ·æ–°Ollamaæ¨¡å‹åˆ—è¡¨"""
            return gr.Dropdown(choices=app.get_ollama_models())

        def toggle_tools(enabled):
            """åˆ‡æ¢å·¥å…·å¯ç”¨çŠ¶æ€"""
            app.tools_enabled = enabled
            status = "âœ… å·²å¯ç”¨" if enabled else "âŒ å·²ç¦ç”¨"
            logger.info(f"å·¥å…·çŠ¶æ€å·²æ›´æ”¹: {status}")
            return f"å·¥å…·çŠ¶æ€: {status}"

        def execute_command_test(command):
            """æµ‹è¯•æ‰§è¡Œå‘½ä»¤"""
            if not command.strip():
                return {"error": "è¯·è¾“å…¥å‘½ä»¤"}

            try:
                # ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·
                if app.available_tools:
                    cmd_tool = app.available_tools[0]  # ç¬¬ä¸€ä¸ªå·¥å…·åº”è¯¥æ˜¯å‘½ä»¤è¡Œå·¥å…·
                    result = cmd_tool.execute({"command": command})
                    return result
                else:
                    return {"error": "å‘½ä»¤è¡Œå·¥å…·æœªåˆå§‹åŒ–"}
            except Exception as e:
                return {"error": f"æ‰§è¡Œå¤±è´¥: {str(e)}"}

        def get_mcp_status():
            """è·å–MCPçŠ¶æ€ä¿¡æ¯"""
            if not MCP_AVAILABLE:
                return "âŒ MCPåŠŸèƒ½ä¸å¯ç”¨ (æ¨¡å—æœªå®‰è£…)"

            mcp_available = app.check_mcp_availability()
            if not mcp_available:
                return "âŒ MCPåŠŸèƒ½ä¸å¯ç”¨ (é…ç½®ç¼ºå¤±)"

            # è·å–æ›´è¯¦ç»†çš„çŠ¶æ€ä¿¡æ¯
            status_parts = ["âœ… MCPåŠŸèƒ½å¯ç”¨"]

            if app.get_mcp_manager():
                try:
                    # å¯ä»¥æ·»åŠ æ›´å¤šçŠ¶æ€æ£€æŸ¥
                    status_parts.append("- MCPç®¡ç†å™¨å·²åˆå§‹åŒ–")
                except Exception as e:
                    status_parts.append(f"- MCPç®¡ç†å™¨é”™è¯¯: {e}")
            else:
                status_parts.append("- MCPç®¡ç†å™¨æœªåˆå§‹åŒ–")

            return "\n".join(status_parts)

        # ç»‘å®šå‘é€æ¶ˆæ¯äº‹ä»¶ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        msg.submit(
            respond,
            inputs=[msg, chatbot, system_prompt, image_url_input, enable_reasoning],
            outputs=[msg, chatbot],
            show_progress="minimal",
        )
        send_btn.click(
            respond,
            inputs=[msg, chatbot, system_prompt, image_url_input, enable_reasoning],
            outputs=[msg, chatbot],
            show_progress="minimal",
        )

        # JavaScriptäº‹ä»¶å¤„ç† - å•ç‹¬ç»‘å®šä»¥é¿å…å¹²æ‰°ä¸»è¦åŠŸèƒ½
        msg.submit(
            js="""
            function() {
                console.log('ğŸ’¬ å¼€å§‹æ–°å¯¹è¯ï¼Œå¯ç”¨è‡ªåŠ¨æ»šåŠ¨');
                if (window.enableAutoScroll) {
                    window.enableAutoScroll();
                } else {
                    console.log('âš ï¸ enableAutoScrollå‡½æ•°æœªæ‰¾åˆ°');
                }
            }
        """
        )
        send_btn.click(
            js="""
            function() {
                console.log('ğŸ’¬ å¼€å§‹æ–°å¯¹è¯ï¼Œå¯ç”¨è‡ªåŠ¨æ»šåŠ¨');
                if (window.enableAutoScroll) {
                    window.enableAutoScroll();
                } else {
                    console.log('âš ï¸ enableAutoScrollå‡½æ•°æœªæ‰¾åˆ°');
                }
            }
        """
        )

        # ç»‘å®šæ¸…é™¤å¯¹è¯äº‹ä»¶
        clear_btn.click(clear_conversation, outputs=[chatbot, msg])

        # JavaScriptäº‹ä»¶å¤„ç† - å•ç‹¬ç»‘å®šä»¥é¿å…å¹²æ‰°ä¸»è¦åŠŸèƒ½
        clear_btn.click(
            js="""
            function() {
                console.log('ğŸ§¹ æ¸…é™¤å¯¹è¯ï¼Œç¦ç”¨è‡ªåŠ¨æ»šåŠ¨');
                
                // ç¦ç”¨è‡ªåŠ¨æ»šåŠ¨
                if (window.disableAutoScroll) {
                    window.disableAutoScroll();
                } else {
                    console.log('âš ï¸ disableAutoScrollå‡½æ•°æœªæ‰¾åˆ°');
                }
                
                // æ»šåŠ¨åˆ°é¡¶éƒ¨ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼Œä¸å†é‡å¤ï¼‰
                function scrollToTopOnce() {
                    // ä½¿ç”¨å…¨å±€å‡½æ•°
                    if (window.scrollToTop) {
                        window.scrollToTop();
                        console.log('âœ… ä½¿ç”¨å…¨å±€å‡½æ•°æ»šåŠ¨åˆ°é¡¶éƒ¨');
                        return;
                    }
                    
                    // å¤‡ç”¨æ–¹æ¡ˆ
                    const chatbot = document.querySelector('.chatbot');
                    if (chatbot) {
                        const scrollContainer = chatbot.querySelector('div[style*="overflow"], .overflow-y-auto') || chatbot.querySelector('div');
                        if (scrollContainer) {
                            scrollContainer.scrollTop = 0;
                            console.log('âœ… ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæ»šåŠ¨åˆ°é¡¶éƒ¨');
                        }
                    }
                    
                    // å…¨å±€æ»šåŠ¨é‡ç½®
                    document.body.scrollTop = 0;
                    document.documentElement.scrollTop = 0;
                    window.scrollTo(0, 0);
                }
                
                // å»¶è¿Ÿæ‰§è¡Œæ»šåŠ¨åˆ°é¡¶éƒ¨ï¼Œç¡®ä¿DOMæ›´æ–°å®Œæˆ
                setTimeout(scrollToTopOnce, 100);
                
                console.log('ğŸ¯ æ¸…é™¤å¯¹è¯å®Œæˆï¼Œè‡ªåŠ¨æ»šåŠ¨å·²ç¦ç”¨');
            }
        """
        )

        # ç»‘å®šæä¾›å•†é€‰æ‹©äº‹ä»¶ - æ›´æ–°æ¨¡å‹åˆ—è¡¨
        provider_dropdown.change(update_models_by_provider, inputs=[provider_dropdown], outputs=[model_dropdown])

        # ç»‘å®šæ¨¡å‹åˆ‡æ¢äº‹ä»¶
        switch_btn.click(
            switch_model_by_provider_and_model,
            inputs=[provider_dropdown, model_dropdown],
            outputs=[switch_result, model_info],
        )

        # ç»‘å®šåˆ·æ–°äº‹ä»¶
        refresh_btn.click(refresh_provider_list, outputs=[provider_dropdown])

        # ç»‘å®šæ‰‹åŠ¨åˆ‡æ¢äº‹ä»¶
        manual_switch_btn.click(manual_switch_model, inputs=[provider_input], outputs=[switch_result, model_info])

        # ç»‘å®šOllamaæ¨¡å‹åˆ·æ–°äº‹ä»¶
        refresh_ollama_btn.click(refresh_ollama_models, outputs=[ollama_models])

        # ç»‘å®šå·¥å…·å¯ç”¨åˆ‡æ¢äº‹ä»¶
        tools_enabled.change(toggle_tools, inputs=[tools_enabled], outputs=[])

        # ç»‘å®šå‘½ä»¤æ‰§è¡Œäº‹ä»¶
        cmd_execute_btn.click(execute_command_test, inputs=[cmd_input], outputs=[cmd_result])

    return demo


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()

    try:
        # åˆå§‹åŒ–åº”ç”¨
        logger.info("æ­£åœ¨åˆå§‹åŒ– Vertex Chat åº”ç”¨...")
        app = WorkflowChatApp(config_path=args.config)

        # åˆ›å»º Gradio ç•Œé¢
        demo = create_gradio_interface(app)

        # å¯åŠ¨åº”ç”¨
        logger.info(f"å¯åŠ¨ Vertex Chat åº”ç”¨åœ¨ {args.host}:{args.port}")
        demo.launch(server_name=args.host, server_port=args.port, share=args.share, show_error=True)

    except Exception as e:
        logger.error(f"åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("\nè¯·æ£€æŸ¥:")
        print("1. é…ç½®æ–‡ä»¶æ˜¯å¦æ­£ç¡® (vertex_flow/config/llm.yml)")
        print("2. æ˜¯å¦æœ‰å¯ç”¨çš„ LLM æä¾›å•†")
        print("3. API å¯†é’¥æ˜¯å¦é…ç½®æ­£ç¡®")
        return 1


if __name__ == "__main__":
    exit(main())
