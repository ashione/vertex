#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é¡¶ç‚¹ç»„åŠŸèƒ½ç»¼åˆæµ‹è¯•
åŒ…å«æµå¼å‚æ•°ä¼ é€’ã€å­å›¾æ‰§è¡Œç­‰åŠŸèƒ½æµ‹è¯•
"""

import os
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.constants import ENABLE_STREAM, SYSTEM, USER
from vertex_flow.workflow.context import WorkflowContext
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex
from vertex_flow.workflow.vertex.vertex_group import VertexGroup

logger = LoggerUtil.get_logger()


class MockMessage:
    """æ¨¡æ‹Ÿæ¶ˆæ¯å¯¹è±¡"""

    def __init__(self, content):
        self.content = content


class MockChoice:
    """æ¨¡æ‹Ÿé€‰æ‹©å¯¹è±¡"""

    def __init__(self, content, finish_reason="stop"):
        self.message = MockMessage(content)
        self.finish_reason = finish_reason


class MockResponse:
    """æ¨¡æ‹Ÿå“åº”å¯¹è±¡"""

    def __init__(self, content):
        self.choices = [MockChoice(content)]


class MockChatModel:
    """æ¨¡æ‹ŸèŠå¤©æ¨¡å‹"""

    def __init__(self):
        self.responses = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å“åº”"]
        self.response_index = 0

    def chat(self, messages, **kwargs):
        response = self.responses[self.response_index % len(self.responses)]
        self.response_index += 1
        return MockChoice(response)

    def chat_stream(self, messages, **kwargs):
        response = self.responses[self.response_index % len(self.responses)]
        self.response_index += 1
        # æ¨¡æ‹Ÿæµå¼è¾“å‡º
        for char in response:
            yield MockChoice(char)

    def model_name(self):
        return "MockModel"


def test_vertex_group_streaming_parameter_passing():
    """æµ‹è¯•VertexGroupä¸­LLMVertexçš„æµå¼å‚æ•°ä¼ é€’"""
    print("=== æµ‹è¯•VertexGroupä¸­LLMVertexçš„æµå¼å‚æ•°ä¼ é€’ ===")

    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        mock_model = MockChatModel()

        # åˆ›å»ºVertexGroup
        vertex_group = VertexGroup(id="test_group")

        # åˆ›å»ºLLMVertexï¼ˆåˆå§‹æ—¶ä¸å¯ç”¨æµå¼ï¼‰
        llm_vertex = LLMVertex(
            id="test_llm",
            params={
                "model": mock_model,
                SYSTEM: "ä½ æ˜¯ä¸€ä¸ªæµ‹è¯•åŠ©æ‰‹",
                USER: ["è¯·å›ç­”æµ‹è¯•é—®é¢˜"],
                ENABLE_STREAM: False,  # åˆå§‹è®¾ç½®ä¸ºFalse
            },
        )

        # æ·»åŠ åˆ°å­å›¾
        vertex_group.add_subgraph_vertex(llm_vertex)

        # åˆ›å»ºå·¥ä½œæµä¸Šä¸‹æ–‡
        context = WorkflowContext()

        # æµ‹è¯•1: ä¸å¯ç”¨æµå¼çš„æƒ…å†µ
        print("\næµ‹è¯•1: ä¸å¯ç”¨æµå¼è¾“å‡º")
        inputs_no_stream = {"test_input": "æµ‹è¯•æ•°æ®"}

        print(f"æ‰§è¡Œå‰ LLMVertex ENABLE_STREAM: {llm_vertex.params.get(ENABLE_STREAM)}")
        vertex_group.execute_subgraph(inputs_no_stream, context)
        print(f"æ‰§è¡Œå LLMVertex ENABLE_STREAM: {llm_vertex.params.get(ENABLE_STREAM)}")

        # éªŒè¯æµå¼å‚æ•°æœªè¢«ä¿®æ”¹
        assert llm_vertex.params.get(ENABLE_STREAM) == False, "ä¸å¯ç”¨æµå¼æ—¶ï¼ŒENABLE_STREAMåº”è¯¥ä¿æŒFalse"

        # æµ‹è¯•2: å¯ç”¨æµå¼çš„æƒ…å†µ
        print("\næµ‹è¯•2: å¯ç”¨æµå¼è¾“å‡º")
        inputs_with_stream = {"test_input": "æµ‹è¯•æ•°æ®", "stream": True}

        # é‡ç½®LLMVertexçš„ENABLE_STREAMå‚æ•°
        llm_vertex.params[ENABLE_STREAM] = False
        print(f"æ‰§è¡Œå‰ LLMVertex ENABLE_STREAM: {llm_vertex.params.get(ENABLE_STREAM)}")

        vertex_group.execute_subgraph(inputs_with_stream, context)
        print(f"æ‰§è¡Œå LLMVertex ENABLE_STREAM: {llm_vertex.params.get(ENABLE_STREAM)}")

        # éªŒè¯ç»“æœ
        if llm_vertex.params.get(ENABLE_STREAM) == True:
            print("âœ… æµ‹è¯•é€šè¿‡ï¼šVertexGroupæ­£ç¡®è®¾ç½®äº†LLMVertexçš„æµå¼å‚æ•°")
            return True
        else:
            print("âŒ æµ‹è¯•å¤±è´¥ï¼šVertexGroupæœªèƒ½æ­£ç¡®è®¾ç½®LLMVertexçš„æµå¼å‚æ•°")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼Œå‡ºç°å¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_vertex_group_subgraph_execution():
    """æµ‹è¯•VertexGroupçš„å­å›¾æ‰§è¡ŒåŠŸèƒ½"""
    print("\n=== æµ‹è¯•VertexGroupå­å›¾æ‰§è¡ŒåŠŸèƒ½ ===")

    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        mock_model = MockChatModel()

        # åˆ›å»ºVertexGroup
        vertex_group = VertexGroup(id="subgraph_test_group", name="å­å›¾æµ‹è¯•ç»„")

        # åˆ›å»ºå¤šä¸ªLLMVertex
        llm_vertex1 = LLMVertex(
            id="llm1",
            params={
                "model": mock_model,
                SYSTEM: "ä½ æ˜¯åŠ©æ‰‹1",
                USER: ["é—®é¢˜1"],
                ENABLE_STREAM: False,
            },
        )

        llm_vertex2 = LLMVertex(
            id="llm2",
            params={
                "model": mock_model,
                SYSTEM: "ä½ æ˜¯åŠ©æ‰‹2",
                USER: ["é—®é¢˜2"],
                ENABLE_STREAM: False,
            },
        )

        # æ·»åŠ åˆ°å­å›¾
        vertex_group.add_subgraph_vertex(llm_vertex1)
        vertex_group.add_subgraph_vertex(llm_vertex2)

        # åˆ›å»ºå·¥ä½œæµä¸Šä¸‹æ–‡
        context = WorkflowContext()

        # æ‰§è¡Œå­å›¾
        inputs = {"test_data": "å­å›¾æµ‹è¯•æ•°æ®"}
        result = vertex_group.execute_subgraph(inputs, context)

        print(f"å­å›¾æ‰§è¡Œç»“æœ: {result}")

        # éªŒè¯å­å›¾ä¸­çš„é¡¶ç‚¹éƒ½è¢«æ‰§è¡Œäº†
        assert len(vertex_group.subgraph_vertices) == 2, "åº”è¯¥æœ‰2ä¸ªå­å›¾é¡¶ç‚¹"
        print("âœ… å­å›¾æ‰§è¡ŒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_vertex_group_streaming_propagation():
    """æµ‹è¯•VertexGroupä¸­æµå¼å‚æ•°çš„ä¼ æ’­æœºåˆ¶"""
    print("\n=== æµ‹è¯•VertexGroupæµå¼å‚æ•°ä¼ æ’­æœºåˆ¶ ===")

    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ¨¡å‹
        mock_model = MockChatModel()

        # åˆ›å»ºVertexGroup
        vertex_group = VertexGroup(id="propagation_test_group")

        # åˆ›å»ºå¤šä¸ªLLMVertexï¼Œåˆå§‹éƒ½ä¸å¯ç”¨æµå¼
        llm_vertices = []
        for i in range(3):
            llm_vertex = LLMVertex(
                id=f"llm_{i}",
                params={
                    "model": mock_model,
                    SYSTEM: f"ä½ æ˜¯åŠ©æ‰‹{i}",
                    USER: [f"é—®é¢˜{i}"],
                    ENABLE_STREAM: False,
                },
            )
            llm_vertices.append(llm_vertex)
            vertex_group.add_subgraph_vertex(llm_vertex)

        # åˆ›å»ºå·¥ä½œæµä¸Šä¸‹æ–‡
        context = WorkflowContext()

        # æµ‹è¯•æµå¼å‚æ•°ä¼ æ’­
        print("\næµ‹è¯•æµå¼å‚æ•°ä¼ æ’­")
        inputs_with_stream = {"test_data": "ä¼ æ’­æµ‹è¯•", "stream": True}

        # æ‰§è¡Œå‰æ£€æŸ¥æ‰€æœ‰LLMVertexçš„æµå¼çŠ¶æ€
        print("æ‰§è¡Œå‰çš„æµå¼çŠ¶æ€:")
        for i, vertex in enumerate(llm_vertices):
            print(f"  LLM{i}: {vertex.params.get(ENABLE_STREAM)}")
            assert vertex.params.get(ENABLE_STREAM) == False, f"LLM{i}åˆå§‹åº”è¯¥ä¸å¯ç”¨æµå¼"

        # æ‰§è¡Œå­å›¾
        vertex_group.execute_subgraph(inputs_with_stream, context)

        # æ‰§è¡Œåæ£€æŸ¥æ‰€æœ‰LLMVertexçš„æµå¼çŠ¶æ€
        print("æ‰§è¡Œåçš„æµå¼çŠ¶æ€:")
        all_streaming_enabled = True
        for i, vertex in enumerate(llm_vertices):
            stream_enabled = vertex.params.get(ENABLE_STREAM)
            print(f"  LLM{i}: {stream_enabled}")
            if not stream_enabled:
                all_streaming_enabled = False

        if all_streaming_enabled:
            print("âœ… æµå¼å‚æ•°ä¼ æ’­æµ‹è¯•é€šè¿‡ï¼šæ‰€æœ‰LLMVertexéƒ½å¯ç”¨äº†æµå¼")
            return True
        else:
            print("âŒ æµå¼å‚æ•°ä¼ æ’­æµ‹è¯•å¤±è´¥ï¼šéƒ¨åˆ†LLMVertexæœªå¯ç”¨æµå¼")
            return False

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def test_vertex_group_edge_cases():
    """æµ‹è¯•VertexGroupçš„è¾¹ç•Œæƒ…å†µ"""
    print("\n=== æµ‹è¯•VertexGroupè¾¹ç•Œæƒ…å†µ ===")

    try:
        # æµ‹è¯•ç©ºå­å›¾
        empty_group = VertexGroup(id="empty_group")
        context = WorkflowContext()

        result = empty_group.execute_subgraph({"test": "data"}, context)
        print(f"ç©ºå­å›¾æ‰§è¡Œç»“æœ: {result}")

        # æµ‹è¯•æ— æ•ˆè¾“å…¥
        vertex_group = VertexGroup(id="edge_case_group")
        mock_model = MockChatModel()

        llm_vertex = LLMVertex(
            id="edge_llm",
            params={
                "model": mock_model,
                SYSTEM: "æµ‹è¯•åŠ©æ‰‹",
                USER: ["æµ‹è¯•é—®é¢˜"],
                ENABLE_STREAM: False,
            },
        )
        vertex_group.add_subgraph_vertex(llm_vertex)

        # æµ‹è¯•Noneè¾“å…¥
        result_none = vertex_group.execute_subgraph(None, context)
        print(f"Noneè¾“å…¥æ‰§è¡Œç»“æœ: {result_none}")

        # æµ‹è¯•ç©ºå­—å…¸è¾“å…¥
        result_empty = vertex_group.execute_subgraph({}, context)
        print(f"ç©ºå­—å…¸è¾“å…¥æ‰§è¡Œç»“æœ: {result_empty}")

        print("âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡")
        return True

    except Exception as e:
        print(f"âŒ è¾¹ç•Œæƒ…å†µæµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹é¡¶ç‚¹ç»„åŠŸèƒ½ç»¼åˆæµ‹è¯•...\n")

    tests = [
        test_vertex_group_streaming_parameter_passing,
        test_vertex_group_subgraph_execution,
        test_vertex_group_streaming_propagation,
        test_vertex_group_edge_cases,
    ]

    passed = 0
    failed = 0

    for test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å¼‚å¸¸: {e}")
            failed += 1

    print(f"\n=== æµ‹è¯•ç»“æœæ±‡æ€» ===")
    print(f"é€šè¿‡: {passed}")
    print(f"å¤±è´¥: {failed}")
    print(f"æ€»è®¡: {passed + failed}")

    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰é¡¶ç‚¹ç»„åŠŸèƒ½æµ‹è¯•é€šè¿‡ï¼")
        return True
    else:
        print(f"\nğŸ’¥ æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
