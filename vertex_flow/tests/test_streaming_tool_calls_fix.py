#!/usr/bin/env python3
"""
æµ‹è¯•æµå¼å·¥å…·è°ƒç”¨ä¿®å¤
éªŒè¯mcp_llm_vertexå’Œllm_vertexåœ¨stream chatæ—¶å·¥å…·è°ƒç”¨ä¸ä¼šè¢«ä¸­æ–­
"""

import json
import time
from typing import Dict, Any, Generator

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.chat import ChatModel
from vertex_flow.workflow.context import WorkflowContext
from vertex_flow.workflow.tools.functions import FunctionTool
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex
from vertex_flow.workflow.vertex.mcp_llm_vertex import MCPLLMVertex

logger = LoggerUtil.get_logger(__name__)


def create_test_tool():
    """åˆ›å»ºæµ‹è¯•å·¥å…·"""
    def test_func(arguments: Dict[str, Any], context) -> str:
        return f"å·¥å…·è°ƒç”¨æˆåŠŸï¼Œå‚æ•°: {arguments}"
    
    return FunctionTool(
        name="test_tool",
        description="æµ‹è¯•å·¥å…·ï¼Œç”¨äºéªŒè¯æµå¼å·¥å…·è°ƒç”¨",
        func=test_func,
        schema={
            "type": "object",
            "properties": {
                "message": {"type": "string", "description": "æµ‹è¯•æ¶ˆæ¯"},
            },
            "required": ["message"],
        },
    )


class MockStreamingChatModel(ChatModel):
    """æ¨¡æ‹Ÿæ”¯æŒæµå¼è¾“å‡ºçš„èŠå¤©æ¨¡å‹"""
    
    def __init__(self):
        super().__init__(name="mock-streaming-model", sk="mock-key", base_url="mock-url", provider="mock")
        self.call_count = 0
        self.max_calls = 5  # é™åˆ¶æœ€å¤§è°ƒç”¨æ¬¡æ•°
    
    def chat_stream(self, messages, option=None, tools=None):
        """æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ŒåŒ…å«å·¥å…·è°ƒç”¨"""
        self.call_count += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if self.call_count > self.max_calls:
            logger.warning(f"MockStreamingChatModelè¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶: {self.max_calls}")
            return
        
        # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦å·²æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        has_tool_result = any(msg.get("role") == "tool" for msg in messages)
        
        # å¦‚æœå·²æœ‰å·¥å…·ç»“æœï¼Œè¿”å›æœ€ç»ˆå“åº”
        if has_tool_result:
            # æ¨¡æ‹Ÿæœ€ç»ˆå“åº”
            final_chunks = [
                self._create_chunk("å·¥å…·è°ƒç”¨å®Œæˆï¼Œç»“æœå·²å¤„ç†ã€‚"),
                self._create_chunk("è¿™æ˜¯æœ€ç»ˆçš„æ€»ç»“ã€‚")
            ]
            for chunk in final_chunks:
                yield chunk
            return
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè¿”å›å·¥å…·è°ƒç”¨
        tool_call_chunks = [
            self._create_chunk("", tool_calls=[{
                "id": "call_test_123",
                "type": "function",
                "function": {
                    "name": "test_tool",
                    "arguments": json.dumps({"message": "æµ‹è¯•æµå¼å·¥å…·è°ƒç”¨"})
                }
            }])
        ]
        
        for chunk in tool_call_chunks:
            yield chunk
    
    def chat(self, messages, option=None, tools=None):
        """æ¨¡æ‹Ÿéæµå¼èŠå¤©"""
        self.call_count += 1
        
        # é˜²æ­¢æ— é™å¾ªç¯
        if self.call_count > self.max_calls:
            return self._create_choice("è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶", "stop")
        
        # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦å·²æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        has_tool_result = any(msg.get("role") == "tool" for msg in messages)
        
        # å¦‚æœå·²æœ‰å·¥å…·ç»“æœï¼Œè¿”å›æœ€ç»ˆå“åº”
        if has_tool_result:
            return self._create_choice("å·¥å…·è°ƒç”¨å®Œæˆï¼Œç»“æœå·²å¤„ç†ã€‚", "stop")
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè¿”å›å·¥å…·è°ƒç”¨
        return self._create_choice("", "tool_calls", tool_calls=[{
            "id": "call_test_123",
            "type": "function",
            "function": {
                "name": "test_tool",
                "arguments": json.dumps({"message": "æµ‹è¯•éæµå¼å·¥å…·è°ƒç”¨"})
            }
        }])
    
    def _create_chunk(self, content: str, tool_calls=None):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„chunkå¯¹è±¡"""
        class MockDelta:
            def __init__(self, content=None, tool_calls=None):
                self.content = content
                self.tool_calls = tool_calls
        
        class MockChoice:
            def __init__(self, delta):
                self.delta = delta
        
        class MockChunk:
            def __init__(self, choices):
                self.choices = choices
        
        delta = MockDelta(content, tool_calls)
        choice = MockChoice(delta)
        return MockChunk([choice])
    
    def _create_choice(self, content: str, finish_reason: str, tool_calls=None):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„choiceå¯¹è±¡"""
        class MockMessage:
            def __init__(self, content, tool_calls=None):
                self.content = content
                self.tool_calls = tool_calls
                self.role = "assistant"
        
        class MockChoice:
            def __init__(self, content, finish_reason, tool_calls=None):
                self.message = MockMessage(content, tool_calls)
                self.finish_reason = finish_reason
        
        return MockChoice(content, finish_reason, tool_calls)


def test_llm_vertex_streaming_tool_calls():
    """æµ‹è¯•LLMVertexçš„æµå¼å·¥å…·è°ƒç”¨"""
    logger.info("=== æµ‹è¯•LLMVertexæµå¼å·¥å…·è°ƒç”¨ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•å·¥å…·
        test_tool = create_test_tool()
        
        # åˆ›å»ºLLMVertex
        llm_vertex = LLMVertex(
            id="test_llm_streaming",
            name="æµ‹è¯•LLMæµå¼",
            model=MockStreamingChatModel(),
            params={
                "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨test_toolå·¥å…·ã€‚",
                "user": [],
                "enable_stream": True,
                "enable_reasoning": False,
                "show_reasoning": False,
            },
            tools=[test_tool],
        )
        
        # è®¾ç½®æ¶ˆæ¯
        llm_vertex.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨test_toolå·¥å…·ã€‚"},
            {"role": "user", "content": "è¯·è°ƒç”¨test_toolå·¥å…·è¿›è¡Œæµ‹è¯•"},
        ]
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = WorkflowContext()
        
        print("ğŸ§ª å¼€å§‹æµ‹è¯•LLMVertexæµå¼å·¥å…·è°ƒç”¨...")
        print("=" * 50)
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        print("ğŸ“¤ æµå¼è¾“å‡º:")
        chunk_count = 0
        full_response = ""
        
        for chunk in llm_vertex.chat_stream_generator({}, context):
            chunk_count += 1
            full_response += str(chunk)
            print(f"  Chunk {chunk_count}: {chunk}")
        
        print(f"\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")
        print(f"ğŸ“ å®Œæ•´å“åº”: {full_response}")
        
        # æ£€æŸ¥messagesä¸­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        print("\nğŸ“‹ æ£€æŸ¥messages:")
        for i, msg in enumerate(llm_vertex.messages):
            print(f"  Message {i}: {msg}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        tool_messages = [msg for msg in llm_vertex.messages if msg.get("role") == "tool"]
        if tool_messages:
            print(f"\nğŸ› ï¸ å‘ç° {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœ:")
            for msg in tool_messages:
                print(f"  Tool: {msg}")
        else:
            print("\nâš ï¸ æœªå‘ç°å·¥å…·è°ƒç”¨ç»“æœ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_mcp_llm_vertex_streaming_tool_calls():
    """æµ‹è¯•MCPLLMVertexçš„æµå¼å·¥å…·è°ƒç”¨"""
    logger.info("=== æµ‹è¯•MCPLLMVertexæµå¼å·¥å…·è°ƒç”¨ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•å·¥å…·
        test_tool = create_test_tool()
        
        # åˆ›å»ºMCPLLMVertex
        mcp_llm_vertex = MCPLLMVertex(
            id="test_mcp_llm_streaming",
            name="æµ‹è¯•MCPLLMæµå¼",
            model=MockStreamingChatModel(),
            params={
                "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨test_toolå·¥å…·ã€‚",
                "user": [],
                "enable_stream": True,
                "enable_reasoning": False,
                "show_reasoning": False,
            },
            tools=[test_tool],
            mcp_enabled=False,  # ç¦ç”¨MCPä»¥é¿å…ä¾èµ–é—®é¢˜
        )
        
        # è®¾ç½®æ¶ˆæ¯
        mcp_llm_vertex.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨test_toolå·¥å…·ã€‚"},
            {"role": "user", "content": "è¯·è°ƒç”¨test_toolå·¥å…·è¿›è¡Œæµ‹è¯•"},
        ]
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = WorkflowContext()
        
        print("ğŸ§ª å¼€å§‹æµ‹è¯•MCPLLMVertexæµå¼å·¥å…·è°ƒç”¨...")
        print("=" * 50)
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        print("ğŸ“¤ æµå¼è¾“å‡º:")
        chunk_count = 0
        full_response = ""
        
        for chunk in mcp_llm_vertex.chat_stream_generator({}, context):
            chunk_count += 1
            full_response += str(chunk)
            print(f"  Chunk {chunk_count}: {chunk}")
        
        print(f"\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")
        print(f"ğŸ“ å®Œæ•´å“åº”: {full_response}")
        
        # æ£€æŸ¥messagesä¸­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        print("\nğŸ“‹ æ£€æŸ¥messages:")
        for i, msg in enumerate(mcp_llm_vertex.messages):
            print(f"  Message {i}: {msg}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        tool_messages = [msg for msg in mcp_llm_vertex.messages if msg.get("role") == "tool"]
        if tool_messages:
            print(f"\nğŸ› ï¸ å‘ç° {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœ:")
            for msg in tool_messages:
                print(f"  Tool: {msg}")
        else:
            print("\nâš ï¸ æœªå‘ç°å·¥å…·è°ƒç”¨ç»“æœ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_multiple_tool_calls():
    """æµ‹è¯•å¤šæ¬¡å·¥å…·è°ƒç”¨"""
    logger.info("=== æµ‹è¯•å¤šæ¬¡å·¥å…·è°ƒç”¨ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•å·¥å…·
        test_tool = create_test_tool()
        
        # åˆ›å»ºæ”¯æŒå¤šæ¬¡å·¥å…·è°ƒç”¨çš„æ¨¡æ‹Ÿæ¨¡å‹
        class MultiToolMockModel(MockStreamingChatModel):
            def __init__(self):
                super().__init__()
                self.tool_call_count = 0
                self.max_tool_calls = 2
            
            def chat_stream(self, messages, option=None, tools=None):
                self.call_count += 1
                
                # é˜²æ­¢æ— é™å¾ªç¯
                if self.call_count > self.max_calls:
                    return
                
                # æ£€æŸ¥å·¥å…·è°ƒç”¨æ¬¡æ•°
                tool_results = [msg for msg in messages if msg.get("role") == "tool"]
                
                if len(tool_results) >= self.max_tool_calls:
                    # æ‰€æœ‰å·¥å…·è°ƒç”¨å®Œæˆï¼Œè¿”å›æœ€ç»ˆå“åº”
                    final_chunks = [
                        self._create_chunk("æ‰€æœ‰å·¥å…·è°ƒç”¨å·²å®Œæˆã€‚"),
                        self._create_chunk("è¿™æ˜¯æœ€ç»ˆçš„æ€»ç»“ã€‚")
                    ]
                    for chunk in final_chunks:
                        yield chunk
                    return
                
                # è¿”å›å·¥å…·è°ƒç”¨
                tool_call_chunks = [
                    self._create_chunk("", tool_calls=[{
                        "id": f"call_test_{self.tool_call_count}",
                        "type": "function",
                        "function": {
                            "name": "test_tool",
                            "arguments": json.dumps({"message": f"ç¬¬{self.tool_call_count + 1}æ¬¡å·¥å…·è°ƒç”¨"})
                        }
                    }])
                ]
                
                self.tool_call_count += 1
                
                for chunk in tool_call_chunks:
                    yield chunk
        
        # åˆ›å»ºLLMVertex
        llm_vertex = LLMVertex(
            id="test_multi_tool_llm",
            name="æµ‹è¯•å¤šæ¬¡å·¥å…·è°ƒç”¨LLM",
            model=MultiToolMockModel(),
            params={
                "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨test_toolå·¥å…·ã€‚",
                "user": [],
                "enable_stream": True,
                "enable_reasoning": False,
                "show_reasoning": False,
            },
            tools=[test_tool],
        )
        
        # è®¾ç½®æ¶ˆæ¯
        llm_vertex.messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥å¤šæ¬¡è°ƒç”¨test_toolå·¥å…·ã€‚"},
            {"role": "user", "content": "è¯·å¤šæ¬¡è°ƒç”¨test_toolå·¥å…·è¿›è¡Œæµ‹è¯•"},
        ]
        
        # åˆ›å»ºä¸Šä¸‹æ–‡
        context = WorkflowContext()
        
        print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤šæ¬¡å·¥å…·è°ƒç”¨...")
        print("=" * 50)
        
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        print("ğŸ“¤ æµå¼è¾“å‡º:")
        chunk_count = 0
        full_response = ""
        
        for chunk in llm_vertex.chat_stream_generator({}, context):
            chunk_count += 1
            full_response += str(chunk)
            print(f"  Chunk {chunk_count}: {chunk}")
        
        print(f"\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")
        print(f"ğŸ“ å®Œæ•´å“åº”: {full_response}")
        
        # æ£€æŸ¥å·¥å…·è°ƒç”¨ç»“æœ
        tool_messages = [msg for msg in llm_vertex.messages if msg.get("role") == "tool"]
        print(f"\nğŸ› ï¸ å‘ç° {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœ:")
        for i, msg in enumerate(tool_messages):
            print(f"  Tool {i+1}: {msg}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("å¼€å§‹æµå¼å·¥å…·è°ƒç”¨ä¿®å¤æµ‹è¯•...")
    
    # æµ‹è¯•LLMVertex
    llm_success = test_llm_vertex_streaming_tool_calls()
    
    # æµ‹è¯•MCPLLMVertex
    mcp_success = test_mcp_llm_vertex_streaming_tool_calls()
    
    # æµ‹è¯•å¤šæ¬¡å·¥å…·è°ƒç”¨
    multi_success = test_multiple_tool_calls()
    
    # æ€»ç»“ç»“æœ
    logger.info("=== æµ‹è¯•ç»“æœæ€»ç»“ ===")
    logger.info(f"LLMVertexæµå¼å·¥å…·è°ƒç”¨æµ‹è¯•: {'æˆåŠŸ' if llm_success else 'å¤±è´¥'}")
    logger.info(f"MCPLLMVertexæµå¼å·¥å…·è°ƒç”¨æµ‹è¯•: {'æˆåŠŸ' if mcp_success else 'å¤±è´¥'}")
    logger.info(f"å¤šæ¬¡å·¥å…·è°ƒç”¨æµ‹è¯•: {'æˆåŠŸ' if multi_success else 'å¤±è´¥'}")
    
    if llm_success and mcp_success and multi_success:
        logger.info("æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æµå¼å·¥å…·è°ƒç”¨ä¿®å¤æˆåŠŸï¼")
        return 0
    else:
        logger.error("éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)