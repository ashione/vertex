#!/usr/bin/env python3
"""
æµ‹è¯•å·¥å…·è°ƒç”¨ä¿®å¤åçš„åŠŸèƒ½
"""

import json
import time

from vertex_flow.utils.logger import LoggerUtil
from vertex_flow.workflow.chat import ChatModel
from vertex_flow.workflow.context import WorkflowContext
from vertex_flow.workflow.tools.functions import FunctionTool, today_func
from vertex_flow.workflow.vertex.llm_vertex import LLMVertex

logger = LoggerUtil.get_logger(__name__)


def test_tool_calls_in_streaming():
    """æµ‹è¯•æµå¼æ¨¡å¼ä¸‹çš„å·¥å…·è°ƒç”¨"""

    # åˆ›å»ºtodayå·¥å…·
    today_tool = FunctionTool(
        name="today",
        description="è·å–å½“å‰æ—¶é—´ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œæ—¶åŒºã€‚",
        func=today_func,
        schema={
            "type": "object",
            "properties": {
                "format": {
                    "type": "string",
                    "enum": [
                        "timestamp",
                        "timestamp_ms",
                        "iso",
                        "iso_utc",
                        "date",
                        "time",
                        "datetime",
                        "rfc2822",
                        "custom",
                    ],
                    "description": "è¾“å‡ºæ ¼å¼",
                },
                "timezone": {"type": "string", "description": "æ—¶åŒºï¼ˆå¦‚UTC, Asia/Shanghaiï¼‰"},
                "custom_format": {"type": "string", "description": "è‡ªå®šä¹‰æ ¼å¼å­—ç¬¦ä¸²"},
            },
            "required": [],
        },
    )

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ChatModelï¼ˆç”¨äºæµ‹è¯•ï¼‰
    class MockChatModel(ChatModel):
        def __init__(self):
            super().__init__(name="mock-model", sk="mock-key", base_url="mock-url", provider="mock")
            self.call_count = 0
            self.max_calls = 3  # é™åˆ¶æœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯

        def chat_stream(self, messages, option=None, tools=None):
            """æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ŒåŒ…å«å·¥å…·è°ƒç”¨"""
            self.call_count += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if self.call_count > self.max_calls:
                logger.warning(f"MockChatModelè¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶: {self.max_calls}")
                return

            # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦å·²æœ‰å·¥å…·è°ƒç”¨ç»“æœ
            has_tool_result = any(msg.get("role") == "tool" for msg in messages)

            # å¦‚æœå·²æœ‰å·¥å…·ç»“æœï¼Œè¿”å›æœ€ç»ˆå“åº”
            if has_tool_result:
                final_chunks = [
                    type(
                        "MockChunk",
                        (),
                        {
                            "choices": [
                                type(
                                    "MockChoice",
                                    (),
                                    {"delta": type("MockDelta", (), {"content": "æˆ‘å·²ç»è·å–äº†å½“å‰æ—¶é—´ï¼š"})()},
                                )()
                            ]
                        },
                    )(),
                    type(
                        "MockChunk",
                        (),
                        {
                            "choices": [
                                type(
                                    "MockChoice",
                                    (),
                                    {"delta": type("MockDelta", (), {"content": "2024-01-15T10:30:00"})()},
                                )()
                            ]
                        },
                    )(),
                ]
                for chunk in final_chunks:
                    yield chunk
                return

            # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè¿”å›å·¥å…·è°ƒç”¨
            tool_call_chunks = [
                # å·¥å…·è°ƒç”¨å¼€å§‹
                type(
                    "MockChunk",
                    (),
                    {
                        "choices": [
                            type(
                                "MockChoice",
                                (),
                                {
                                    "delta": type(
                                        "MockDelta",
                                        (),
                                        {
                                            "tool_calls": [
                                                type(
                                                    "MockToolCall",
                                                    (),
                                                    {
                                                        "id": "call_123",
                                                        "function": type(
                                                            "MockFunction",
                                                            (),
                                                            {"name": "today", "arguments": '{"format": "iso"}'},
                                                        )(),
                                                        "type": "function",
                                                    },
                                                )()
                                            ]
                                        },
                                    )()
                                },
                            )()
                        ]
                    },
                )(),
            ]

            for chunk in tool_call_chunks:
                yield chunk

    # åˆ›å»ºLLMVertex
    llm_vertex = LLMVertex(
        id="test_llm",
        name="æµ‹è¯•LLM",
        model=MockChatModel(),
        params={
            "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨todayå·¥å…·æ¥è·å–æ—¶é—´ä¿¡æ¯ã€‚",
            "user": [],
            "enable_stream": True,
            "enable_reasoning": False,
            "show_reasoning": False,
        },
        tools=[today_tool],
    )

    # è®¾ç½®æ¶ˆæ¯
    llm_vertex.messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨todayå·¥å…·æ¥è·å–æ—¶é—´ä¿¡æ¯ã€‚"},
        {"role": "user", "content": "è¯·å‘Šè¯‰æˆ‘ç°åœ¨çš„æ—¶é—´"},
    ]

    # åˆ›å»ºä¸Šä¸‹æ–‡
    context = WorkflowContext()

    print("ğŸ§ª å¼€å§‹æµ‹è¯•æµå¼å·¥å…·è°ƒç”¨...")
    print("=" * 50)

    try:
        # ä½¿ç”¨æµå¼ç”Ÿæˆå™¨
        print("ğŸ“¤ æµå¼è¾“å‡º:")
        chunk_count = 0
        for chunk in llm_vertex.chat_stream_generator({}, context):
            chunk_count += 1
            print(f"  Chunk {chunk_count}: {chunk}")

        print(f"\nâœ… æµå¼è¾“å‡ºå®Œæˆï¼Œå…±æ”¶åˆ° {chunk_count} ä¸ªchunk")

        # æ£€æŸ¥messagesä¸­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        print("\nğŸ“‹ æ£€æŸ¥messages:")
        for i, msg in enumerate(llm_vertex.messages):
            print(f"  Message {i}: {msg}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        tool_messages = [msg for msg in llm_vertex.messages if msg.get("role") == "tool"]
        if tool_messages:
            print(f"\nğŸ› ï¸ å‘ç° {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœ:")
            for msg in tool_messages:
                print(f"  Tool: {msg}")
        else:
            print("\nâš ï¸ æœªå‘ç°å·¥å…·è°ƒç”¨ç»“æœ")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


def test_tool_calls_in_non_streaming():
    """æµ‹è¯•éæµå¼æ¨¡å¼ä¸‹çš„å·¥å…·è°ƒç”¨"""

    # åˆ›å»ºtodayå·¥å…·
    today_tool = FunctionTool(
        name="today",
        description="è·å–å½“å‰æ—¶é—´ï¼Œæ”¯æŒå¤šç§æ ¼å¼å’Œæ—¶åŒºã€‚",
        func=today_func,
        schema={
            "type": "object",
            "properties": {
                "format": {
                    "type": "string",
                    "enum": [
                        "timestamp",
                        "timestamp_ms",
                        "iso",
                        "iso_utc",
                        "date",
                        "time",
                        "datetime",
                        "rfc2822",
                        "custom",
                    ],
                    "description": "è¾“å‡ºæ ¼å¼",
                },
                "timezone": {"type": "string", "description": "æ—¶åŒºï¼ˆå¦‚UTC, Asia/Shanghaiï¼‰"},
                "custom_format": {"type": "string", "description": "è‡ªå®šä¹‰æ ¼å¼å­—ç¬¦ä¸²"},
            },
            "required": [],
        },
    )

    # åˆ›å»ºæ¨¡æ‹Ÿçš„ChatModelï¼ˆç”¨äºæµ‹è¯•ï¼‰
    class MockChatModel(ChatModel):
        def __init__(self):
            super().__init__(name="mock-model", sk="mock-key", base_url="mock-url", provider="mock")
            self.call_count = 0
            self.max_calls = 3  # é™åˆ¶æœ€å¤§è°ƒç”¨æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯

        def chat(self, messages, option=None, tools=None):
            """æ¨¡æ‹Ÿéæµå¼è¾“å‡ºï¼ŒåŒ…å«å·¥å…·è°ƒç”¨"""
            self.call_count += 1

            # é˜²æ­¢æ— é™å¾ªç¯
            if self.call_count > self.max_calls:
                logger.warning(f"MockChatModelè¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶: {self.max_calls}")
                return type(
                    "MockChoice",
                    (),
                    {
                        "finish_reason": "stop",
                        "message": type(
                            "MockMessage",
                            (),
                            {
                                "role": "assistant",
                                "content": "è¾¾åˆ°æœ€å¤§è°ƒç”¨æ¬¡æ•°é™åˆ¶",
                            },
                        )(),
                    },
                )()

            # æ£€æŸ¥æ¶ˆæ¯ä¸­æ˜¯å¦å·²æœ‰å·¥å…·è°ƒç”¨ç»“æœ
            has_tool_result = any(msg.get("role") == "tool" for msg in messages)

            # å¦‚æœå·²æœ‰å·¥å…·ç»“æœï¼Œè¿”å›æœ€ç»ˆå“åº”
            if has_tool_result:
                return type(
                    "MockChoice",
                    (),
                    {
                        "finish_reason": "stop",
                        "message": type(
                            "MockMessage",
                            (),
                            {
                                "role": "assistant",
                                "content": "æˆ‘å·²ç»è·å–äº†å½“å‰æ—¶é—´ï¼š2024-01-15T10:30:00",
                            },
                        )(),
                    },
                )()

            # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šè¿”å›å·¥å…·è°ƒç”¨
            mock_choice = type(
                "MockChoice",
                (),
                {
                    "finish_reason": "tool_calls",
                    "message": type(
                        "MockMessage",
                        (),
                        {
                            "role": "assistant",
                            "content": "",
                            "tool_calls": [
                                type(
                                    "MockToolCall",
                                    (),
                                    {
                                        "id": "call_123",
                                        "function": type(
                                            "MockFunction", (), {"name": "today", "arguments": '{"format": "iso"}'}
                                        )(),
                                        "type": "function",
                                    },
                                )
                            ],
                        },
                    )(),
                },
            )()
            return mock_choice

    # åˆ›å»ºLLMVertex
    llm_vertex = LLMVertex(
        id="test_llm",
        name="æµ‹è¯•LLM",
        model=MockChatModel(),
        params={
            "system": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨todayå·¥å…·æ¥è·å–æ—¶é—´ä¿¡æ¯ã€‚",
            "user": [],
            "enable_stream": False,  # ç¦ç”¨æµå¼
            "enable_reasoning": False,
            "show_reasoning": False,
        },
        tools=[today_tool],
    )

    # è®¾ç½®æ¶ˆæ¯
    llm_vertex.messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨todayå·¥å…·æ¥è·å–æ—¶é—´ä¿¡æ¯ã€‚"},
        {"role": "user", "content": "è¯·å‘Šè¯‰æˆ‘ç°åœ¨çš„æ—¶é—´"},
    ]

    # åˆ›å»ºä¸Šä¸‹æ–‡
    context = WorkflowContext()

    print("ğŸ§ª å¼€å§‹æµ‹è¯•éæµå¼å·¥å…·è°ƒç”¨...")
    print("=" * 50)

    try:
        # ä½¿ç”¨éæµå¼æ¨¡å¼
        result = llm_vertex.chat({}, context)
        print(f"ğŸ“¤ éæµå¼è¾“å‡º: {result}")

        # æ£€æŸ¥messagesä¸­æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
        print("\nğŸ“‹ æ£€æŸ¥messages:")
        for i, msg in enumerate(llm_vertex.messages):
            print(f"  Message {i}: {msg}")

        # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ç»“æœ
        tool_messages = [msg for msg in llm_vertex.messages if msg.get("role") == "tool"]
        if tool_messages:
            print(f"\nğŸ› ï¸ å‘ç° {len(tool_messages)} ä¸ªå·¥å…·è°ƒç”¨ç»“æœ:")
            for msg in tool_messages:
                print(f"  Tool: {msg}")
        else:
            print("\nâš ï¸ æœªå‘ç°å·¥å…·è°ƒç”¨ç»“æœ")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸ”§ æµ‹è¯•å·¥å…·è°ƒç”¨ä¿®å¤")
    print("=" * 60)

    print("\n1ï¸âƒ£ æµ‹è¯•æµå¼æ¨¡å¼ä¸‹çš„å·¥å…·è°ƒç”¨:")
    test_tool_calls_in_streaming()

    print("\n" + "=" * 60)

    print("\n2ï¸âƒ£ æµ‹è¯•éæµå¼æ¨¡å¼ä¸‹çš„å·¥å…·è°ƒç”¨:")
    test_tool_calls_in_non_streaming()

    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆ!")
